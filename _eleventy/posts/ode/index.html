<p>As with most of these posts, what I have included here closely follows and often directly copies or quotes a text written by someone more errudite than me. In this case thank you to <em>Prof. Peter Kuchment</em> for <a href="https://www.math.tamu.edu/~kuchment/ode.pdf">A brief sketch of the main ODE theorems</a> written for Texas A&M University's Math 611, Fall 2017.</p>

<h3>Main notions</h3>
<p>
    <b>Definition 1:</b>  An ODE of order \( k \) is an equation relating the values of one or more unknown functions of a single variable \( t \) (which we will call "time"), their derivatives up to the order \( k \), and the independent variable itself:
  </p>
  
  

<p style="overflow: auto;">\[
\Phi(t, x_1, x_2, \ldots, x_n, x_i, x_i', \ldots, x_i^{(k)}, x_{i+1}^{(1)}, \ldots, x_{i+1}^{(n)}) = 0. \quad (1)
\]</p>

<p>
  If more than one unknown function is involved, a system of such equations is usually needed. A system can be neatly written in vector form so that it looks like a single equation, for example:
</p>

<p style="overflow: auto;">\[
\Phi(t, \mathbf{x}) = 0, \quad (2)
\]</p>

<p>
  where boldface font is used to denote vectors.
</p>

<blockquote>
<p>
  Example 2:
</p>
<ol>
  <li>
    \( x_i(t)x(t)^2 - 3t \sin(x_{ii}(t)) = 8 \) is an ODE (of what order? linear or non-linear?)
  </li>
  <li>
    \( x_i(t) = -5x(t + 7) \) is NOT an ODE!
  </li>
  <li>
    \( x''(t) = -x(t) + \int \cos(x(\tau)) \, d\tau \) is NOT an ODE.
  </li>
</ol>
</blockquote>

<p>
    Question:
  </p>
  <ul>
    <li>Why aren't the latter two examples ODEs? If you read the definition, it looks at first glance like there is nothing wrong with these examples.</li>
    <li>What was missing in the wording of the definition? How should it be changed to make sure we exclude such cases?</li>
    <li>Do you know how the equations of the type shown in the last two examples are called?</li>
  </ul>
  
  
<p>I skipped section 2 because it's like three lines listing ODE vs PDE, Order, and Linear vs Non-linear.</p>
<h3>Order reduction</h3>
<p>TLDR; (<em>Prof. Peter Kuchment's</em>) idea: just use vectors to represent systems of equations, then its one row of vectors.</p>

        <div class="text-secondary pl-5">
        <p>
        Introducing new unknown functions, an ODE or a system \(\Phi(t, \mathbf{x}) = 0, \quad (2)\) can be reduced to a first-order system:
        </p>

        <p style="overflow: auto;">\[
        \Phi(t, x_1, x_2, \ldots, x_k, x_i) = 0 \quad (3)
        \]</p>

        <p>
        where
        </p>

        <p style="overflow: auto;">\[
        \begin{align}
        x_{1i} &= x_2 \\
        x_{2i} &= x_3 \\
        &\ldots \\
        x_{k-1i} &= x_k \\
        \end{align}
        \]</p>

        <p>
        So now we can always deal with the first-order systems:
        </p>

        <p style="overflow: auto;">\[
        \Phi(t, \mathbf{x}, \mathbf{x_i}) = 0 \quad (4)
        \]</p>
        </div>

<p>
  <b>Definition 3:</b> 
</p>
<p>TLDR; with all due respect to <em>Prof. Peter Kuchment</em> his is just about whether the ODE does rely (non-autonomous) on stuff other than the initial condition \(x=0\), or if the ODE does no and only relies on the intial condition (autonomous).</p>
    <div class="text-secondary pl-5">
    <ul>
    <li>Normal:</li>
    </ul>

    <p style="overflow: auto;">\[
    x_i = F(t, \mathbf{x}) \quad (5)
    \]</p>

    <ul>
    <li>Autonomous:</li>
    </ul>

    <p style="overflow: auto;">\[
    x_i = F(\mathbf{x}) \quad (6)
    \]</p>

    <p>
    and non-autonomous (5) equations.
    </p>
    </div>

<h4>Reduction of a non-autonomous equation to an autonomous one:</h4>
<p>
  
  Introduce a new time \( \tau \) and consider the autonomous system:
</p>

<p style="overflow: auto;">\[
\begin{align}
  x'_0(\tau) &= F(t(\tau), x(\tau)) \\
  t'_0(\tau) &= 1
\end{align}
\]</p>

<p>
  This autonomous system is equivalent to the non-autonomous (5).
  In these notes, we will assume that \( x(t) \) is a differentiable function of \( t \in (a, b) \subseteq \mathbb{R}^n \), and \( F : \mathbb{R}^{n+1} \mapsto \mathbb{R}^n \).
  (The complex case is possible, but we will not consider it here.)
</p>
<blockquote>
    <p>
        What evolutionary (i.e., depending on time \( t \)) process can be described by ODEs?
        Let us have a process (mechanical, biological, etc.) whose instantaneous state can be described by some parameters \( x \).
        We call the space of these parameters the phase space.
        Since the system evolves with time, the parameters become functions of time as well: \( x(t) \).
        When can such a process be described by an ODE?
        Three conditions tell you when this is the case:
      </p>
      
      

<ul>
  <li>\(1\) The system is finite-dimensional, i.e., it can be described by finitely many parameters \( x_1, \ldots, x_n \).
    This is not the case, for instance, in fluid dynamics, heat conduction, and quantum mechanics.</li>
  <li>\(2\) Smoothness: the parameters change in a differentiable manner with time.
    This is not the case with shock waves.</li>
  <li>\(3\) The process is deterministic: the state of the system at a certain moment determines the whole future behavior of the system.
    Indeed, due to the first condition, we can describe the evolution of the system by a finite-dimensional vector function \( x(t) \).
    This function is differentiable, as the second condition tells us.
    The third condition says that if we know \( x(t) \) for some moment \( t \), this determines all the future values of \( x(\tau) \).
    In particular, this determines the value of the derivative \( x'_0 \) at the moment \( t \).</li>
</ul>
</blockquote>

<p>
  Hence, \( x'_0(t) \) is determined by \( t \) and \( x(t) \). In mathematical notations, we write that \( x'_0(t) \) is a function of \( t \) and \( x(t) \): \( x'_0(t) = F(t, x(t)) \), which is an ODE (a system of ODEs).
</p>

<p>
  IVP (Initial Value Problems):
</p>

<p style="overflow: auto;">\[
\begin{align}
  \frac{dx}{dt} &= F(t, x) \\
  x(t_0) &= x_0 \quad \quad (8)
\end{align}
\]</p>
<p>For those familiar with index notation (i.e., Kronecker delta, Levi-Civita, and generally "linear algebra without matrices"), skip the following four cards, and jump into <b><a href="#def4">Definition 4</a></b></p>
<div class="card"><div class="card-body">
    <p>A note on index notation:</p>
    <p>Thank you to <em>Prof. David Roylance</em> for his brief <a href="https://web.mit.edu/course/3/3.11/www/modules/index.pdf">Matrix and Index Notation</a> from MIT September 18, 2000</p>
    <p>A vector can be represented by its components along the Cartesian axes, denoted as \(u_x, u_y, u_z\) for the displacement vector \(\mathbf{u}\). The components can also be indicated with numerical subscripts, such as \(u_1, u_2, u_3\), corresponding to the \(x, y\), and \(z\) directions. In a shorthand notation, the displacement vector can be written as \(u_i\), where the subscript \(i\) ranges over 1, 2, 3 (or 1 and 2 in two-dimensional problems). This is known as the range convention for index notation. With this convention, the vector equation \(u_i = a\) yields three scalar equations:</p>
    </p><p style="overflow: auto;">\[
    \begin{aligned}
    & u_{1}=a \\
    & u_{2}=a \\
    & u_{3}=a
    \end{aligned}
    \]
    
    </p><p>We will often find it convenient to denote a vector by listing its components in a vertical list enclosed in braces, and this form will help us keep track of matrix-vector multiplications a bit more easily. We therefore have the following equivalent forms of vector notation:
    
    </p><p style="overflow: auto;">\[
    \mathbf{u}=u_{i}=\left\{\begin{array}{l}
    u_{1} \\
    u_{2} \\
    u_{3}
    \end{array}\right\}=\left\{\begin{array}{l}
    u_{x} \\
    u_{y} \\
    u_{z}
    \end{array}\right\}
    \]
    
    </p><p>Second-rank quantities such as stress, strain, moment of inertia, and curvature can be denoted as \(3 \times 3\) matrix arrays; for instance the stress can be written using numerical indices as
    
    </p><p style="overflow: auto;">\[
    [\sigma]=\left[\begin{array}{lll}
    \sigma_{11} & \sigma_{12} & \sigma_{13} \\
    \sigma_{21} & \sigma_{22} & \sigma_{23} \\
    \sigma_{31} & \sigma_{32} & \sigma_{33}
    \end{array}\right]
    \]
    
    </p>
    
    <p>Here the first subscript index denotes the row and the second the column. The indices also have a physical meaning, for instance \(\sigma_{23}\) indicates the stress on the 2 face (the plane whose normal is in the 2 , or \(y\), direction) and acting in the 3 , or \(z\), direction. To help distinguish them, we'll use brackets for second-rank tensors and braces for vectors.
    
    </p><p>Using the range convention for index notation, the stress can also be written as \(\sigma_{i j}\), where both the \(i\) and the \(j\) range from 1 to 3 ; this gives the nine components listed explicitly above. (Since the stress matrix is symmetric, i.e. \(\sigma_{i j}=\sigma_{j i}\), only six of these nine components are independent.)
    
    </p><p>A subscript that is repeated in a given term is understood to imply summation over the range of the repeated subscript; this is the summation convention for index notation. For instance, to indicate the sum of the diagonal elements of the stress matrix we can write:
    
    </p>
    
    <p style="overflow: auto;">\[
    \sigma_{k k}=\sum_{k=1}^{3} \sigma_{k k}=\sigma_{11}+\sigma_{22}+\sigma_{33}
    \]
    
    </p><p>The multiplication rule for matrices can be stated formally by taking \(\mathbf{A}=\left(a_{i j}\right)\) to be an \((M \times N)\) matrix and \(\mathbf{B}=\left(b_{i j}\right)\) to be an \((R \times P)\) matrix. The matrix product \(\mathbf{A B}\) is defined only when \(R=N\), and is the \((M \times P)\) matrix \(\mathbf{C}=\left(c_{i j}\right)\) given by
    
    </p><p style="overflow: auto;">\[
    c_{i j}=\sum_{k=1}^{N} a_{i k} b_{k j}=a_{i 1} b_{1 j}+a_{i 2} b_{2 j}+\cdots+a_{i N} b_{N k}
    \]
    
    </p><p>Using the summation convention, this can be written simply
    
    </p><p style="overflow: auto;">\[
    c_{i j}=a_{i k} b_{k j}
    \]
    
    </p><p>where the summation is understood to be over the repeated index \(k\). In the case of a \(3 \times 3\) matrix multiplying a \(3 \times 1\) column vector we have
    
    </p><p style="overflow: auto;">\[
    \left[\begin{array}{lll}
    a_{11} & a_{12} & a_{13} \\
    a_{21} & a_{22} & a_{23} \\
    a_{31} & a_{32} & a_{33}
    \end{array}\right]\left\{\begin{array}{l}
    b_{1} \\
    b_{2} \\
    b_{3}
    \end{array}\right\}=\left\{\begin{array}{c}
    a_{11} b_{1}+a_{12} b_{2}+a_{13} b_{3} \\
    a_{21} b_{1}+a_{22} b_{2}+a_{23} b_{3} \\
    a_{31} b_{1}+a_{32} b_{2}+a_{33} b_{3}
    \end{array}\right\}=a_{i j} b_{j}
    \]
    
    </p><p>The comma convention uses a subscript comma to imply differentiation with respect to the variable following, so \(f_{, 2}=\partial f / \partial y\) and \(u_{i, j}=\partial u_{i} / \partial x_{j}\). For instance, the expression \(\sigma_{i j, j}=0\) uses all of the three previously defined index conventions: range on \(\mathrm{i}\), sum on \(\mathrm{j}\), and differentiate:
    
    </p><p style="overflow: auto;">\[
    \begin{aligned}
    & \frac{\partial \sigma_{x x}}{\partial x}+\frac{\partial \sigma_{x y}}{\partial y}+\frac{\partial \sigma_{x z}}{\partial z}=0 \\
    & \frac{\partial \sigma_{y x}}{\partial x}+\frac{\partial \sigma_{y y}}{\partial y}+\frac{\partial \sigma_{y z}}{\partial z}=0 \\
    & \frac{\partial \sigma_{z x}}{\partial x}+\frac{\partial \sigma_{z y}}{\partial y}+\frac{\partial \sigma_{z z}}{\partial z}=0
    \end{aligned}
    \]
    
    </p><p>The Kronecker delta is a useful entity is defined as
    
    </p><p style="overflow: auto;">\[
    \delta_{i j}= \begin{cases}0, & i \neq j \\ 1, & i=j\end{cases}
    \]
    
    </p><p>This is the index form of the unit matrix \(\mathbf{I}\) :
    
    </p><p style="overflow: auto;">\[
    \delta_{i j}=\mathbf{I}=\left[\begin{array}{ccc}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & 1
    \end{array}\right]
    \]
    
    </p><p>So, for instance
    
    </p><p style="overflow: auto;">\[
    \sigma_{k k} \delta_{i j}=\left[\begin{array}{ccc}
    \sigma_{k k} & 0 & 0 \\
    0 & \sigma_{k k} & 0 \\
    0 & 0 & \sigma_{k k}
    \end{array}\right]
    \]
    
    </p><p>where \(\sigma_{k k}=\sigma_{11}+\sigma_{22}+\sigma_{33}\).
    
    </p>
</div></div><br>

<p>Now, before returning to <em>Prof. Peter Kuchment's</em> lovely review of the ODE, and for one last note beyond <em>Prof. David Roylance's</em> notes on index notation, (this time from the faceless physics department at <a href="https://bohr.physics.berkeley.edu/classes/209/f02/leviciv.pdf">UC Berkeley, Fall 2002</a>): I want to address the Levi-Civita Symbol which is closely tied to index notation and the Kronecker delta.</em>

    <div class="card"><div class="card-body"><br>
    
<p>The Levi-Civita symbol is useful for converting cross products and curls into the language of tensor analysis, and for many other purposes. The following is a summary of its most useful properties in three-dimensional Euclidean space.</p>
<p>The Levi-Civita symbol is defined by</p>
<p style="overflow: auto;">\[\epsilon_{ijk}= \begin{cases}1, & \text{ if }(ijk) \text{ is an even permutation of }(123) ; \\ -1, & \text{ if }(ijk) \text{ is an odd permutation of }(123) ; \\ 0, & \text{ otherwise. }\end{cases}\]</p>
<p>It has 27 components, of which only 6 are nonzero. It follows directly from this definition that \(\epsilon_{ijk}\) changes sign if any two of its indices are exchanged,</p>
<p style="overflow: auto;">\[\epsilon_{ijk}=\epsilon_{jki}=\epsilon_{kij}=-\epsilon_{jik}=-\epsilon_{ikj}=-\epsilon_{kji} .\]</p>
<p>The Levi-Civita symbol is convenient for expressing cross products and curls in tensor notation. For example, if \(\mathbf{A}\) and \(\mathbf{B}\) are two vectors, then</p>
<p style="overflow: auto;">\[(\mathbf{A} \times \mathbf{B})_{i}=\epsilon_{ijk} A_{j} B_{k}\]</p>
<p>and</p>
<p style="overflow: auto;">\[(\nabla \times \mathbf{B})_{i}=\epsilon_{ijk} \frac{\partial B_{k}}{\partial x_{j}}\]</p>
<p>Any combination of an even number of Levi-Civita symbols (or an even number of cross products and curls) can be reduced to dot products with the following system of identities. Similarly, any combination of an odd number of Levi-Civita symbols (or an odd number of cross products and curls) can be reduced to a single Levi-Civita symbol (or a cross product or a curl) plus dot products. The first is the most general:</p>
<p style="overflow: auto;">\[\epsilon_{ijk} \epsilon_{\ell mn}=\left|\begin{array}{ccc}
\delta_{i\ell} & \delta_{im} & \delta_{in} \\
\delta_{j\ell} & \delta_{jm} & \delta_{jn} \\
\delta_{k\ell} & \delta_{km} & \delta_{kn}
\end{array}\right|\]</p><p>Notice that the indices \((i j k)\) label the rows, while ( \(\ell m n)\) label the columns. If this is contracted on \(i\) and \(l\), we obtain</p>
<p style="overflow: auto;">\[\epsilon_{ijk} \epsilon_{imn}=\left|\begin{array}{cc}
\delta_{jm} & \delta_{jn} \\
\delta_{km} & \delta_{kn}
\end{array}\right|=\delta_{jm} \delta_{kn}-\delta_{jn} \delta_{km} .\]</p>
<p>This identity is the one used most often, for boiling down two cross products that have one index in common, such as \(\nabla \times(\mathbf{A} \times \mathbf{B})\). By contracting \(\epsilon_{ijk} \epsilon_{ijn}\) in \(j\) and \(m\) we obtain</p>
<p style="overflow: auto;">\[\epsilon_{ijk} \epsilon_{ijn}=2 \delta_{kn} \text {. }\]</p>
<p>Finally, contracting on \(k\) and \(n\) we obtain</p>
<p style="overflow: auto;">\[\epsilon_{ijk} \epsilon_{ijk}=6\]</p>
<p>It should be clear how to generalize these identities to higher dimensions.</p>
<p>If \(A_{ij}=-A_{ji}\) is an antisymmetric, \(3 \times 3\) tensor, it has 3 independent components that we can associate with a 3 -vector \(\mathbf{A}\), as follows:</p>
<p style="overflow: auto;">\[A_{ij}=\left(\begin{array}{ccc}
0 & A_{3} & -A_{2} \\
-A_{3} & 0 & A_{1} \\
A_{2} & -A_{1} & 0
\end{array}\right)=\epsilon_{ijk} A_{k} .\]</p>
<p>The inverse of this is</p><p style="overflow: auto;">\[A_{ij}=\frac{1}{2} \epsilon_{ijk} A_{k}\]</p>
<p>Using these identities, the multiplication of an antisymmetric matrix times a vector can be reexpressed in terms of a cross product. That is, if</p>
<p style="overflow: auto;">\[X_{i}=A_{ij} Y_{j}\]</p>
<p>then</p>
<p style="overflow: auto;">\[\mathbf{X}=\mathbf{Y} \times \mathbf{A}\]</p>
<p>Similarly, if \(\mathbf{A}\) and \(\mathbf{B}\) are two vectors, then</p>
<p style="overflow: auto;">\[A_{i} B_{j}-A_{j} B_{i}=\epsilon_{ijk}(\mathbf{A} \times \mathbf{B})_{k},\]</p>
<p>and</p>
<p style="overflow: auto;">\[\frac{\partial B_{j}}{\partial x_{i}}-\frac{\partial B_{i}}{\partial x_{j}}=\epsilon_{ijk}(\nabla \times \mathbf{B})_{k}\]</p>
<p>Finally, if \(M_{ij}\) is a \(3 \times 3\) matrix (or tensor), then</p>
<p style="overflow: auto;">\[\operatorname{det} M=\epsilon_{ijk} M_{1i} M_{2j} M_{3k}=\frac{1}{6} \epsilon_{ijk} \epsilon_{\ell mn} M_{i\ell} M_{jm} M_{kn} .\]</p><p>The Levi-Civita symbol has been defined here only on <span class="math inline">\(\mathbb{R}^{3}\)</span>, but most of the properties above are easily generalized to <span class="math inline">\(\mathbb{R}^{n}\)</span> (including the case <span class="math inline">\(n=2\)</span> ). It only transforms as a tensor under proper orthogonal changes of coordinates, which is why we are calling it a "symbol" instead of a "tensor." It can, however, be used to create so-called tensor densities on arbitrary manifolds with a metric, and has fascinating applications in Hodge-de Rham theory in differential geometry.</p>
</div></div><br>

<p>As a postscript to that last note on the Levi-Civita symbol (you should be glad I have not touched on the dozens of notations for differential geometry, & all that.) I just wanted to give the definition and properties of the \(n\)-dimensional case for Levi-Civita from <a href="https://en.wikipedia.org/wiki/Levi-Civita_symbol">Wikipedia: Levi-Civita symbol</a></p>

<div class="card"><div class="card-body">
    <p>\(n\)-Levi-Civita Definition:</p>
    <p>
        
        More generally, in \(n\) dimensions, the Levi-Civita symbol is defined by:
       </p> 
       <p style="overflow: auto;">
        \[
        \varepsilon_{a_1 a_2 a_3 \ldots a_n} = 
        \begin{cases}
        +1 & \text{if }(a_1, a_2, a_3, \ldots, a_n) \text{ is an even permutation of } (1, 2, 3, \dots, n) \\
        -1 & \text{if }(a_1, a_2, a_3, \ldots, a_n) \text{ is an odd permutation of } (1, 2, 3, \dots, n) \\
        0 & \text{otherwise}
        \end{cases}
        \]
        </p>
        <p>
        Thus, it is the sign of the permutation in the case of a permutation, and zero otherwise.
        </p>
        <p>
        Using the capital pi notation \(\Pi\) for ordinary multiplication of numbers, an explicit expression for the symbol is:
        </p>
        <p style="overflow: auto;">
        \[
        \begin{align*}
        \varepsilon_{a_1 a_2 a_3 \ldots a_n} & = \prod_{1 \leq i < j \leq n} \text{sgn} (a_j - a_i) \\
        & = \text{sgn}(a_2 - a_1) \text{sgn}(a_3 - a_1) \dotsm \text{sgn}(a_n - a_1) \text{sgn}(a_3 - a_2) \text{sgn}(a_4 - a_2) \dotsm \text{sgn}(a_n - a_2) \dotsm \text{sgn}(a_n - a_{n-1})
        \end{align*}
        \]
        </p>
        <p>
        where the signum function (denoted \(\text{sgn}\)) returns the sign of its argument while discarding the absolute value if nonzero. The formula is valid for all index values, and for any \(n\) (when \(n = 0\) or \(n = 1\), this is the empty product). However, computing the formula above naively has a time complexity of \(O(n^2)\), whereas the sign can be computed from the parity of the permutation from its disjoint cycles in only \(O(n \log(n))\) cost.
        </p>
</div></div><br>

<p>Now, the properties of \(n\)-Levi-Civita are harder to shoot off because the \(n\)-case is more like the top rung of the Levi-Civita latter. Where low dimensional intuition is merely helpful for the definition and general understanding of Levi-Civita, such low dimensional intuition is almost necessary for familiarity with the \(n\)-dimensional properties.</p>
<div class='card'><div class='card-body'>
    <p>TLDR; this first paragraph describes the tensor properties of Levi-Civita, but that might warrant a whole different post, so I paraphrased plus reduced the font size and color it to skip or read as you please.</p>
   <div class="text-secondary pl-5"> 
    <ul>
        <li><sub>A permutation tensor has components given by the Levi-Civita symbol in an orthonormal basis. It is a tensor of covariant rank.</sub></li>
        <li><sub>The Levi-Civita symbol remains unchanged under pure rotations and in coordinate systems related by orthogonal transformations. However, it is classified as a pseudotensor because it does not change under certain orthogonal transformations that would result in a sign change if it were a tensor.</sub></li>
        <li><sub>Taking a cross product using the Levi-Civita symbol yields a pseudovector, not a vector.</sub></li>
        <li><sub>Under a general coordinate change, the components of the permutation tensor are scaled by the Jacobian of the transformation matrix. This means that in different coordinate frames, the components can differ from those of the Levi-Civita symbol by an overall factor. In an orthonormal frame, the factor will be ±1 depending on the orientation.</sub></li>
        <li><sub>In index-free tensor notation, the Hodge dual replaces the Levi-Civita symbol.</sub></li>
        <li><sub>Einstein notation allows the elimination of summation symbols, where a repeated index implies summation over that index.</sub></li>
      </ul></div>
<p><span class="math display">\[\varepsilon_{ijk} \varepsilon^{imn} \equiv \sum_{i=1,2,3} \varepsilon_{ijk} \varepsilon^{imn}\]</span>
In the following examples, Einstein notation is used.</p>
<b id="two_dimensions">Two dimensions</b>
<p>In two dimensions, when all \(i, \ j, \ m, \ n\)  each take the values 1 and 2:</p>
<p style="overflow: auto;">\[\varepsilon_{ij} \varepsilon^{mn} = {\delta_i}^m {\delta_j}^n - {\delta_i}^n {\delta_j}^m\]</p>
<p style="overflow: auto;">\[\varepsilon_{ij} \varepsilon^{in} = {\delta_j}^n\]</p>
<p style="overflow: auto;">\[\varepsilon_{ij} \varepsilon^{ij} = 2.\]</p>
<b id="three_dimensions">Three dimensions</b>
<p>In three dimensions, when all \(i, \ j, \ m, \ n\) each take values 1, 2, and 3:</p>

<p style="overflow: auto;">\[\varepsilon_{ijk} \varepsilon^{pqk}=\delta_i{}^{p}\delta_j{}^q - \delta_i{}^q\delta_j{}^p\] </p>
<p style="overflow: auto;">\[\varepsilon_{jmn} \varepsilon^{imn}=2{\delta_j}^i\] </p>
<p style="overflow: auto;">\[\varepsilon_{ijk} \varepsilon^{ijk}=6.\] </p>

<b id="product">Product</b>
<p>The Levi-Civita symbol is related to the Kronecker delta. In three dimensions, the relationship is given by the following equations (vertical lines denote the determinant):</p>
<p style="overflow: auto;">
    \[\begin{align}
  \varepsilon_{ijk}\varepsilon_{lmn} &amp;= \begin{vmatrix}
    \delta_{il} &amp; \delta_{im} &amp; \delta_{in} \\
    \delta_{jl} &amp; \delta_{jm} &amp; \delta_{jn} \\
    \delta_{kl} &amp; \delta_{km} &amp; \delta_{kn} \\
  \end{vmatrix} \\[6pt]
                                     &amp;= \delta_{il}\left( \delta_{jm}\delta_{kn} - \delta_{jn}\delta_{km}\right) - \delta_{im}\left( \delta_{jl}\delta_{kn} - \delta_{jn}\delta_{kl} \right) + \delta_{in} \left( \delta_{jl}\delta_{km} - \delta_{jm}\delta_{kl} \right). 
\end{align}\]</p>
<p>A special case of this result is</p>
<p><span class="math display">\[\sum_{i=1}^3 \varepsilon_{ijk}\varepsilon_{imn} = \delta_{jm}\delta_{kn} - \delta_{jn}\delta_{km}\]</span></p>
<p>sometimes called the "contracted epsilon identity" </p>
<p>In Einstein notation, the duplication of the \(i\) index implies the sum on \(i\). The previous is then denoted \( \varepsilon_{ijk}\varepsilon_{ijn} = \delta_{jm}\delta_{kn} - \delta_{jn}\delta_{km} \).</p>
<p><span class="math display">\[\sum_{i=1}^3 \sum_{j=1}^3 \varepsilon_{ijk}\varepsilon_{ijn} = 2\delta_{kn}\]</span></p>
</div></div><br>


<p>Hopefully, that looked quite similar to the UC Berkeley physics exposition because here are the properties for the \(n\)-Levi-Civita case:</p>
<div class='card'><div class='card-body'>
    <p>\(n\)-Levi-Civita Properties:</p>
    <p>In \(n\) dimensions, when all \(i_1, \ldots, i_n, j_1, \ldots, j_n\) take values from \(1, 2, \ldots, n\):</p>
<ul style="overflow: auto;">
<li>\((A) \quad \varepsilon_{i_1 \dots i_n} \varepsilon^{j_1 \dots j_n} = \delta^{j_1 \dots j_n}_{i_1 \dots i_n} \)</li>
<li>\((B) \quad \varepsilon_{i_1 \dots i_k~i_{k+1} \dots i_n} \varepsilon^{i_1 \dots i_k~j_{k+1} \dots j_n} = \delta_{ i_1 \ldots i_k~i_{k+1} \ldots i_n}^{i_1 \dots i_k~j_{k+1}\ldots j_n} = k!~\delta^{j_{k+1} \dots j_n}_{i_{k+1} \dots i_n}\) </li>
<li>\((C) \quad \varepsilon_{i_1 \dots i_n}\varepsilon^{i_1 \dots i_n} = n!\) </li>
</ul>
<p>where the exclamation mark (\(!\)) denotes the factorial, and \(\delta^{\text{α} \ldots}_{\text{β} \ldots}\) is the generalized Kronecker delta. For any \(n\), the property:</p>
<p style="overflow: auto;">\[\sum_{i, j, k, \ldots = 1}^n \varepsilon_{ijk\ldots}\varepsilon_{ijk\ldots} = n!\]</p>
<p>follows from the facts that:</p>
<ol>
<li>every permutation is either even or odd,</li>
<li>\(1 = (+1)^2 = (-1)^2 = 1\), and</li>
<li>the number of permutations of any \(n\)-element set is exactly \(n!\).</li>
</ol>
<p>The particular case above of </p><p style="overflow: auto;">\((B) \quad \varepsilon_{i_1 \dots i_k~i_{k+1} \dots i_n} \varepsilon^{i_1 \dots i_k~j_{k+1} \dots j_n} = \delta_{ i_1 \ldots i_k~i_{k+1} \ldots i_n}^{i_1 \dots i_k~j_{k+1}\ldots j_n} = k!~\delta^{j_{k+1} \dots j_n}_{i_{k+1} \dots i_n}\)</p>
<p> with \(k = n-2\) is:</p>
<p style="overflow: auto;">\[\varepsilon_{i_1\ldots i_{n-2}jk}\varepsilon^{i_1\ldots i_{n-2}lm} = (n-2)!(\delta_j^l\delta_k^m - \delta_j^m\delta_l^k)\]</p>
<b>Product</b>
<p>In general, for \(n\) dimensions, the product of two Levi-Civita symbols can be written as:</p>
<p style="overflow: auto;">\[\varepsilon_{i_1 i_2 \ldots i_n} \varepsilon_{j_1 j_2 \ldots j_n} = \begin{vmatrix}
\delta_{i_1 j_1} & \delta_{i_1 j_2} & \ldots  & \delta_{i_1 j_n} \\
\delta_{i_2 j_1} & \delta_{i_2 j_2} & \ldots  & \delta_{i_2 j_n} \\
\vdots           & \vdots           & \ddots & \vdots \\
\delta_{i_n j_1} & \delta_{i_n j_2} & \ldots  & \delta_{i_n j_n} \\
\end{vmatrix}\]</p>
<p><strong>Proof:</strong> Both sides change signs upon switching two indices, so without loss of generality, assume \(i_1 \leq \ldots \leq i_n\) and \(j_1 \leq \ldots \leq j_n\). If some \(i_c = i_{c+1}\), then the left side is zero, and the right side is also zero since two of its columns are equal. The same applies if \(j_c = j_{c+1}\). Finally, if \(i_1 < \ldots < i_n\) and \(j_1 < \ldots < j_n\), then both sides equal 1.</p>

</div></div><br>
<p>Now, time to put that index notation to use!</p>
<p>
  <b id="def4">Definition 4:</b>  For a differentiable function \( F: \mathbb{R}^m \rightarrow \mathbb{R}^n \), the differential \( DF(y) \) of \( F \) at a point \( y \) is the linear mapping from \( \mathbb{R}^m \) to \( \mathbb{R}^n \) with the matrix:
</p>

<p style="overflow: auto;">\[
\{DF\}_{ij}(y) = \frac{\partial F_i}{\partial x_j}(y) \quad (9)
\]</p>

<p>
  For a vector \( x \) of small norm, \( DF(y)x \) is the linear approximation of the change of the function \( F(y + x) - F(y) \), i.e., (Taylor formula of first order, or linearization formula):
</p>

<p style="overflow: auto;">\[
F(y + x) = F(y) + DF(y)x + o(|x|)
\] </p>


<p><b>Theorem 5:</b>  Existence and Uniqueness Theorem.</p>

<p>
  
  Let \( \Omega \subset \mathbb{R}^n \) be an open domain, \( (a, b) \) be an open segment of the line \( \mathbb{R} \), and \( F(t, x) \) and \( D_xF(t, x) \) be continuous in \( (a, b) \times \Omega \).
  Then, for any point \( (t_0, x_0) \) in \( (a, b) \times \Omega \), there exists a unique solution \( x(t) \) of the IVP <span style="overflow: auto;">\(  \frac{dx}{dt} = F(t, x) \Longrightarrow x(t_0) = x_0 \quad (8)\)</span> defined in a neighborhood of \( t_0 \).
</p>
<p>
<strong>
  Remark 6:
</strong>
</p>

<ol>
  <li>No global (i.e., on the whole \( (a, b) \)) uniqueness is guaranteed. Why?</li>
  <li>The proof will show that another condition on \( F \) instead of differentiability suffices: it is enough that \( F \) is Lipschitz, i.e.,
    \( |F(t, x) - F(t, y)| \leq K|x - y| \) for some constant \( K \) and all \( (t, x), (t, y) \) in our domain.
    Is this condition weaker than the one in the theorem?</li>
</ol>


<h3>Now about the proof: contraction mappings and such</h3>

<h4>Some notions and notations:</h4>
<p>
  For a continuous function \( x: [a, b] \rightarrow \mathbb{R}^n \) on a finite segment \([a, b]\), we denote by
</p>

<p style="overflow: auto;">\( \| x \| = \max_{t \in [a, b]} \| x(t) \| \quad (10) \)</p>

<p>
  its norm in the space of such continuous functions, where \( \| x \| \) is the Euclidean norm of a vector in \( \mathbb{R}^n \).
</p>

<p style="overflow: auto;">
  \( C^r \) - class of \( r \) times continuously differentiable functions (applicable to both scalar and vector-valued functions of different numbers of variables, should be understandable from the context).
</p>

<p><b>Definition 7:</b>  A real-valued function \( A(x) \) on \( \mathbb{R} \) is a <b>contraction</b> if it satisfies the inequality</p>

<p style="overflow: auto;">\[
| A(x) - A(y) | \leq K | x - y |
\]</p>

<p>
  for some \( K < 1 \) and all real \( x \) and \( y \).
</p>

<p>
  <strong>Remark 8:</strong>
</p>
<ol>
  <li>Condition of continuous differentiability of \( A \) and estimate \( | A'(x) | \leq k < 1 \) guarantee that \( A \) is a <b>contraction</b>.</li>
  <li>The definition of a <b>contraction</b> can be naturally extended to any metric space \( M \) with a metric (distance function) \( \rho \) instead of the real line, replacing \( | A(x) - A(y) | \), \( | x - y | \) above with \( \rho(A(x), A(y)) \), \( \rho(x, y) \).</li>
</ol>

<h4>A simple instance of the contraction mapping principle:</h4>

<p>
  <strong>Theorem 9:</strong> Let \( A(x) \) be a contraction on \( \mathbb{R} \). Then:
</p>
<ol>
  <li>The equation \( x = A(x) \) has a unique solution \( x^* \) (called the fixed point of \( A(x) \)).</li>
  <li>This fixed point can be found as \( x^* = \lim_{j \to \infty} x_j \), where \( x_0 \) is arbitrary and \( x_{i+1} = A(x_i) \).</li>
</ol>


<h4>The general contraction mapping principle:</h4>

<p>
  Let \( X \) be a <b>metric space</b> (i.e., a set equipped with a metric or "distance" \( \rho(x, y) \) with properties \( \rho \geq 0 \), \( \rho(x, y) = \rho(y, x) \), \( \rho(x, y) = 0 \) only if \( x = y \), and the triangle inequality is satisfied \( \rho(x, y) \leq \rho(x, z) + \rho(y, z) \)).
</p><p>Also let \( X \) be a <b>complete</b> metric space (i.e., if a sequence \( x_n \) is such that \( \rho(x_n, x_m) \to 0 \) as \( n,m \to \infty \), then there exists its limit \( x \) such that \( \rho(x_n, x) \to 0 \)).
</p>

<p>
  A mapping \( A: X \to X \) is a <b>contraction</b> if
</p>

<p style="overflow: auto;">\[
\rho(A(x), A(y)) \leq K \rho(x, y)
\]</p>

<p>
  for some \( K < 1 \) and all \( x, y \in X \).
</p>

<b>Theorem 10:</b>

<p>
  Let \( A(x) \) be a contraction on a complete metric space \( X \). Then:
</p>
<ol>
  <li>The equation \( x = A(x) \) has a unique solution \( x^* \) in \( X \) (called the fixed point of \( A(x) \)).</li>
  <li>This fixed point can be found as \( x^* = \lim_{j \to \infty} x_j \), where \( x_0 \) is arbitrary and \( x_{i+1} = A(x_i) \).</li>
</ol>


<h4>An equivalent integral equation reformulation of the IVP <span style="overflow: auto;">\(  \frac{dx}{dt} = F(t, x) \Longrightarrow x(t_0) = x_0 \quad (8)\)</span>:</h4>

<p style="overflow: auto;">\( x(t) = x_0 + \int_{t_0}^t F(\tau, x(\tau)) d\tau  \quad (11)\)</p>

<p>
  <strong>Lemma 11:</strong> Continuous solutions of <span style="overflow: auto;">\( x(t) = x_0 + \int_{t_0}^t F(\tau, x(\tau)) d\tau  \quad (11)\)</span> are exactly the continuously differentiable solutions of <span style="overflow: auto;">\(  \frac{dx}{dt} = F(t, x) \Longrightarrow x(t_0) = x_0 \quad (8)\)</span>.
</p>

<p>
  Now the proof of <b>Theorem 5</b> would be concluded if we prove the existence and uniqueness of continuous solutions of <span style="overflow: auto;">\( x(t) = x_0 + \int_{t_0}^t F(\tau, x(\tau)) d\tau  \quad (11)\)</span> on a small segment around \( t_0 \).
</p>

<p>
  <strong>The metric space:</strong> Consider the interval \([t_0 - d, t_0 + d] \subset (a, b)\) with a small \( d \) (it will be determined later on how small it should be).
  We also consider a ball \( B = \{ x \in \mathbb{R}^n \,|\, \| x - x_0 \| \leq b \} \) that is entirely contained in \( \Omega \).
  Now define on the set \( X \) of all continuous functions \( x(t) \) from \([t_0 - d, t_0 + d]\) to \( B \) the max norm \(\| x \| = \max_{t \in [a, b]} \| x(t) \| \quad (10)\) as before and the corresponding metric \( \rho(x, y) = \| x - y \| \).
</p>

<p>
  Define the following integral operator \( x \to A(x) \):
  <p style="overflow: auto;">\[ [A(x)](t) = x_0 + \int_{t_0}^t F(\tau, x(\tau)) d \tau \quad (12) \]</p>
</p>

<p>
  Note that this definition works for functions that map \([t_0 - d, t_0 + d]\) to \( B \).
</p>

<p>
  <strong>Lemma 12:</strong> For a sufficiently small \( d \), the operator \( A(x) \) maps the above class of functions into itself and is a contraction, i.e.,
  <span style="overflow: auto;">\( \| A(x) - A(y) \| \leq \| x - y \| \)</span> for some \( k < 1 \).
</p>

<p>
  <strong>Corollary 13:</strong>
</p>
<ol>
  <li>The integral equation \( x(t) = x_0 + \int_{t_0}^t F(\tau, x(\tau)) d\tau  \quad (11)\) has a unique continuous solution in a neighborhood of \( t_0 \)</li>
</ol>


<p>2. This solution can be found as the limit in the norm <span style="overflow: auto;">\( \| x \| = \max_{t \in [a, b]} \| x(t) \| \quad (10) \)</span> of Picard iterations:</p>

<p style="overflow: auto;">\[
y_{i+1}(t) = x_0 + \int_{t_0}^t F(\tau, y_i(\tau)) d\tau \quad (14)
\]</p>

<p>
  where \( y_0 \) can be chosen arbitrarily in such a way that \( y_0(t_0) = x_0 \), e.g., \( y_0 \equiv x_0 \).
</p>

<p>3. The Uniqueness and Existence <b>Theorem 5</b>  is proven.</p>

<h3>An existence theorem:</h3>

<p>
  <strong><b>Theorem 14:</b>  Peano's existence theorem.</strong> Continuity of \( F \) alone guarantees local existence of a solution of the IVP <span style="overflow: auto;">\(  \frac{dx}{dt} = F(t, x) \Longrightarrow x(t_0) = x_0 \quad (8)\)</span>.
</p>

<p>
  <strong>Remark 15:</strong>
</p>

<blockquote>
<ol>
    <li>In the proof of Peano's theorem, the solution is also found as the limit of some sequence of functions, but rather than Picard's iterations, a sequence of Euler's piecewise linear functions (recall the Euler's method of numerical solution) is constructed.</li>
    <li>Example of the IVP problem: \(\frac{dx}{dt} = 3x^{2/3}\), \(x(0) = 0\) shows that the conditions of Peano's theorem cannot guarantee uniqueness.</li>
  </ol>
</blockquote>

<h3>Geometry of ODEs: vector fields</h3>

<p>
  Consider the autonomous case:
</p>

<p style="overflow: auto;">\[
\frac{dx}{dt} = F(x), \quad x(t) \in \mathbb{R}^n \quad (15)
\]</p>

<p>
  We can think that \( F(x) \) assigns to each point \( x \) a vector \( F(x) \) (a vector "grows" out of any point).
  Then we call \( F(x) \) a vector field. We will consider at least continuous (or smoother) functions \( F(x) \) and corresponding vector fields.
  If \( F \) is of some class \( C^r \), we will also say that the field is of this class.
</p>


<p><b>Lemma 16:</b> </p>

<p>
  Trajectories of solutions of (15) are exactly the curves that are tangent at each point to the vector field corresponding to this equation. Such curves are called phase curves of the field.
</p>

<p>
  Note that vector fields are NOT defined for non-autonomous systems.
  The field \( F(x) \) is said to be non-singular at a point \( x_0 \), if \( F(x_0) \neq 0 \).
</p>

<p>Exercise 17:</p>

<p>
  Show that the fields arising from turning a non-autonomous system into autonomous ones are always non-singular at all points.
  Example of a non-singular vector field: a constant vector field, where \( F(x) \) is a constant non-zero vector.
</p>

<p>
  <strong>Question 18:</strong> Is the existence and uniqueness theorem obvious for a constant vector field?
</p>

<blockquote>
    <p>
        A diffeomorphism of class \( C^r \) is a mapping \( G \) from a domain such that it is one-to-one and both \( G \) and its inverse \( G^{-1} \) are of mappings class \( C^r \).
        In other words, a diffeomorphism smoothly deforms the domain.
        At each point \( x \), the differential \( (DG)(x) \) is an invertible linear mapping of vectors in \( \mathbb{R}^n \).
        One can act by diffeomorphisms on vector fields as well.
        One can come up with a right definition using the following heuristics:
        Let \( x(t) \) be a solution of the equation defined by our vector field: \( x_0 = F(x) \).
        We can act on this solution by our diffeomorphism to get a new function \( x_G(t) = G(x(t)) \).
        Then the chain rule gives </p>
        <p style="overflow: auto;">\[ x_0^G = (DG)(x) x_0 = (DG)(x) F(x) = (DG)(G^{-1}(x_G)) F(G^{-1}(x_G)) \]</p>
        In other words, the \( G \)-modified function \( x_G \) satisfies the ODE \( y_0 = F_G(y) \) with a vector field \( F_G(y) = (DG)(G^{-1}(y)) F(G^{-1}(y)) \).
      </p>
        
</blockquote>

<p><b>Definition 19:</b> </p>

<p>
  Let \( F(x) \) be a vector field and \( G \) be a diffeomorphism.
  Then one defines a new vector field as follows: \( F_G(x) = (DG)(G^{-1}(x)) F(G^{-1}(x)) \).
</p>

<p><b>Theorem 20:</b>  Vector Field Rectification Theorem.</p>

<p>
  Any vector field of class \( C^r \) in a neighborhood of any of its non-singular point \( x_0 \) can be reduced to a constant field ("rectified") by applying a diffeomorphism of class \( C^r \).
  One of the exercises is to show that the rectification theorem implies the existence and uniqueness one.
</p>

<p>
  <strong>Question:</strong> Can one do the converse, i.e., get an idea of the local construction of the rectifying diffeomorphism from a known solution?
</p>


<h3>Dependence of solutions on parameters and initial data.</h3>

<p>
  The solution of the IVP <span style="overflow: auto;">\(  \frac{dx}{dt} = F(t, x) \Longrightarrow x(t_0) = x_0 \quad (8)\)</span> depends on the values of \( t_0 \) and \( x_0 \). How smooth is this dependence?
  Another important question: Assume that the right hand side (the vector field) also depends on some parameter(s) \( \mu \):
</p>

<p style="overflow: auto;">\[
\frac{dx}{dt} = F(t, x, \mu), \quad x(t_0) = x_0. \quad (16)
\]</p>

<p>
  How smoothly does the solution depend on the parameter?
  In fact, it can be seen that dependence on the initial data reduces to dependence on parameters.
  Indeed, introducing a new time variable \( \tau = t - t_0 \) and a new spatial variable \( y = x - x_0 \), one reduces <span style="overflow: auto;">\(  \frac{dx}{dt} = F(t, x) \Longrightarrow x(t_0) = x_0 \quad (8)\)</span> to:
</p>

<p style="overflow: auto;">\[
\frac{dy}{d\tau} = F(\tau + t_0, y + x_0), \quad y(0) = 0. \quad (17)
\]</p>

<p>
  Now all variable parameters are in the right hand side rather than in the initial data (which become constant). So, this is the only case to handle.
</p>

<p>
  <strong>Theorem 21:</strong> Let the vector field \( F(x, \mu) \) (where \( \mu \) belongs to an open domain of a space \( \mathbb{R}^m \)) be of class \( C^r \). Let also \( F(x_0, \mu_0) \neq 0 \). Then the (unique) solution \( x(t, t_0, x, x_0, \mu) \) of the IVP:
</p>

<p style="overflow: auto;">\[
\frac{dx}{dt} = F(x, \mu), \quad x(t_0) = x_0. \quad (18)
\]</p>

<p>
  depends differentiably of class \( C^r \) on \( (t, t_0, x, x_0, \mu) \) for sufficiently small \( |t - t_0| \), \( |x - x_0| \), \( |\mu - \mu_0| \).
</p>

<h3>Extendability of local solutions.</h3>

<blockquote>
    <p>
        Our theorems guaranteed the existence of a local solution only with no guarantee of how long it will survive.
        Simple examples show the disappearance of solutions into a singular point.
        Even without singular points, a solution curve can grow fast and disappear in a finite time.
        An example is the IVP \( \frac{dx}{dt} = x^2 \), \( x(0) = 1 \) that has the solution \( x = \frac{1}{1 - t} \) that disappears at infinity when \( t \) approaches 1.
        Are there any other options? Answer: no.
      </p>
      
      
</blockquote>


<p><b>Theorem 22:</b>  Extendability Theorem.</p>
<p>
  Let \( N \) be a compact (bounded closed) subset in \( \Omega \) (the domain where the smooth field is defined).
  Let also \( F \) have no singular points in \( N \).
  Then any local solution of <span style="overflow: auto;">\(  \frac{dx}{dt} = F(t, x) \Longrightarrow x(t_0) = x_0 \quad (8)\)</span> in \( (a, b) \times \Omega \) can be extended forward (for \( t > t_0 \)) and backward (for \( t < t_0 \)) either indefinitely or until it reaches the boundary of \( N \).
</p>
<h3>Boundary value problems (BVPs)</h3>

<blockquote>
    <ul>
        <li>Here the conditions are imposed at both ends of a time interval, rather than at one end only in the IVP case.</li>
        <li>Important applications.</li>
        <li>The number of conditions should still be correct (depending on the order of the system and the number of unknown functions).</li>
        <li>No such nice existence and uniqueness theorem.</li>
      </ul>
</blockquote>