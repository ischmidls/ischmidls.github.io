<h3>table of contents</h3>
<ol>
    <li><a href="#the-vector-differential-drr">vector differential</a></li>
    <li><a href="#coord">vector differential  in other coordinate systems</a></li>
    <li><a href="#integration">integration</a></li>
    <li><a href="#line-integral">line integral</a></li>
    <li><a href="#scalar">scalar line integrals</a></li>
    <li><a href="#vec-line-int">vector line integrals</a></li>
    <li><a href="#path-ind">path independence</a></li>
    <li><a href="#con-vec">conservative vector fields</a></li>
    <li><a href="#surface">surface</a></li>
    <li><a href="#cross-prod">a bigger better cross product</a></li>
    <li><a href="#gen-surf">general surface</a></li>
    <li><a href="#flux">flux"></a></li>
    <li><a href="#symm">to symmetry or not to symmetry</a></li>
    <li><a href="#"></a></li>
    <li><a href="#"></a></li>
</ol>
<p>This time I am pulling from the lovely Oregon State <b><a href="https://sites.science.oregonstate.edu/math/BridgeBook/">Bridge Book</a></b>. It was written
 by Tevian Dray and Corinne A. Manogue circa 2009&mdash;2015 and funded by the National Science Foundation. </p>


 <h3 id="the-vector-differential-drr"><span id="the_vector_differential_d_rr">The Vector Differential, \(d\mathbf{r}\)</span></h3>
<p>Tevian Dray and Corinne A. Manogue kick it off with a great start.</p>

    <p><img src="../../img/curvilinear vector differentials/mag vec diff.png"  /></p>
    
    <p><strong>Figure 7</strong> illustrates the infinitesimal displacement vector \(d\mathbf{r}\) along a curve, shown in an "infinite magnifying glass." The position vector \(\mathbf{r}=x\mathbf{\hat{x}}+y\mathbf{\hat{y}}+z\mathbf{\hat{z}}\) represents the location of the point \((x,y,z)\) in rectangular coordinates, originating from the origin. It is helpful to visualize the small change \(\Delta\mathbf{r}=\Delta x\mathbf{\hat{x}}+\Delta y\mathbf{\hat{y}}+\Delta z\mathbf{\hat{z}}\) in the position vector between nearby points. Taking a step further, we consider an infinitesimal change in position, denoted by \(d\mathbf{r}\), representing the vector between infinitesimally close points. Figure 7 provides a magnified view of the curve to illustrate this concept.</p>
 
 
    <p><img src="../../img/curvilinear vector differentials/fig8a.png" class="mediabox2" /></p>
 
<p> <strong>Figure 8a:</strong></p>
 <p><em>The rectangular components of the vector differential \(d\mathbf{r}\) in two dimensions.</em>
 </p> 
 <p><span id="ds1"></span>Like any vector, \(d\mathbf{r}\) can be expanded with respect to \(\mathbf{\hat{x}}\), \(\mathbf{\hat{y}}\), \(\mathbf{\hat{z}}\); the components of \(d\mathbf{r}\) are just the infinitesimal changes \(dx\), \(dy\), \(dz\) in the \(x\), \(y\), and \(z\) directions, respectively. That is,</p>
 
 <p style="overflow: auto;">
 \[
 d\mathbf{r} = dx\mathbf{\hat{x}} + dy\mathbf{\hat{y}} + dz\mathbf{\hat{z}} \label{drdef} \tag{1}
 \]
</p>
 
 <p>as shown in Figure 8a. The geometric notion of \(d\mathbf{r}\) as an infinitesimal vector displacement will be a unifying theme to help us visualize the geometry of all of vector calculus.</p>
 
 <p><span id="dsdef"></span>What is the infinitesimal distance \(ds\) between nearby points? It is simply the length of \(d\mathbf{r}\). We have</p>
 
 <p style="overflow: auto;">
 \[
 ds = |d\mathbf{r}| = \sqrt{d\mathbf{r} \cdot d\mathbf{r}} = \sqrt{dx^2 + dy^2 + dz^2} \label{dsdef} \tag{2}
 \]
</p>
 
 
    <p><img src="../../img/curvilinear vector differentials/fig8b.png" /></p>

<p> <strong>Figure 8b:</strong></p>
<p> <em>The infinitesimal version of the Pythagorean Theorem.</em></p>
 
 <p><span id="drdef"></span>Squaring both sides of Equation \(\ref{dsdef}\) leads to</p>
 
 <p style="overflow: auto;">
 \[
 ds^2 = |d\mathbf{r}|^2 = d\mathbf{r} \cdot d\mathbf{r} = dx^2 + dy^2 + dz^2 \label{ds1} \tag{3}
 \]
</p>
 
 <p>which is just the infinitesimal Pythagorean Theorem, the two-dimensional version of which is shown in Figure 8b.</p>
 
 When dealing with infinitesimals, we prefer to avoid second-order errors by anchoring all vectors to the same point.
 <h3 id="coord">vector differential  in other coordinate systems</h3>
<p>Staying strong with Tevian Dray and Corinne A. Manogue.</p>
      <p><img src="../../img/curvilinear vector differentials/vec diff.png" title="" alt="" /></p>



<p>      <strong>Figure 1a:</strong></p>
     <p> <em>The infinitesimal vector version of the Pythagorean Theorem in rectangular coordinates.</em>
     </p>
      <p><img src="../../img/curvilinear vector differentials/polar vec diff.png" class="mediabox2" title="" alt="" /></p>

<p>
    <strong>Figure 1b:</strong></p>
      <p><em>The infinitesimal vector version of the Pythagorean Theorem, in polar coordinates.</em>
      </p>
      <p>The Pythagorean Theorem in rectangular coordinates does not define \(d\vec{r}\) and \(ds\) in terms of component expressions (\(d\vec{r}\) being coordinate-independent). Instead, studying \(d\vec{r}\) in another coordinate system, such as polar coordinates (\(r, \phi\)), is useful. In polar coordinates, basis vectors \(\{\hat{r}, \hat{p}\}\) are used, with \(\hat{r}\) as the unit vector in the radial direction and \(\hat{p}\) as the unit vector in the direction of increasing \(\phi\).</p>

<p>By determining the lengths of the sides of the infinitesimal polar "rectangle" in Figure 1b, we find \(d\vec{r} = dr\,\hat{r} + r\,d\phi\,\hat{p}\). Note that the \(\hat{p}\) term includes a factor of \(r\), as \(d\phi\) alone is not a length. The length of an infinitesimal arc is \(r\,d\phi\). Using Equation (3) from the Vector Differential section, the length of \(d\vec{r}\) is given by \(ds^2 = dr^2 + r^2\,d\phi^2\), which is the infinitesimal Pythagorean Theorem in polar coordinates.</p>

<p>We adopt the polar angle \(\phi\) to align with the standard conventions for spherical coordinates used universally, except by American mathematicians.</p>

<p>It is possible to relate \(\hat{r}\) and \(\hat{p}\) to \(\hat{x}\) and \(\hat{y}\), but in most physical applications, an appropriate choice of coordinates eliminates the need for this step.</p>

<h3 id="integration">integration</h3>
<p>Not too bad with Tevian Dray and Corinne A. Manogue at this point.</p>
<p>Integration involves dividing and adding smaller pieces, which is particularly useful for solving problems with multiple integrals. For example, when finding the mass of a straight wire with linear density \(\lambda\), we chop it into small pieces of length \(dx\). Each piece has a mass of \(\lambda dx\), and the total mass \(M\) is obtained by integrating \(\lambda(x)\) with respect to \(x\).</p>

<p>Similarly, in higher dimensions, such as for a rectangular plate with surface density \(\sigma\), we divide it into small rectangular pieces with area \(dA = dxdy\). The mass of each piece is \(\sigma dA\), and the total mass \(M\) is obtained by integrating \(\sigma(\mathbf{r})\) with respect to \(dA\).</p>

<p>In polar coordinates, if we have a circular plate with density \(\sigma = \sigma(r,\phi)\), we use polar pieces with area \(dA = rdrd\phi\). The total mass \(M\) is obtained by integrating \(\sigma(r,\phi)r\) with respect to \(r\) and \(\phi\).</p>

<p>While integration is commonly associated with finding antiderivatives and representing areas, it is important to understand the physical significance of infinitesimal mass \(\lambda dx\) and the choice of differential notation. Infinitesimally small pieces can be achieved by taking limits, and the notation involving differentials allows for a concise representation of this concept.</p>

<p>Note:</p>
<ol>
  <li>Differential notation emphasizes the ability to make pieces arbitrarily small and is shorthand for using Riemann sums with appropriate limits.</li>
  <li>A single integral sign is used for adding rectangular pieces, while multiple integral signs are used for iterated single integrals.</li>
  <li>The areas of infinitesimal rectangular and polar pieces are not comparable, and the choice of how to divide them does not affect the final result.</li>
</ol>
<h3 id="line-integral">line integral</h3>
<p>Oh, you thought I'd forget to mention Tevian Dray and Corinne A. Manogue.</p>
<p>There are many ways to describe a curve. Consider the following descriptions:</p>
<ol>
  <li>The unit circle</li>
  <li>\(x^2 + y^2 = 1\)</li>
  <li>\(y = 1 - \sqrt{1 - x^2}\)</li>
  <li>\(r = 1\)</li>
  <li>\(x = \cos\phi\), \(y = \sin\phi\)</li>
  <li>\(\mathbf{r}(\phi) = \cos\phi\mathbf{\hat{x}} + \sin\phi\mathbf{\hat{y}}\)</li>
</ol>

<p>If you want to add up something along a curve, you need to compute a line integral. Common examples include determining the length of a curve, the mass of a wire, or the amount of work done when moving an object along a specific path.</p>

<p>Let's consider the problem of finding the length of a quarter of a circle. In polar coordinates, a circle is represented by \(r = \text{constant}\), which implies \(dr = 0\). By substituting this fact into the expression for arc length in polar coordinates, we immediately obtain:</p>

<p style="overflow: auto;">
\[ds^2 = r^2d\phi^2 \quad (1)\]
</p>

<p>and finally:</p>

<p style="overflow: auto;">
\[\int_C ds = \int_0^{\frac{\pi}{2}} r d\phi = \frac{\pi r^2}{2} \quad (2)\]
</p>

<p> The calculation is not much harder in rectangular coordinates. You know that \(x = r\cos\phi\) and \(y = r\sin\phi\) with \(r = \text{constant}\), which gives \(dx = -r\sin\phi d\phi\) and \(dy = r\cos\phi d\phi\). By inserting this into (3) of The Vector Differential, \(ds^2 = |d\mathbf{r}|^2 = d\mathbf{r} \cdot d\mathbf{r} = dx^2 + dy^2 + dz^2 \), we arrive at (2) again.</p>

<p>But what if you didn't even remember how to parameterize a circle or how to use polar coordinates? Well, you still know that \(x^2 + y^2 = r^2 = \text{constant}\), which implies \(2xdx + 2ydy = 0\). Solving for \(dy\) and inserting this into (3) of The Vector Differential \(ds^2 = |d\mathbf{r}|^2 = d\mathbf{r} \cdot d\mathbf{r} = dx^2 + dy^2 + dz^2 \) yields:</p>

<p style="overflow: auto;">
\[ds^2 = \left(1 + \frac{x^2}{y^2}\right)dx^2 = \frac{r^2}{r^2 - x^2}dx^2 \quad (3)\]
</p>

<p>This leads to the (improper!) integral:</p>

<p style="overflow: auto;">
\[\int_0^r \frac{dx}{\sqrt{1 - \frac{x^2}{r^2}}} \quad (4)\]
</p>

<p>which can be easily computed using a trigonometric substitution or numerically, yielding the same answer.</p>
<div class='card'><div class='card-body'>
    <p>\[\begin{aligned} \int \frac{d x}{\sqrt{a^2-x^2}} & =\int \frac{a \cos \theta d \theta}{\sqrt{a^2-a^2 \sin ^2 \theta}} \\ & =\int \frac{a \cos \theta d \theta}{\sqrt{a^2\left(1-\sin ^2 \theta\right)}} \\ & =\int \frac{a \cos \theta d \theta}{\sqrt{a^2 \cos ^2 \theta}} \\ & =\int d \theta \\ & =\theta+C \\ & =\arcsin \frac{x}{a}+C\end{aligned}\]</p>
    <p>(Adapted from <a href="https://en.wikipedia.org/wiki/Trigonometric_substitution">Wikipedia: Trig sub</a>)</p>
    </div></div><br>

<h3 id="scalar">scalar line integrals</h3>
<p>Tevian Dray and Corinne A. Manogue stay strong as we move deeper in.</p>
<p>What if you want to determine the mass of a wire in the shape of the curve \(C\) if you know the density \(\lambda\)? The same procedure still works: chop and add. In this case, the length of a small piece of the wire is \(ds = |\mathbf{dr}|\), so its mass is \(\lambda ds\), and the integral becomes:</p>

<p style="overflow: auto;">
\[m = \int_C \lambda ds \quad (1)\]
</p>

<p>which can also be written as:</p>

<p style="overflow: auto;">
\[m = \int_C \lambda(\mathbf{r}) |\mathbf{dr}| \quad (2)\]
</p>

<p>which emphasizes both that \(\lambda\) is not constant and that \(ds\) is the magnitude of \(\mathbf{dr}\).</p>

<p>Another standard application of this type of line integral is to find the center of mass of a wire. This is done by averaging the values of the coordinates, weighted by the density \(\lambda\) as follows:</p>

<p style="overflow: auto;">
\[ \bar{x} = \frac{1}{m} \int_C x \lambda(\mathbf{r}) ds \quad (3) \]
</p>

<p>with \(m\) as defined above. Similar formulas hold for \(\bar{y}\) and \(\bar{z}\); the center of mass is then the point \((\bar{x}, \bar{y}, \bar{z})\).</p>

<h3 id="vec-line-int">vector line integrals</h3>

<p>Tevian Dray and Corinne A. Manogue do not have much to say about "Vector Line Integrals", but they give more context in other sections.</p>

<p>Consider now the problem of finding the work \(W\) done by a force \(\mathbf{F}\) in moving a particle along a curve \(C\). We begin with the relationship:</p>

<p style="overflow: auto;">
\[ \text{work} = \text{force} \times \text{distance} \quad (1) \]
</p>

<p>Suppose you take a small step \(\mathbf{dr}\) along the curve. How much work was done? Since only the component along the curve matters, we need to take the dot product of \(\mathbf{F}\) with \(\mathbf{dr}\). Adding this up along the curve yields:</p>

<p style="overflow: auto;">
\[ W = \int_C \mathbf{F} \cdot \mathbf{dr} \quad (2) \]
</p>

<p>So how do you evaluate such an integral?</p>

<div class='card'><div class='card-body'>
    <p>Here's an example from "Paul's Math Notes" 
        <b>
        <a href="https://tutorial.math.lamar.edu/classes/calciii/LineIntegralsVectorFields.aspx">Line Integrals Of Vector Fields</a></b>
    </p>

    <p>Note the notation in the integral on the left side. That really is a dot product of the vector field and the differential, which really is a vector. Also, \(\mathbf{F}(\mathbf{r}(t))\) is a shorthand for:</p>
<p style="overflow: auto;">
\[
\mathbf{F}(\mathbf{r}(t)) = \mathbf{F}(x(t), y(t), z(t))
\]
</p>

<p>We can also write line integrals of vector fields as a line integral with respect to arc length as follows:</p>
<p style="overflow: auto;">
\[
\int_C \mathbf{F} \cdot d\mathbf{r} = \int_C \mathbf{F} \cdot \mathbf{T} ds
\]
</p>
where \(\mathbf{T}(t)\) is the unit tangent vector given by:
<p style="overflow: auto;">
\[
\mathbf{T}(t) = \frac{\mathbf{r}'(t)}{\|\mathbf{r}'(t)\|}
\]
</p>

<p>If we use our knowledge of how to compute line integrals with respect to arc length, we can see that this second form is equivalent to the first form given above:</p>
<p style="overflow: auto;">
\[
\int_C \mathbf{F} \cdot d\mathbf{r} = \int_a^b \mathbf{F}(\mathbf{r}(t)) \cdot \mathbf{r}'(t) \|\mathbf{r}'(t)\| dt = \int_a^b \mathbf{F}(\mathbf{r}(t)) \cdot \mathbf{r}'(t) dt
\]
</p>

<p>In general, we use the first form to compute these line integrals as it is usually much easier to use.</p>
    
        <p>Example 2: Evaluate</p>
<p style="overflow: auto;">
\[ \int_C \mathbf{F} \cdot d\mathbf{r} \]
</p>
where \(\mathbf{F}(x, y, z) = xz\mathbf{i} - yz\mathbf{k}\) and \(C\) is the line segment from \((-1, 2, 0)\) to \((3, 0, 1)\).

<p><strong>Solution:</strong></p>
<p>We'll first need the parameterization of the line segment. We saw how to get the parameterization of line segments in the first section on line integrals. Here is the parameterization for the line:</p>
<p style="overflow: auto;">
\[ \mathbf{r}(t) = (1 - t)\langle -1, 2, 0 \rangle + t\langle 3, 0, 1 \rangle = \langle 4t - 1, 2 - 2t, t \rangle, \quad 0 \leq t \leq 1 \]
</p>

<p>Now let's evaluate the vector field along the curve:</p>
<p style="overflow: auto;">
\[ \mathbf{F}(\mathbf{r}(t)) = (4t - 1)(t)\mathbf{i} - (2 - 2t)(t)\mathbf{k} = (4t^2 - t)\mathbf{i} - (2t - 2t^2)\mathbf{k} \]
</p>

<p>Next, we need the derivative of the parameterization:</p>
<p style="overflow: auto;">
\[ \mathbf{r}'(t) = \langle 4, -2, 1 \rangle \]
</p>

<p>Now, we can calculate the dot product:</p>
<p style="overflow: auto;">
\[ \mathbf{F}(\mathbf{r}(t)) \cdot \mathbf{r}'(t) = 4(4t^2 - t) - (2t - 2t^2) = 18t^2 - 6t \]
</p>

<p>Finally, the line integral becomes:</p>
<p style="overflow: auto;">
\[ \int_C \mathbf{F} \cdot d\mathbf{r} = \int_0^1 (18t^2 - 6t) dt = \left(6t^3 - 3t^2 \right) \bigg|_0^1 = 3 \]
</p>
</div></div><br>

<p>An important special case of a line integral occurs when the curve \(C\) is closed, meaning it starts and ends at the same point. In this case, we write the integral in the form:</p>
<p style="overflow: auto;">
\[W = \oint_C \mathbf{F} \cdot d\mathbf{r} \quad (1)\]
</p>
<p>
and refer to it as the circulation of \(\mathbf{F}\) around \(C\). Unless otherwise specified, it is assumed that the curve is oriented in the counterclockwise direction. Reversing the orientation results in an overall minus sign, as with all vector line integrals.
</p>
<p>This notation can also be used for scalar line integrals, such as finding the mass of a ring of wire, which could be written in the form:</p>
<p style="overflow: auto;">
\[M = \oint_C \lambda ds \quad (2)\]
</p>
<p>
    Although in this case, the orientation doesn't matter. One way to remember this difference between vector and scalar integrals is to realize that \(ds = |d\mathbf{r}|\), and the magnitude of a vector does not depend on its direction.
    </p>
<h3 id="path-ind">path independence</h3>
<p>I am grateful for Tevian Dray and Corinne A. Manogue.</p>
<p><strong>RECALL:</strong> \(\int_a^b f'(x) \, dx = f(b) - f(a)\)</p>


<p>This is the Fundamental Theorem of Calculus, which states that the integral of a derivative is equal to the original function. We can also write it simply as:
\(\int df = f\)</p>

<p>But recall the master formula of Gradients, which says:
\(df = \nabla \cdot f \cdot d\mathbf{r}\)</p>

<p>Putting this all together, we arrive at the fundamental theorem for line integrals, which states that:</p>
<p>\[\int_C \nabla \cdot f \cdot d\mathbf{r} = f \, \bigg|_{A}^B\[</p>
<p>for any curve \(C\) starting at point A and ending at point B </p>

<p>Notice that the right-hand side does not depend on the curve \(C\). </p>
<p>There is some fine print here: the curve must lie in a connected region (one without holes) where \(\nabla \cdot f\) is defined everywhere. This is usually not a problem if \(f\) is differentiable everywhere, but there are important examples where this condition fails.</p>
<p>This behavior leads us to the notion of path independence. A line integral of the form \(\int_C \mathbf{F} \cdot d\mathbf{r}\) is said to be path independent if its value depends only on the endpoints A and B of the curve \(C\), and not on the specific curve connecting them. If a line integral is path independent, we can simply write:</p>
<p>\[\int_{A}^B \mathbf{F} \cdot d\mathbf{r}\]</p>

<p>If you know that a line integral is path independent, you can choose a different path (with the same endpoints) that simplifies the evaluation of the integral as much as possible!</p>


<h3 id="con-vec">conservative vector fields</h3>
<p>Ok, Tevian Dray and Corinne A. Manogue really got into it for this section.</p>
<p>
The fundamental theorem implies that vector fields of the form \(\mathbf{F} = \nabla f\) are special; the corresponding line integrals are always independent of path. One way to think of this is to imagine the level curves of \(f\); the change in \(f\) depends only on where you start and end, not on how you get there. These special vector fields have a name: A vector field \(\mathbf{F}\) is said to be conservative if there exists a potential function \(f\) such that \(\mathbf{F} = \nabla f\).
</p>
<p>
If \(\mathbf{F}\) is conservative, then \(\int_C \mathbf{F} \cdot d\mathbf{r}\) is independent of path; the converse is also true. But how do you know if a given vector field \(\mathbf{F}\) is conservative? That's the next lesson.
</p>

<p>
    We describe here a variation of the usual procedure for determining whether a vector field is conservative and, if it is, for finding a potential function.
    </p>
    <p><img alt="" title="" src="../../img/curvilinear vector differentials/1a.png"></p>
    <p>
    Figure 1a: Symbolic "tree diagram" for computing mixed partial derivatives with two variables.
    </p>
    <p><img alt="" title="" src="../../img/curvilinear vector differentials/1b.png"></p>
    <p>
    Figure 1b: Symbolic "tree diagram" for computing mixed partial derivatives with three variables.
    </p>
    <p>
    It is helpful to make a diagram of the structure underlying potential functions and conservative vector fields. For functions of two variables, this is shown in Figure 1a. The potential function \(f\) is shown at the top. Slanted lines represent derivatives of \(f\); derivatives with respect to \(x\) go to the left, while derivatives with respect to \(y\) go to the right. The second line thus gives the components of \(\nabla f\). The bottom line shows the mixed second derivatives, which can of course be computed in either order.
    </p>
    <p>
    But what if we are not given \(f\)? Suppose we are given a vector field, such as \(\mathbf{F} = y\mathbf{i} + (x+2y)\mathbf{j}\) and need to determine whether it is conservative, that is, whether \(\mathbf{F}\) is the gradient of some potential function \(f\). This is the second line of the diagram! We could start by checking that the mixed derivatives agree. However, what we really want is the potential function; we should be moving up the diagram, not down. What happens if we simply integrate both components, as shown in Figure 2a? The potential function is clearly contained in the results of these two integrals; it is just a question of combining them correctly.
    </p>
    <p>
    Furthermore, there is enough information here to determine whether \(\mathbf{F}\) is conservative in the first place; there is no need to check the derivatives. For example, had we been given the vector field \(\mathbf{H} = y\mathbf{i} + 2y\mathbf{j}\) and integrated its components, we would obtain Figure 2b. Simply by noticing that \(xy\), a function of two variables, only occurs once, we see that \(\mathbf{H}\) is not conservative.
    </p>
    <p><img alt="" title="" src="../../img/curvilinear vector differentials/2a.png"></p>
    <p>
    Figure 2a: An inverted tree diagram for trying to find a potential function.
    </p>
    <p><img alt="" title="" src="../../img/curvilinear vector differentials/2b.png"></p>
    <p>
    Figure 2b: Another inverted tree diagram for trying to find a potential function.
    </p>
    <p>
    We describe this as a murder mystery. A crime has been committed by the unknown murderer \(f\); your job is to find the identity of \(f\) by interviewing the witnesses. Who are the witnesses? The components of the vector field. What do they tell you? Well, you have to integrate ("interrogate") them! Now for the fun part.
    </p>
    <p>
    If two witnesses say they saw someone with red hair, that doesn't mean the suspect has two red hairs! So if you get the same clue more than once, you only count it once.
    </p>
    <p>
    On the other hand, some clues require corroboration. These witnesses were situated in such a way that each could only look in one direction. Thus, one witness, the \(x\)-component, only sees terms involving \(x\), etc. If a clue contains more than one variable, it should have been seen by more than one witness! In fact, functions of \(n\) variables should occur precisely \(n\) times. In the case of the vector field \(\mathbf{H}\), the clue \(xy\) was only seen by one witness, not both; somebody is lying! In short, clues must be consistent.
    </p>
    <p>
    Here is the Murder Mystery Method in a nutshell:
    </p>
    <p>
    1. Integrate: Integrate the \(x\)-component with respect to \(x\), etc.
    2. Check consistency: Functions of \(n\) variables must occur exactly \(n\) times. 
       (If the consistency check fails, the vector field is not conservative.)
    3. Combine clues: Use each clue once to determine the potential function.
    </p>
    <p>
    The power of the murder mystery method is even more apparent in three dimensions. We encourage you to try to find a potential function for the vector field \(\mathbf{G} = yz\mathbf{i} + (xz+z)\mathbf{j} + (xy+y+2z)\mathbf{k}\) using this method. The underlying structure is shown in Figure 1b, where now \(y\) derivatives are shown going straight down, and \(z\) derivatives go to the right.
    </p>
    <p>
    Consistency is traditionally checked by computing the last line of the appropriate diagram in Figures 1a and 1b. We reiterate that this is not necessary with the Murder Mystery Method. We will, however, return to these diagrams later when discussing the curl of a vector field.
    </p>
    <p>
    1) Checking consistency may not be straightforward. Are \(-\cos^2 x \cos^2 y\) and \(\sin^2 x \cos^2 y\) the same? We have \(2\sin^2 x \cos^2 y = -\cos^2 x \cos^2 y + \cos^2 y = \sin^2 x \cos^2 y + \sin^2 x\), which shows that the "xy" parts of these functions agree. (If this cannot be done, consistency fails.) But how do we count the remaining functions of 1 variable? Recall that the \(x\)-witness can only reliably provide clues involving \(x\)! If rewriting one or more functions this way generates a new such term, count it as usual. If instead, this generates a term involving the wrong variable(s), such as a clue provided by the \(x\)-witness that does not involve \(x\), discard it as unsubstantiated; this amounts to allowing the constant of integration to depend on the other variables. This procedure does not depend on the manner in which particular functions are rewritten, and such manipulations are not usually needed for typical classroom examples.
    </p>

    <h3 id="surface">surface</h3>
<p>Tevian Dray and Corinne A. Manogue say it like it is here.</p>
    <p>Consider the following descriptions:</p>
<ol>
<li>the unit sphere;</li>
<li>\(x^2+y^2+z^2=1;\)</li>
<li>\(r=1\) (where \(r\) is the spherical radial coordinate);</li>
<li>\(x=\sin\theta\cos\phi\), \(y=\sin\theta\sin\phi\), \(z=\cos\theta;\)</li>
<li>\(\mathbf{r}(\theta,\phi)=\sin\theta\cos\phi\mathbf{i}+\sin\theta\sin\phi\mathbf{j}+\cos\theta\mathbf{k};\)</li>
</ol>
<p>all of which describe the same surface.</p>

<p>The simplest surfaces are those given by holding one of the coordinates constant. Thus, the xy-plane is given by \(z=0\). Its (surface) area element is \(dA=(dx)(dy)=(dr)(rd\phi)\), as can easily be seen by drawing the appropriate small rectangle. The surface of a cylinder is nearly as easy, as it is given by \(r=a\) in cylindrical coordinates, and drawing a small "rectangle" yields for the surface element:</p>

<p>Cylinder: \(dA=(a d\phi)(dz)=a d\phi dz\)</p>

<p>While a similar construction for the sphere given by \(r=a\) in spherical coordinates yields:</p>

<p>Sphere: \(dA=(a d\theta)(a\sin\theta d\phi)=a^2\sin\theta d\theta d\phi\)</p>

<p>The last expression can of course be used to compute the surface area of a sphere, which is:</p>

<p>\(\int_{\text{sphere}}dA=\int_{0}^{2\pi}\int_{0}^{\pi}a^2\sin\theta d\theta d\phi=4\pi a^2\)</p>

<p>What about more complicated surfaces?</p>

<p>The basic building block for surface integrals is the infinitesimal area \(dA\), obtained by chopping up the surface into small pieces. If the pieces are small parallelograms, then the area can be determined by taking the cross product of the sides!</p>

<p>Note: We write a single integral sign when talking about adding up "bits of area" (or "bits of volume"), reserving multiple integral signs for iterated single integrals. The notation \(\int\int dA\) is also common.</p>

<h3 id="cross-prod">a bigger better cross product</h3>

<p>Tevian Dray and Corinne A. Manogue did fine on this, but there's also a great stack exchange answer about cross products and antisymmetry.</p>

<div class='card'><div class='card-body'>
    <p>There certainly is a connection! Other answers have shown that, of course, but it goes a little deeper than that: determinants and cross products are both based on antisymmetric linear combinations of permutations.</p>

<p><strong>Antisymmetry in permutations</strong></p>

<p>Suppose you have two things, \(a\) and \(b\). There are two ways to order them, i.e. two permutations:</p>

<p style="overflow: auto;">
\[
\begin{align*}
ab & ba
\end{align*}
\]
</p>

<p>Now, if these things can be multiplied and added/subtracted, you can combine these permutations in two distinctly different ways:</p>

<p style="overflow: auto;">
\[
\begin{align*}
ab + ba & ab - ba
\end{align*}
\]
</p>

<p>The first one is called <em>symmetric</em> because, if you exchange the two things, its value stays the same.</p>

<p style="overflow: auto;">
\[
ab + ba \underset{a\leftrightarrow b} \longrightarrow ba + ab = ab + ba
\]
</p>

<p>The second one is called <em>antisymmetric</em> because, if you exchange the two things, it becomes the negative of itself (hence "anti").</p>

<p style="overflow: auto;">
\[
ab - ba \underset{a\leftrightarrow b} \longrightarrow ba - ab = -(ab - ba)
\]
</p>

<p>If you add another thing \(c\) to the set, there are now six permutations:</p>

<p style="overflow: auto;">
\[
\begin{align*}
abc & acb & bca & bac & cab & cba
\end{align*}
\]
</p>

<p>Again, there's a <em>symmetric</em> way to combine these, where switching any two of the elements \(a\), \(b\), and \(c\) leaves the value unchanged:</p>

<p style="overflow: auto;">
\[
abc + acb + bac + bca + cab + cba
\]
</p>

<p>and there's a (totally<sup>1</sup>) <em>antisymmetric</em> way to combine them, where switching any two of \(a\), \(b\), and \(c\) turns it into the negative of the original value:</p>

<p style="overflow: auto;">
\[
abc - acb + bca - bac + cab - cba
\]
</p>

<p>(If you have a bit of time, I'd encourage you to check all three possible swaps and verify this.)</p>

<p>There are, of course, other ways to add and subtract the six permutations, but none of them are totally symmetric or totally antisymmetric. (If you have a bit more time, feel free to to check all the combinations.)</p>

<p>And while I won't get into the details here, the antisymmetric case is particularly interesting because even if you go beyond permutations to allow repeats like \(aaa\), there's <em>still</em> only one way to form a totally antisymmetric combination. This fact will be useful shortly.</p>

<p><strong>Cross products</strong></p>

<p>Now what does this have to do with cross products? Well, consider this: the "ingredients" that go into a cross product are three components of the first vector \((a_1, a_2, a_3)\), three components of the second vector \((b_1, b_2, b_3)\), and three unit vectors \(\hat{x}_1\), \(\hat{x}_2\), and \(\hat{x}_3\). If you want to make a product out of these things and have it not be "weird", hopefully it makes sense that it should probably involve multiplying a component of \(a\), a component of \(b\), and a unit vector.</p>

<p>So suppose you write out a generic formula for a product of these three things:</p>

<p style="overflow: auto;">
\[
a_i b_j \hat{x}_k,\quad i,j,k\in\{1,2,3\}
\]
</p>

<p>You have to choose an index (\(1\), \(2\), or \(3\)) for each of the component of \(a\), the component of \(b\), and the unit vector. Of course there are many different ways to make this choice, but there's <em>one</em> combination that will be totally antisymmetric:</p>

<p style="overflow: auto;">
\[
a_1 b_2 \hat{x}_3 - a_1 b_3 \hat{x}_2 + a_2 b_3 \hat{x}_1 - a_2 b_1 \hat{x}_3 + a_3 b_1 \hat{x}_2 - a_3 b_2 \hat{x}_1
\]
</p>

<p>That's a cross product. It's the unique totally antisymmetric linear combination of all possible terms that can be formed by multiplying one element of \(a\), one element of \(b\), and one unit vector without repeating indices.</p>

<p>If you think about it, it makes sense why you would want the cross product to be either totally symmetric or totally antisymmetric: if it weren't, then its value would change if you relabeled one dimension as another. You might have two vectors whose cross product is \((5, 3, 2)\) under regular coordinates, but if you changed your coordinate system to switch the first and second dimensions, without (anti)symmetry the cross product could have an entirely different value, like \((-1, 4, 1)\). A mathematical operation that depends on something totally unphysical like how you label your dimensions probably isn't very useful.</p>

<p><strong>Determinants</strong></p>

<p>Given that way of looking at a cross product, the determinant of a \(3\times 3\) matrix is almost trivially the same thing. Suppose you have this matrix:</p>

<p style="overflow: auto;">
\[
\begin{matrix}
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33}
\end{matrix}
\]
</p>

<p>If you choose set of three elements such that each set contains one element from each row and one element from each column, you get exactly six possible sets:</p>

<p style="overflow: auto;">
\[
(\{a_{11}, a_{22}, a_{33}\}, \{a_{11}, a_{23}, a_{32}\}, \{a_{12}, a_{23}, a_{31}\}, \{a_{12}, a_{21}, a_{33}\}, \{a_{13}, a_{21}, a_{32}\}, \{a_{13}, a_{22}, a_{31}\})
\]
</p>

<p>These sets, unsurprisingly correspond to the six permutations of \(\{1,2,3\}\). If you always choose the first index to be in numerical order, then the ways of choosing which second index corresponds to each first index are precisely the permutations. So you can multiply each set and form an antisymmetric linear combination of those products:</p>

<p style="overflow: auto;">
\[
a_{11}a_{22}a_{33} - a_{11}a_{23}a_{32} + a_{12}a_{23}a_{31} - a_{12}a_{21}a_{33} + a_{13}a_{21}a_{32} - a_{13}a_{22}a_{31}
\]
</p>

<p>That's a determinant.</p>

<p>It makes sense for the determinant to be either totally symmetric or totally antisymmetric for much the same reason as the cross product: a matrix of this form can represent some kind of transformation on 3D vectors, in which case the three indices correspond to the three dimensions of space, and a quantity which changes in a major way when you relabel which dimension is which probably won't be very useful.</p>

<hr>

<sup>1</sup><em>Totally antisymmetric</em> is the term to use when exchanging <em>any</em> two elements negates the expression. You can also have an expression which is <em>partially antisymmetric</em>, meaning that exchanging some pairs of elements reverses the sign, but not others. For example, in</p>

<p style="overflow: auto;">
\[
abc - acb + bca - bac - cab + cba
\]
</p>

<p>if you switch \(a\leftrightarrow b\), it negates the expression, but switching \(a\leftrightarrow c\) or \(b\leftrightarrow c\) does not. </p>
<p>Source: <a href="https://math.stackexchange.com/a/3737205/1098426">David Zaslavsky</a></p>
</div></div><br>

<p>
    The cross product is fundamentally a directed area. The magnitude of the cross product is defined to be the area of the parallelogram whose sides are the two vectors in the cross product.
    </p>
    <p>
    In the figure above, if the horizontal vector is \(\mathbf{v}\) and the upward-pointing vector is \(\mathbf{w}\), then the height of the parallelogram is \(\left|\mathbf{w}\right|\sin\theta\), so its area is
    \(\left|\mathbf{v} \times \mathbf{w}\right| = \left|\mathbf{v}\right|\left|\mathbf{w}\right|\sin\theta\) \((1)\)
    which is therefore the magnitude of the cross product.
    </p>
    <p>
    An immediate consequence of Equation \((1)\) is that, if two vectors are parallel, their cross product is zero,
    \(\mathbf{v} \parallel \mathbf{w} \implies \mathbf{v} \times \mathbf{w} = \mathbf{0}\) \((2)\)
    The direction of the cross product is given by the right-hand rule: Point the fingers of your right hand along the first vector (\(\mathbf{v}\)), and curl your fingers toward the second vector (\(\mathbf{w}\)). You may have to flip your hand over to make this work. Now stick out your thumb; that is the direction of \(\mathbf{v} \times \mathbf{w}\). In the example shown above, \(\mathbf{v} \times \mathbf{w}\) points out of the page. The right-hand rule implies that
    \(\mathbf{w} \times \mathbf{v} = -\mathbf{v} \times \mathbf{w}\) \((3)\)
    as you should verify for yourself by suitably positioning your hand. Thus, the cross product is not commutative. 1) Another important property of the cross product is that the cross product of a vector with itself is zero,
    \(\mathbf{v} \times \mathbf{v} = \mathbf{0}\) \((4)\)
    which follows from any of the preceding three equations.
    </p>
    <p>
    In terms of the standard orthonormal basis, the geometric formula quickly yields
   
    <p style="overflow: auto;">
    \[
    \begin{align*}
    \hat{x} \times \hat{y} &= \hat{z} \\
    \hat{y} \times \hat{z} &= \hat{x} \\
    \hat{z} \times \hat{x} &= \hat{y}
    \end{align*}
    \]
    </p>
    
    This cyclic nature of the cross product can be emphasized by abbreviating this multiplication table as shown in the figure below. 2)Products in the direction of the arrow get a plus sign; products against the arrow get a minus sign.
    </p>
    <p>
    Using an orthonormal basis such as \(\{\mathbf{x}, \mathbf{y}, \mathbf{z}\}\), the geometric formula reduces to the standard component form of the cross product. 3) If \(\mathbf{v} = v_x\mathbf{x} + v_y\mathbf{y} + v_z\mathbf{z}\) and \(\mathbf{w} = w_x\mathbf{x} + w_y\mathbf{y} + w_z\mathbf{z}\), then
    \(\mathbf{v} \times \mathbf{w} = (v_xw_y - v_yw_x)\mathbf{x} + (v_zw_x - v_xw_z)\mathbf{y} + (v_yw_z - v_zw_y)\mathbf{z}\) \((8)(9)\)
    which is often written as the symbolic determinant
    
    <p style="overflow: auto;">
    \[\mathbf{v} \times \mathbf{w} = \begin{vmatrix} \mathbf{x} & \mathbf{y} & \mathbf{z} \\ v_x & v_y & v_z \\ w_x & w_y & w_z \end{vmatrix} \quad (10)\]
    </p>
    
    We encourage you to use \((10)\), rather than simply memorizing \((9)\). We also encourage you to compute the determinant as described below, rather than using minors; this tends to minimize sign errors. A \(3 \times 3\) determinant can be computed in the form
    </p>
    <p style="overflow: auto;">
    \[
    \begin{vmatrix} a & b & c \\ d & e & f \\ g & h & i \end{vmatrix} = aei + bfg + cdh - ceg - bdi - afh
    \]
    </p>
    <p>
    where one multiplies the terms along each diagonal line, subtracting the products obtained along lines going down to the left from those along lines going down to the right. While this method works only for \( (2 \times 2\) and \(3 \times 3\) determinants, it emphasizes the cyclic nature of the cross product.
    </p>
    <p>
    Another important skill is knowing when not to use a determinant at all. For simple cross products, such as \((\mathbf{x} + 3\mathbf{y}) \times \mathbf{z}\), it is easier to use the multiplication table directly.
    </p>
    <p>
    It is also worth pointing out that the multiplication table and the determinant method generalize naturally to any (right-handed) orthonormal basis; all that is needed is to replace the rectangular basis \(\{\mathbf{x}, \mathbf{y}, \mathbf{z}\}\) by the one being used (in the right order!). For example, in cylindrical coordinates, not only is
    </p>
    <p style="overflow: auto;">
    \[\mathbf{r} \times \boldsymbol{\phi} = \mathbf{z} \quad (11)\]
        </p>
        <p>
    (and cyclic permutations), but cross products can be computed as
</p>
<p style="overflow: auto;">
    \[\mathbf{v} \times \mathbf{w} = \begin{vmatrix} \mathbf{r} & v_r & v_\phi & v_z \\ \mathbf{r} & w_r & w_\phi & w_z \\ \boldsymbol{\phi} & v_r & v_\phi & v_z \\ \boldsymbol{\phi} & w_r & w_\phi & w_z \\ \mathbf{z} & v_r & v_\phi & v_z \\ \mathbf{z} & w_r & w_\phi & w_z \end{vmatrix} \quad (12)\]
</p>
    <p>where of course \(\mathbf{v} = v_r\mathbf{r} + v_\phi\boldsymbol{\phi} + v_z\mathbf{z}\) and similarly for \(\mathbf{w}\).</p>
    </p>
    <p>
    A good problem emphasizing the geometry of the cross product is to find the area of the triangle formed by connecting the tips of the vectors \(\mathbf{x}\), \(\mathbf{y}\), \(\mathbf{z}\) (whose base is at the origin).
    </p>
    <p>
    1) The cross product also fails to be associative, since for example \(\mathbf{x} \times (\mathbf{x} \times \mathbf{y}) = -\mathbf{y}\) but \((\mathbf{x} \times \mathbf{x}) \times \mathbf{y} = \mathbf{0}\).
    </p>
    <p>
    2) This is really the multiplication table for the unit imaginary quaternions, a number system which generalizes the familiar complex numbers. Quaternions predate vector analysis, which borrowed the \(i\), \(j\), \(k\) notation for the rectangular basis vectors, which are often written as \(\mathbf{i}\), \(\mathbf{j}\), \(\mathbf{k}\). Here, we have adopted instead the more logical names \(\mathbf{x}\), \(\mathbf{y}\), \(\mathbf{z}\).
    </p>
    <p>
    3) This argument uses the distributive property, which must be proved geometrically if one starts with \((1)\) and the right-hand rule. This is straightforward in two dimensions, but somewhat more difficult in three dimensions.
    </p>
<h3 id="gen-surf">general surface</h3>
<p>It is getting real with Tevian Dray and Corinne A. Manogue.</p>
<p>
    Since surfaces are two-dimensional, chopping up a surface is usually done by drawing two families of curves on the surface.  Then you can compute \(d\mathbf{r}\) on each family and take the  cross product, to get the <em>vector surface element</em> in the form  \[
    d\mathbf{S} = d\mathbf{r}_1 \times d\mathbf{r}_2 \label{Surface} \tag{1}
    \]  In order to determine the area of the vector surface element, we need the magnitude of this expression, which is \[
    dS = \lvert d\mathbf{r}_1\times d\mathbf{r}_2 \rvert \label{Scalar} \tag{2}
    \]  and which is called the <em>(scalar) surface element</em>.  This should remind you of the corresponding expression for line integrals, namely \(ds=\lvert d\mathbf{r} \rvert\).
    </p>
    
    
    
    <p>
    We illustrate this technique by computing the surface element for the paraboloid given by \(z=x^2+y^2\), as shown in  Figure&nbsp;1, with the two families of curves corresponding to \(\{x=\text{const}\}\) and \(\{y=\text{const}\}\). We start with the basic formula for the vector differential \(d\mathbf{r}\), namely \[
    d\mathbf{r} = dx\,\mathbf{\hat{x}} + dy\,\mathbf{\hat{y}} + dz\,\mathbf{\hat{z}}
    \] What do you know?  The expression for \(z\) leads to \[
    dz = 2x\,dx + 2y\,dy
    \] In rectangular coordinates, it is natural to consider infinitesimal displacements in the \(x\) and \(y\) directions.  In the \(x\) direction, \(y\) is constant, so \(dy=0\), and we obtain \[
    d\mathbf{r}_1 = dx\,\mathbf{\hat{x}} + 2x\,dx\,\mathbf{\hat{z}} = (\mathbf{\hat{x}} + 2x\,\mathbf{\hat{z}})\, dx
    \] Similarly, in the \(y\) direction, \(dx=0\), which leads to \[
    d\mathbf{r}_2 = dy\,\mathbf{\hat{y}} + 2y\,dy\,\mathbf{\hat{z}} = (\mathbf{\hat{y}} + 2y\,\mathbf{\hat{z}})\, dy
    \] Putting this together, we obtain  \[
    d\mathbf{S}  =  d\mathbf{r}_1 \times d\mathbf{r}_2  = (\mathbf{\hat{x}} + 2x\,\mathbf{\hat{z}}) \times (\mathbf{\hat{y}} + 2y\,\mathbf{\hat{z}}) \,dx\,dy = (-2x\,\mathbf{\hat{x}} - 2y\,\mathbf{\hat{y}} + \mathbf{\hat{z}}) \,dx\,dy \label{parabolaR}
    \] for the vector surface element, and \[
    dS = \left|{-}2x\,\mathbf{\hat{x}} - 2y\,\mathbf{\hat{y}} + \mathbf{\hat{z}}\right| \,dx\,dy = \sqrt{1+4x^2+4y^2}\,dx\,dy
    \] for the scalar surface element.
    </p>
    
    <p>
    This construction emphasizes that “area” is really a <em>vector</em>, whose direction is perpendicular
    
     to the surface, and whose magnitude is the area. Note that there are always <em>two</em> choices for the direction; choosing one determines the <em>orientation</em> of the surface.
    </p>
    
    <p>
    When using&nbsp;\(\eqref{Surface}\) and&nbsp;\(\eqref{Scalar}\), it doesn't matter how you chop up the surface.  It is of course possible to get the opposite orientation, for instance by interchanging the roles of \(d\mathbf{r}_1\) and \(d\mathbf{r}_2\). Rather than worrying too much about getting the “right” orientation from the beginning, it is usually simpler to check after you've calculated \(d\mathbf{S}\) whether the orientations you've got agrees with the requirements of the problem.  If not, insert a minus sign.
    </p>
    
    <p>
    Just as a curve is a 1-dimensional set of points, a surface is 2-dimensional. When computing line integrals, it was necessary to write everything in terms of a single parameter before integrating.  Similarly, for surface integrals you must write everything, including the limits of integration, in terms of exactly <em>two</em> parameters before starting to integrate.
    </p>
    
    <p>
    Finally, a word about notation.  You will often see \(dS\) instead of \(dA\), and \(d\vec{S}\) instead of&nbsp;\(d\mathbf{S}\); most authors use \(dA\) in the \(xy\)-plane.
    </p>

    <h3 id="flux">flux</h3>
    <p>Another slay from Tevian Dray and Corinne A. Manogue.</p>
    <p></p>
    <p>
        At any given point along a curve, there is a natural vector, namely the (unit) tangent vector&nbsp;\(\mathbf{T}\).  Therefore, it is natural to add up the <em>tangential</em> component of a given vector field along a curve.  When the vector field represents force, this integral represents the work done by the force along the curve.  But there is no natural tangential direction at a point on a surface, or rather there are too many of them.  The natural vector at a point on a surface is the (unit) <em>normal</em> vector \(\mathbf{n}\), so on a surface it is natural to add up the normal component of a given vector field; this integral is known as the <em>flux</em> of the vector field through the surface.
        </p>
        
        <p>
        We already know that the vector surface element is given by \begin{equation} d\mathbf{S} = d\mathbf{r}_1 \times d\mathbf{r}_2 \end{equation} Since \(d\mathbf{r}_1\) and \(d\mathbf{r}_2\) are both tangent to the surface, \(d\mathbf{S}\) is perpendicular to the surface, and is therefore often written \begin{equation} d\mathbf{S} = \mathbf{n} \,\mathrm{d}S \end{equation} Putting this all together, the flux of a vector field \(\mathbf{F}\) through the surface is given by \begin{equation} \text{flux of \(\mathbf{F}\) through \(S\)} = \iint_S \mathbf{F} \cdot d\mathbf{S} \end{equation}
        </p>
        
        
        <p>
        We first consider a problem typical of those in calculus textbooks, namely finding the flux of the vector field \(\mathbf{F}=z\,\mathbf{\hat{z}}\) up through the part of the plane \(x+y+z=1\) lying in the first octant, as shown in   Figure&nbsp;1. We begin with the infinitesimal vector displacement in rectangular coordinates in 3 dimensions, namely \begin{equation} d\mathbf{r} = dx\,\mathbf{\hat{x}} + dy\,\mathbf{\hat{y}} + dz\,\mathbf{\hat{z}} \end{equation} A natural choice of curves on this surface is given by setting \(y\) or \(x\) constant, so that \(dy=0\) or \(dx=0\), respectively.  We thus obtain \begin{eqnarray} d\mathbf{r}_1 &=& dx\,\mathbf{\hat{x}} + dz\,\mathbf{\hat{z}} = (\mathbf{\hat{x}}-\mathbf{\hat{z}})\,dx \\ d\mathbf{r}_2 &=& dy\,\mathbf{\hat{y}} + dz\,\mathbf{\hat{z}} = (\mathbf{\hat{y}}-\mathbf{\hat{z}})\,dy \end{eqnarray} where we have used the equation of the plane to determine each expression in terms of a single parameter.  The surface element is thus <a id="triangle"></a> \begin{equation} d\mathbf{S} = d\mathbf{r}_1\times d\mathbf{r}_2 = (\mathbf{\hat{x}}+\mathbf{\hat{y}}+\mathbf{\hat{z}})\,dx\,dy \label{triangle} \end{equation} and the flux becomes    \begin{equation} \iint_S \mathbf{F}\cdot d\mathbf{S}   = \iint_S z \,\mathrm{d}x\,\mathrm{d}y   = \int_0^1 \int_0^{1-y} (1-x-y) \,\mathrm{d}x\,\mathrm{d}y   =\frac{1}{6} \end{equation} The limits were chosen by visualizing the projection of the surface into the \(xy\)-plane, which is a triangle bounded by the \(x\)-axis, the \(y\)-axis, and the line whose equation is \(x+y=1\).  Note that this latter equation is obtained from the equation of the surface by using what we know, namely that \(z=0\).
        </p>
        
        <p>
        Just as for line integrals, there is a rule of thumb which tells you when to stop using what you know to compute surface integrals: Don't start integrating until the integral is expressed in terms of <em>two</em> parameters, and the limits in terms of those parameters have been determined.  Surfaces are two-dimensional!
        </p>
        
        <p>
        Some readers will prefer to change the domain of integration in the second
        integral to be the projection of \(S\) into the \(xy\)-plane.  We prefer to
        integrate over the actual surface whenever possible.
        </p>
        <h3 id="symm">to symmetry or not to symmetry</h3>
        <p>Tevian Dray and Corinne A. Manogue pull out the double bind.</p>
        <p><strong>to symmetry</strong></p>
        <p>
            The electric field of a point charge \(q\) at the origin is given by \begin{equation} \mathbf{E} = \frac{q}{4\pi\epsilon_0} \frac{\mathbf{\hat{r}}}{r^2} = \frac{q}{4\pi\epsilon_0}  \frac{x\,\mathbf{\hat{x}}+y\,\mathbf{\hat{y}}+z\,\mathbf{\hat{z}}}{(x^2+y^2+z^2)^{3/2}} \end{equation} where \(\mathbf{\hat{r}}\) is the unit vector in the radial direction in <em>spherical</em> coordinates.  Note that the first expression clearly indicates both the spherical symmetry of \(\mathbf{E}\) and its \(\frac{1}{r^2}\) fall-off behavior, while the second expression does neither.
            </p>
            
            <p>
            It is easy to determine \(d\mathbf{S}\) on the sphere by inspection; we nevertheless go through the details of the differential approach for this case.  We use &ldquo;physicists' conventions&rdquo; for spherical coordinates, so that \(\theta\) is the angle from the North Pole, and \(\phi\) the angle in the \(xy\)-plane.  We use the obvious families of curves, namely the lines of latitude and longitude. Starting either from the general formula for \(d\mathbf{r}\) in spherical coordinates, namely \begin{equation} d\mathbf{r} = dr\,\mathbf{\hat{r}} + r\,d\theta\,\mathbf{\hat{\theta}} + r\sin\theta\,d\phi\,\mathbf{\hat{\phi}} \end{equation} or directly using the geometry behind that formula, one quickly arrives at \begin{eqnarray} d\mathbf{r}_1 &=& r\,d\theta\,\mathbf{\hat{\theta}} \\ d\mathbf{r}_2 &=& r\sin\theta\,d\phi\,\mathbf{\hat{\phi}} \\ d\mathbf{S} &=& d\mathbf{r}_1 \times d\mathbf{r}_2 = r^2\sin\theta\,d\theta\,d\phi\,\mathbf{\hat{r}} \end{eqnarray} so that \begin{equation} \iint_S \mathbf{E}\cdot d\mathbf{S} = \int_0^{2\pi} \int_0^\pi \frac{q}{4\pi\epsilon_0} \frac{\mathbf{\hat{r}}}{r^2}  \cdot r^2 \sin\theta\,d\theta\,d\phi \,\mathbf{\hat{r}} = \frac{q}{\epsilon_0} \label{Gauss0} \end{equation}  You may recognize this result as <em>Gauss' Law</em>, which says that the equation above gives the relationship between the total charge \(q\) inside  <em>any</em> closed surface \(S\) and the flux of the electric field through the surface.
            </p>
            <p><strong>not to symmetry</strong></p>
            <p>
                The reader may have the feeling that two quite different languages are being spoken here. 
                While the "use what you know" strategy may be somewhat unfamiliar, the basic idea should not be. On the other hand highly symmetric examples will be quite unfamiliar to most mathematicians, due to their use of adapted basis vectors such as \(\mathbf{\hat{r}}\). Mastering these examples helps develop experience looking for symmetry and making geometric arguments. At the same time, not all problems have symmetry!
                </p>
                
                <p>
                We argue, however, that the approach being presented here is much more flexible than may appear at first sight. We demonstrate this flexibility by integrating over a paraboloid, the classic example found in calculus textbooks.
                </p>
                
                <p>
                We compute the flux of the axially symmetric vector field \(\mathbf{F} = r\,\mathbf{\hat{r}} = x\,\mathbf{\hat{x}} + y\,\mathbf{\hat{y}}\) "outwards" through the part of the paraboloid \(z=r^2\) lying below the plane \(z=4\). The first thing we need is the formula for \(d\mathbf{r}\) in cylindrical coordinates, which is a straightforward generalization  in polar coordinates, namely \begin{equation} d\mathbf{r} = dr\,\mathbf{\hat{r}} + r\,d\phi\,\mathbf{\hat{\phi}} + dz\,\mathbf{\hat{z}} \label{dr3} \end{equation}
                </p>
                <h3 id=""></h3>
        <p>Tevian Dray and Corinne A. Manogue</p>