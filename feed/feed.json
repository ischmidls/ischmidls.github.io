{
  "version": "https://jsonfeed.org/version/1",
  "title": "izak",
  "home_page_url": "https://ischmidls.github.io",
  "feed_url": "https://update-me.com/feed/feed.json",
  "description": "a tip of an iceberg",
  "author": {
    "name": "izak",
    "url": ""
  },
  "items": [{
      "id": "https://ischmidls.github.io/posts/pages/linearsix/",
      "url": "https://ischmidls.github.io/posts/pages/linearsix/",
      "title": "six linear algebra theorems",
      "content_html": "<!doctype html>\r\n<html lang=\"en\" dir=\"ltr\">\r\n  <head>\r\n    <meta charset=\"utf-8\">\r\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\r\n    <link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css\" integrity=\"sha384-B0vP5xmATw1+K9KRQjQERJvTumQW0nPEzvF6L/Z6nronJ3oUOFUFpCjEUQouq2+l\" crossorigin=\"anonymous\">\r\n    <!--<link rel=\"stylesheet\" href=\"css/main.css\">-->\r\n    <title>Linear Algebra Theorems</title>\r\n    <script type=\"text/javascript\" id=\"MathJax-script\" async src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"></script>\r\n    <style>\r\n    .card-body {overflow-x: auto}\r\n    .MathJax_Display, .MJXc-display, .MathJax_SVG_Display, .mjx-container[display=\"true\"] {\r\n    overflow-x: auto;\r\n    overflow-y: hidden;\r\n    }\r\n    </style>\r\n  </head><body>\r\n    <div class=\"container\" id=\"bsr-wrapper\">\r\n      <!-- <div style=\"background-color: coral;\">Under construction 6/19/23-...</div> -->\r\n      <h1>Linear Algebra Theorems</h1>\r\n      <p>Izak, June 2023</p>\r\n      <!-- <p class=\"text-secondary\">Estimated reading time: ~25 min. <br> Approx. word count: ~4,100 words <br> Approx math terms: ~600 LaTeX blocks</p> -->\r\n      <p>The six central theorems of linear algebra come from Gilbert Strang's <strong>Introduction to Linear Algebra, 5th Ed</strong>, and I have provided an example for each. </p>\r\n      <p class=\"pl-5\"><em>Note: some HTML was started with ChatGPT, but LLM's cannot do math or even consistent <a href=\"https://www.latex-project.org/\">LaTeX</a>, so much of the math was checked via <a href=\"https://www.wolframalpha.com/\">Wolfram Alpha</a>, and proofs were checked via <a href=\"https://math.stackexchange.com/\">Math Stack Exchange</a>.</em> The CSS requires <a href=\"https://getbootstrap.com/\">Bootstrap</a>, and the LaTeX requires <a href=\"https://www.mathjax.org/\">MathJax</a>, so this page is best viewed with internet connection &#9825;. Finally, Prof. Strang uses \"nullspace\", where I use \"kernel\" &mdash; they're synonyms, but \"kernel\" is <a href=\"https://math.stackexchange.com/a/235353/1098426\">often used outside</a> of linear algebra too.</p>\r\n      <!-- <p>GPT's failure to do matrix operations, to properly parse LaTeX, etc. helped inspire me to do something an AI cannot replace yet. While Prof. Strang is more rigorous than (for example) 3b1b, I also wanted to bring my own exprience to the table. This latter part means that this page is a mix of somewhat cryptic categorical syntax, contrived examples, Math Stack Exchange insight, and the like. </p>\r\n      <p>It seems DeepMind researchers can train AlphaTensor to multiply matrices, but ChatGPT remains little more than an HTML generator.</p>\r\n      <p>\"Why the <em>needless</em> commentary?\" you, dear reader, may ask. Well, you might consider Axler's <em>Linear Algebra Done Right</em> or Hemmingway's <em>The Old Man and the Sea</em> if you prefer succinct prose. This page also has grown increasingly unwieldy with the explorations of each theorem, numbering hundreds of lines of HTML alone. A few lines of dialogue never hurt anyone &mdash; other than Shakespearean casts, & so on. Enjoy. </p> -->\r\n      <ul>\r\n        <li>\r\n          <a href=\"#dimension-theorem\">Dimension Theorem</a>\r\n        </li>\r\n        <li>\r\n          <a href=\"#counting-theorem\">Counting Theorem</a>\r\n        </li>\r\n        <li> \r\n          <a href=\"#rank-theorem\">Rank Theorem</a>\r\n        </li>\r\n        <li>\r\n          <a href=\"#fundamental-theorem\">Fundamental Theorem</a>\r\n        </li>\r\n        <li>\r\n          <a href=\"#singular-value-decomposition\">Singular Value Decomposition</a>\r\n        </li>\r\n        <li>\r\n          <a href=\"#spectral-theorem\">Spectral Theorem</a>\r\n        </li>\r\n        <li>\r\n          <a href=\"#nutshell\">Nutshell</a>\r\n        </li>\r\n      </ul>\r\n      <hr>\r\n      <h2 id=\"dimension-theorem\">Dimension Theorem</h2>\r\n      <p>All bases for a vector space have the same number of vectors.</p>\r\n      <p>Mathematically: \\( \\text{dim}(V) = \\text{dim}(W) \\) for any bases \\( V \\) and \\( W \\) of the vector space.</p>\r\n      <div>\r\n        <p>\r\n          <strong>Example:</strong>\r\n        </p>\r\n        <p>This is sort of a boring example, but the available proofs give an idea for how nuanced the mechanisms underlying this theorem are. See some discussion here: <a href=\"https://math.stackexchange.com/q/4595743/1098426\">Dimension Theorem discussion</a>\r\n        </p>\r\n        <p>Let's consider a vector space \\(V = \\mathbb{R}^2\\) over the field \\(F = \\mathbb{R}\\) (the set of real numbers). In this case, vectors in \\(V\\) are ordered pairs \\((x, y)\\) where \\(x\\) and \\(y\\) are real numbers.</p>\r\n        <p>Now, let's find two different bases for \\(V\\) and observe that they have the same number of vectors.</p>\r\n        <p>\r\n          <em>Basis 1:</em>\r\n        </p>\r\n        <p>We can choose the following two vectors as a basis for \\(V\\):</p>\r\n        <p>\\(\\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\)</p>\r\n        <p>\\(\\mathbf{v}_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\)</p>\r\n        <p>These vectors are linearly independent (meaning no non-trivial linear combination of them yields the zero vector) and span the entire vector space \\(V\\) &mdash; that is, \\(\\forall v \\in V \\ \\exists \\ \\lambda_1 , \\ \\lambda_2 \\in F \\ | \\ v= \\lambda_1 v_1 + \\lambda_2 v_2 \\)</p>\r\n        <p>\r\n          <em>Basis 2:</em>\r\n        </p>\r\n        <p>Alternatively, we can choose the following two vectors as another basis for \\(V\\):</p>\r\n        <p>\\(\\mathbf{u}_1 = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}\\)</p>\r\n        <p>\\(\\mathbf{u}_2 = \\begin{bmatrix} -1 \\\\ 3 \\end{bmatrix}\\)</p>\r\n        <p>Again, these vectors are linearly independent and span the entire vector space \\(V\\).</p>\r\n        <p>Both Basis 1 and Basis 2 consist of two vectors each. This example demonstrates that all bases (ok, at least <em>two</em> bases) for \\(V\\) have the same number of vectors, which in this case is 2. This property holds true for any vector space, indicating that the number of vectors in a basis is a fundamental characteristic of the vector space itself. </p>\r\n      </div>\r\n      <hr>\r\n      <h2 id=\"counting-theorem\">Counting Theorem</h2>\r\n      <p>Dimension of column space + dimension of kernel = number of columns.</p>\r\n      <p>Mathematically: \\( \\text{dim}(\\text{col}(A)) + \\text{dim}(\\text{ker}(A)) = \\text{cols}(A) \\)</p>\r\n      <div>\r\n        <p>\r\n          <strong>Example:</strong>\r\n        </p>\r\n        <p>Consider the matrix:</p>\r\n        <p>\\[ A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\\\ \\end{bmatrix} \\]</p>\r\n        <p>Let's calculate the dimension of the column space (step 1) and the dimension of the kernel (step 2) of \\(A\\), and verify the theorem.</p>\r\n        <p>\r\n          <em>Solution:</em>\r\n        </p>\r\n        <p>STEP 1: To find the column space of \\(A\\), we reduce \\(A\\) to echelon form:</p>\r\n        <p>\\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\\\ \\end{bmatrix} \\xrightarrow{\\text{Row operations}} \\begin{bmatrix} 1 & 0 & -1 \\\\ 0 & 1 & 2 \\\\ 0 & 0 & 0 \\\\ \\end{bmatrix} \\]</p>\r\n        <div class=\"card\">\r\n          <div class=\"card-body\">\r\n            <p> Do row reduction: \\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\\\ \\end{bmatrix} \\] </p>\r\n            <p>Subtract a multiple of one row from another. <br> Subtract \\(4 \\times\\) (row 1) from row 2: <br> \\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & -3 & -6 \\\\ 7 & 8 & 9 \\\\ \\end{bmatrix} \\] </p>\r\n            <p>Subtract a multiple of one row from another. <br> Subtract \\(7 \\times\\) (row 1) from row 3: <br> \\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & -3 & -6 \\\\ 0 & -6 & -12 \\\\ \\end{bmatrix} \\] </p>\r\n            <p>Swap two rows. <br> Swap row 2 with row 3: <br> \\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & -6 & -12 \\\\ 0 & -3 & -6 \\\\ \\end{bmatrix} \\] </p>\r\n            <p>Subtract a multiple of one row from another. <br> Subtract \\(\\frac{1}{2} \\times\\) (row 2) from row 3: <br> \\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & -6 & -12 \\\\ 0 & 0 & 0 \\\\ \\end{bmatrix} \\] </p>\r\n            <p>Divide row 2 by a scalar. <br> Divide row 2 by -6: <br> \\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 1 & 2 \\\\ 0 & 0 & 0 \\\\ \\end{bmatrix} \\] </p>\r\n            <p>Subtract a multiple of one row from another. <br> Subtract \\(2 \\times\\) (row 2) from row 1: <br> \\[ \\begin{bmatrix} 1 & 0 & -1 \\\\ 0 & 1 & 2 \\\\ 0 & 0 & 0 \\\\ \\end{bmatrix} \\] </p>\r\n            <p>Verify matrix is reduced. <br> This matrix is now in reduced row echelon form. <br> All nonzero rows are above rows of all zeros: <br> \\[ \\begin{bmatrix} 1 & 0 & -1 \\\\ 0 & 1 & 2 \\\\ 0 & 0 & 0 \\\\ \\end{bmatrix} \\] </p>\r\n            <p>Verify pivots and their positions. <br> Each pivot is 1 and is strictly to the right of every pivot above it: <br> \\[ \\begin{bmatrix} 1 & 0 & -1 \\\\ 0 & 1 & 2 \\\\ 0 & 0 & 0 \\\\ \\end{bmatrix} \\] </p>\r\n            <sub>Checked with Wolfram &#9825;</sub>\r\n          </div>\r\n        </div>\r\n        <br>\r\n        <p>The pivot columns are the first two columns, and they form a basis for the column space of \\(A\\). So, the dimension of the column space is \\(2\\).</p>\r\n        <p>STEP 2: Now, let's find the kernel of \\(A\\) by solving the homogeneous equation \\(A\\mathbf{x} = \\mathbf{0}\\):</p>\r\n        <p>\\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\\\ \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ \\end{bmatrix} \\]</p>\r\n        <div class=\"card\">\r\n          <div class=\"card-body\">\r\n            <p>The kernel of a matrix \\(M\\) is the set of solutions \\(v\\) to the homogeneous equation \\(M \\cdot v = 0\\). <br> The kernel of matrix \\(M = \\begin{bmatrix} 1 & 0 & -1 \\\\ 0 & 1 & 2 \\\\ 0 & 0 & 0 \\end{bmatrix}\\) is the set of all vectors \\(v = (x_1, x_2, x_3)\\) such that \\(M \\cdot v = 0\\): <br> \\(\\begin{bmatrix} 1 & 0 & -1 \\\\ 0 & 1 & 2 \\\\ 0 & 0 & 0 \\end{bmatrix} \\cdot \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\\) </p>\r\n            <p>Identify free variables. <br> Free variables in the kernel \\((x_1, x_2, x_3)\\) correspond to the columns in \\(\\begin{bmatrix} 1 & 0 & -1 \\\\ 0 & 1 & 2 \\\\ 0 & 0 & 0 \\end{bmatrix}\\) which have no pivot. <br> Column 3 is the only column with no pivot, so we may take \\(x_3\\) to be the only free variable. </p>\r\n            <p>Perform matrix multiplication. <br> Multiply out the reduced matrix \\(\\begin{bmatrix} 1 & 0 & -1 \\\\ 0 & 1 & 2 \\\\ 0 & 0 & 0 \\end{bmatrix}\\) with the proposed solution vector \\((x_1, x_2, x_3)\\): <br> \\(\\begin{bmatrix} 1 & 0 & -1 \\\\ 0 & 1 & 2 \\\\ 0 & 0 & 0 \\end{bmatrix} \\cdot \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} = \\begin{bmatrix} x_1 - x_3 \\\\ x_2 + 2x_3 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\\) </p>\r\n            <p>Convert to a system and solve in terms of the free variables. <br> Solve the equations \\(x_1 - x_3 = 0\\), \\(x_2 + 2x_3 = 0\\), and \\(0 = 0\\) for \\(x_1\\) and \\(x_2\\): <br> \\(\\{x_1 = x_3, x_2 = -2x_3, 0 = 0\\) for \\(x_1\\) and \\(x_2 \\}\\) </p>\r\n            <p>Replace the pivot variables with free variable expressions. <br> Rewrite \\(v\\) in terms of the free variable \\(x_3\\), and assign it an arbitrary real value of \\(x\\): <br> \\(v = (x_1, x_2, x_3) = (x_3, -2x_3, x_3) = (x, -2x, x)\\) for \\(x \\in \\mathbb{R}\\) </p>\r\n            <p> Rewrite the solution vector \\(v = (x, -2x, x)\\) in set notation: <br> Answer: \\(\\{(x, -2x, x) : x \\in \\mathbb{R}\\}\\) </p>\r\n            <sub>Checked with Wolfram &#9825;</sub>\r\n          </div>\r\n        </div>\r\n        <br>\r\n        <p>Thus, the <em>nullity</em> &mdash;or dimension of the kernel&mdash; is one, \\( \\text{dim}(\\text{ker}(A)) = 1 \\) </p>\r\n        <p>Now, by the theorem, the dimension of the column space plus the dimension of the kernel should be equal to the number of columns, \\( \\text{dim}(\\text{col}(A)) + \\text{dim}(\\text{ker}(A)) = \\text{cols}(A) \\)</p>\r\n        <p>Here, \\( \\text{dim}(\\text{col}(A)) = 2 \\) and \\( \\text{dim}(\\text{ker}(A)) = 1 \\) so \\( \\text{cols}(A) = 2 + 1 = 3 \\). The number of columns in \\(A\\) is also \\(3\\).</p>\r\n      </div>\r\n      <hr>\r\n      <h2 id=\"rank-theorem\">Rank Theorem</h2>\r\n      <p>Dimension of column space = dimension of row space.</p>\r\n      <p>Mathematically: \\( \\text{dim}(\\text{col}(A)) = \\text{dim}(\\text{row}(A)) \\)</p>\r\n      <div>\r\n        <p>\r\n          <strong>Example:</strong>\r\n        </p>\r\n        <p>Consider the matrix:</p>\r\n        <p>\\[ A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\\\ \\end{bmatrix} \\]</p>\r\n        <p>Let's calculate the dimensions of the column space and the row space of \\(A\\), and verify the theorem.</p>\r\n        <p>\r\n          <em>Solution:</em>\r\n        </p>\r\n        <p>First, consider the column space and then, second, consider the row space.</p>\r\n        <p>For both the column and row spaces, we reduce \\(A\\) to echelon form exactly as in step 1 of the example for the <em>Counting Theorem</em>: </p>\r\n        <p>\\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\\\ \\end{bmatrix} \\xrightarrow{\\text{Row operations}} \\begin{bmatrix} 1 & 0 & -1 \\\\ 0 & 1 & 2 \\\\ 0 & 0 & 0 \\\\ \\end{bmatrix} \\]</p>\r\n        <!-- Rank Theorem steps -->\r\n        <p>For the column space: the pivot columns of this matrix \\(A\\) are the first column and the second column &mdash; where \\(1\\) is the only nonzero element of the pivot columns in row-reduced echelon form \\(\\text{rref}(A)\\).</p>\r\n        <p>Thus, the second and first columns of the orignial matrix \\( \\begin{bmatrix} 1 & 4 & 7 \\\\ \\end{bmatrix} \\) and \\( \\begin{bmatrix} 2 & 5 & 8 \\\\ \\end{bmatrix} \\) form a basis for the column space of \\(A\\). So, the dimension of the column space is \\( \\text{dim}(\\text{col}(A)) = 2 \\).</p>\r\n        <p>The row space comes from the non-zero rows of the row-reduced echelon matrix. The row space of \\(A\\) is spanned by \\( \\begin{bmatrix} 1 & 0 & -1 \\\\ \\end{bmatrix} \\) and \\( \\begin{bmatrix} 0 & 1 & 2 \\\\ \\end{bmatrix} \\), so they form a basis for the row space of \\(A\\), and the dimension of the row space is \\( \\text{dim}(\\text{row}(A)) = 2 \\).</p>\r\n        <p>We see \\( \\text{dim}(\\text{col}(A)) = \\text{dim}(\\text{row}(A)) = 2 \\). <br>\r\n          <b>Q.E.D.</b>\r\n        </p>\r\n        <p>That ends the example, but some rigorous (albiet a tad daunting) proofs are here: <a href=\"https://math.stackexchange.com/q/1900437/1098426\">Rank Theorem Proofs</a>\r\n        </p>\r\n      </div>\r\n      <hr>\r\n      <!-- FUNDAMENTAL THEOREM -->\r\n      <h2 id=\"fundamental-theorem\">Fundamental Theorem</h2>\r\n      <p>The row space and kernel of \\( A \\) are orthogonal complements in \\( \\mathbb{R}^n \\).</p>\r\n      <p>Mathematically: \\( \\text{row}(A) \\perp \\text{ker}(A) \\) in \\( \\mathbb{R}^n \\)</p>\r\n      <p>\r\n        <strong>Example:</strong>\r\n      </p>\r\n      <p>Consider the matrix:</p>\r\n      <p>\\[ A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\\\ \\end{bmatrix} \\]</p>\r\n      <p>Let's calculate the row space (step 1) and the kernel (step 2) of \\(A\\) and verify that they are orthogonal complements in \\(\\mathbb{R}^n\\).</p>\r\n      <p>\r\n        <em>Solution:</em>\r\n      </p>\r\n      <div>\r\n        <p>This one requires a bit more thinking outside of the row reduction algorithm</p>\r\n        <p> Recall from <em>The Counting Theorem</em>, the kernel has dimension equal to the difference between the number of columns and the dimension of the row space, \\(\\text{col}(A) - \\text{dim}(\\text{row}(A))\\). It follows that the orthogonal complement of the kernel has dimension \\(\\text{col}(A) - (\\text{col}(A) - \\text{dim}(\\text{row}(A)))=\\text{dim}(\\text{row}(A))\\). Now, the row space is an \\(r\\) dimensional subspace of the orthogonal complement of the kernel, which in turn has dimension \\(r\\). The only \\(r\\) dimensional subspace of an \\(r\\)-dimensional space is the entirety of the space itself, \\( A \\subseteq B, \\ \\text{dim}(A)=\\text{dim}(B) \\vdash B \\subseteq A, \\ A=B\\). So, the row-space is not only a subspace of the orthogonal complement but comprises the entirety of the orthogonal complement. </p>\r\n      </div>\r\n      <p>Also Recall from <em>The Counting Theorem</em>, the kernel is \\(\\{(x, -2x, x) : x \\in \\mathbb{R}\\}\\). </p>\r\n      <p>Similarly, from the row reduction of the matrix, we know that the basis for the row space is \\( \\{ \\begin{bmatrix} 1 & 0 & -1 \\end{bmatrix}, \\ \\begin{bmatrix} 0 & 1 & 2 \\end{bmatrix} \\} \\) </p>\r\n      <p>Now, orthogonal vectors have an inner product equal to zero \\( x^T y = y^T x = 0\\). Spaces are orthogonal when every vector in one is orthogonal to every vector in the other.\r\n      <p>How to check that \\( \\forall n \\in \\{(x, -2x, x) : x \\in \\mathbb{R}\\} \\) and \\( \\forall r \\in \\) the <em>row space</em> which is in some ways more abstract: </p>\r\n      <p>\r\n        <em>For a vector space \\(V\\), a family in \\(V\\) consists of a set \\(I\\) together with a function \\(e: I \\rightarrow V\\). A basis of \\(V\\) is a family \\((I, e)\\) in \\(V\\) such that for all \\(x \\in V\\) there exists a unique finitely-supported function \\(a: I \\rightarrow \\mathbb{R}\\) satisfying \\(x = \\sum_{i \\in I} a_i e_i \\ \\) as well as conditions for well ordering</em> (See <a href=\"https://math.stackexchange.com/a/1898250/1098426\">this definition's source</a> for further context on well ordering and matrices).\r\n      </p>\r\n      <p>And that is not even considering <em>order</em>, which is an essential part of elimination on matrices. All the same, the definition can be even more intuitive if simply considered \\(x = \\sum a e\\) to mean all the linear combinations of \\( \\begin{bmatrix} 1 & 0 & -1 \\end{bmatrix} \\) and \\( \\begin{bmatrix} 0 & 1 & 2 \\end{bmatrix}\\). </p>\r\n      <p>Even more intuitively, the kernel can be seen as a line or a one dimensional subspace of \\( \\mathbb{R}^3 \\), the row space a plane or two dimensional subspace of \\( \\mathbb{R}^3 \\).</p>\r\n      <p>Thus, the normal vector to \\(\\text{row}(A)\\) is \\( \\begin{bmatrix} 1 & 0 & -1 \\end{bmatrix} \\times \\begin{bmatrix} 0 & 1 & 2 \\end{bmatrix} \\)</p>\r\n      <div class=\"card\">\r\n        <div class=\"card-body\">\r\n          <p>Compute the following cross product:</p>\r\n          <p>\\((1, 0, -1) \\times (0, 1, 2)\\)</p>\r\n          <p>Create a matrix out of the vectors \\((1, 0, -1)\\) and \\((0, 1, 2)\\) along with the unit vectors \\(\\hat{i}\\), \\(\\hat{j}\\), and \\(\\hat{k}\\).</p>\r\n          <p>Construct a matrix where the first row contains unit vectors \\(\\hat{i}\\), \\(\\hat{j}\\), and \\(\\hat{k}\\); and the second and third rows are made of vectors \\((1, 0, -1)\\) and \\((0, 1, 2)\\):</p>\r\n          <p>\\[ \\begin{bmatrix} \\hat{i} & \\hat{j} & \\hat{k} \\\\ 1 & 0 & -1 \\\\ 0 & 1 & 2 \\\\ \\end{bmatrix} \\]</p>\r\n          <p>The cross product of the vectors \\((1, 0, -1)\\) and \\((0, 1, 2)\\) is the determinant of the matrix:</p>\r\n          <p>\\[ \\begin{vmatrix} \\hat{i} & \\hat{j} & \\hat{k} \\\\ 1 & 0 & -1 \\\\ 0 & 1 & 2 \\\\ \\end{vmatrix} \\]</p>\r\n          <p>Take the determinant of this matrix:</p>\r\n          <p>\\(\\begin{vmatrix} \\hat{i} & \\hat{j} & \\hat{k} \\\\ 1 & 0 & -1 \\\\ 0 & 1 & 2 \\end{vmatrix}\\)</p>\r\n          <p>Find an optimal row or column to use for Laplace's expansion.</p>\r\n          <p>Expand with respect to row 1:</p>\r\n          <p>The determinant of the matrix \\(\\begin{bmatrix} a_{1,1} & a_{1,2} & a_{1,3} \\\\ a_{2,1} & a_{2,2} & a_{2,3} \\\\ a_{3,1} & a_{3,2} & a_{3,3} \\end{bmatrix}\\) is given by \\(\\sum_{j=1}^{3}(-1)^{1+j}a_{1,j}M_{1,j}\\) where \\(M_{i,j}\\) is the determinant of the matrix obtained by removing row \\(i\\) and column \\(j\\).</p>\r\n          <p>The determinant of the matrix \\(\\begin{bmatrix} \\hat{i} & \\hat{j} & \\hat{k} \\\\ 1 & 0 & -1 \\\\ 0 & 1 & 2 \\end{bmatrix}\\) is given by \\(\\hat{i}\\begin{vmatrix} 0 & -1 \\\\ 1 & 2 \\end{vmatrix} + (-\\hat{j})\\begin{vmatrix} 1 & -1 \\\\ 0 & 2 \\end{vmatrix} + \\hat{k}\\begin{vmatrix} 1 & 0 \\\\ 0 & 1 \\end{vmatrix}\\):</p>\r\n          <p>\\(=\\hat{i}\\begin{vmatrix} 0 & -1 \\\\ 1 & 2 \\end{vmatrix} + (-\\hat{j})\\begin{vmatrix} 1 & -1 \\\\ 0 & 2 \\end{vmatrix} + \\hat{k}\\begin{vmatrix} 1 & 0 \\\\ 0 & 1 \\end{vmatrix}\\)</p>\r\n          <p>The determinant of the matrix \\(\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\\) is given by \\(ad - bc\\).</p>\r\n          <p>\\(\\hat{i}\\begin{vmatrix} 0 & -1 \\\\ 1 & 2 \\end{vmatrix} = \\hat{i}\\)</p>\r\n          <p>\\(=(-\\hat{j})\\begin{vmatrix} 1 & -1 \\\\ 0 & 2 \\end{vmatrix} + \\hat{k}\\begin{vmatrix} 1 & 0 \\\\ 0 & 1 \\end{vmatrix}\\)</p>\r\n          <p>Compute the determinant of the matrix \\(\\begin{bmatrix} 1 & -1 \\\\ 0 & 2 \\end{bmatrix}\\) and multiply the result by \\(-\\hat{j}\\).</p>\r\n          <p>\\((-\\hat{j})\\begin{vmatrix} 1 & -1 \\\\ 0 & 2 \\end{vmatrix} = -2\\hat{j}\\)</p>\r\n          <p>\\(=\\hat{i} - 2\\hat{j} + \\hat{k}\\begin{vmatrix} 1 & 0 \\\\ 0 & 1 \\end{vmatrix}\\)</p>\r\n          <p>Compute the determinant of the matrix \\(\\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\\) and multiply the result by \\(\\hat{k}\\).</p>\r\n          <p>\\(\\hat{k}\\begin{vmatrix} 1 & 0 \\\\ 0 & 1 \\end{vmatrix} = \\hat{k}\\)</p>\r\n          <p>\\(=\\hat{i} - 2\\hat{j} + \\hat{k}\\)</p>\r\n          <p>Collect the coefficients of \\(\\hat{i}\\), \\(\\hat{j}\\), and \\(\\hat{k}\\) into a vector ordered as \\((\\hat{i}, \\hat{j}, \\hat{k})\\).</p>\r\n          <p>\\(\\hat{i} - 2\\hat{j} + \\hat{k} = (1, -2, 1)\\)</p>\r\n          <sub>Checked with Wolfram &#9825;</sub>\r\n        </div>\r\n      </div>\r\n      <br>\r\n      <p>We see the normal of \\( \\text{row}(A)\\) is \\( \\hat{s} = \\begin{bmatrix} 1 & -2 & 1 \\end{bmatrix} \\). Now, it is simple to see that \\( \\text{ker}(A) \\) has direction vector \\( \\hat{s} = \\begin{bmatrix} 1 & -2 & 1 \\end{bmatrix} \\) (i.e. factor out \\(x\\)).</p>\r\n      <p>Now, proving \\( \\text{row}(A) \\perp \\text{ker}(A) \\) in \\( \\mathbb{R}^n \\) is proving that the normal vector \\( \\hat{n} \\) of \\( \\text{row}(A) \\) is parallel to the direction vector \\( \\hat{s} \\) of \\( \\text{ker}(A) \\).</p>\r\n      <p>This parallelism, again, is true if \\( \\hat{n} \\times \\hat{s} = 0 \\). But, we do not need to go this far because out results have yielded the same vector.</p>\r\n      <p>That is, \\( ( \\hat{n} = \\hat{s} ) \\Rightarrow ( \\hat{n} \\times \\hat{s} = 0 ) \\), and \\( ( \\hat{n} \\times \\hat{s} = 0 ) \\Rightarrow (\\hat{n} \\parallel \\hat{s}) \\), so finally, \\( \\hat{n} \\parallel \\hat{s} \\Rightarrow (\\text{row}(A) \\perp \\text{ker}(A)) \\).</p>\r\n      <hr>\r\n      <!-- SINGULAR VALUE DECOMPOSITION-->\r\n      <h2 id=\"singular-value-decomposition\">Singular Value Decomposition</h2>\r\n      <p>This is where the computations become quite intense.</p>\r\n      <p>There are orthonormal bases (\\( v \\)'s and \\( u \\)'s) for the row and column spaces so that \\( Av_i = \\sigma_iu_i \\).</p>\r\n      <p>Mathematically: \\( A = US V^T \\) where \\( U \\) and \\( V \\) are orthonormal matrices, and \\( S \\) is a diagonal matrix of singular values.</p>\r\n      <p>A concise explanation (adapted from an <a href=\"https://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm\">MIT bio engineering tutorial</a>):</p>\r\n      <div class=\"pl-5\">\r\n      <p>\r\n        Singular value decomposition takes a rectangular matrix(defined as \\( A \\), where \\( A \\)\r\n        is an \\( n \\times p \\) matrix) with \\( n \\) rows and \\( p \\) columns.\r\n      </p>\r\n      <p>\r\n        \\[ A = U \\cdot S\\cdot V^T \\]\r\n      </p>\r\n      <p>\r\n        Where:\r\n      </p>\r\n      <p>\r\n        \\[ U^TU = I_{n \\times n} \\]\r\n        \\[ V^TV = I_{p \\times p} \\Rightarrow U \\perp V\\]\r\n      </p>\r\n      <p>\r\n        Where the columns of \\( U \\) are the left singular vectors; \\(S\\) (the same dimensions\r\n        as \\( A \\)) has singular values and is diagonal; and \\( V^T \\) has rows that are the right singular\r\n        vectors.\r\n      </p>\r\n      <p>\r\n        Calculating the SVD consists of finding the eigenvalues and eigenvectors of \\( A^TA \\) and \\( AA^T \\). The\r\n        eigenvectors of \\( AA^T \\) make up the columns of \\( V \\), the eigenvectors of \\( A^TA \\) make up the columns of\r\n        \\( U \\). Also, the singular values in \\(S\\) are the square roots of eigenvalues from \\( AA^T \\) or \\( A^TA \\). The\r\n        singular values are the diagonal entries of the \\(S\\) matrix and are arranged in descending order. The singular\r\n        values are always real numbers. If the matrix \\( A \\) is a real matrix, then \\( U \\) and \\( V \\) are also real.\r\n      </p>\r\n    </div>\r\n      <div>\r\n        <p>\r\n          <strong>Example:</strong>\r\n        </p>\r\n        <p>While I would love to consider this matrix:</p>\r\n        <p>\\[ A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\\\ \\end{bmatrix} \\]</p>\r\n        <p>This yields a horrifying SVD that requires the <code>\\tiny</code> command from LaTeX and the <code>overflow-x: auto</code> command for CSS:</p>\r\n        <div class=\"card\">\r\n          <div class=\"card-body\">\r\n            <p> Find \\(M = U.Σ.V^†\\) where...\r\n              \\[M = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\\\ \\end{bmatrix}\\]\r\n            </p>\r\n            <p> Apologies, I had to shrink this to fit it on the page: \\( U =\\tiny{ \\begin{bmatrix} \\frac{3 - \\frac{2(-1223 - 13\\sqrt{8881})}{3(477 + 5\\sqrt{8881})} - \\frac{(-1015 - 11\\sqrt{8881})}{3(477 + 5\\sqrt{8881})}}{\\sqrt{(9 - \\frac{8(-1223 - 13\\sqrt{8881})}{3(477 + 5\\sqrt{8881})} - \\frac{7(-1015 - 11\\sqrt{8881})}{3(477 + 5\\sqrt{8881})})^2 + (6 - \\frac{5(-1223 - 13\\sqrt{8881})}{3(477 + 5\\sqrt{8881})} - \\frac{4(-1015 - 11\\sqrt{8881})}{3(477 + 5\\sqrt{8881})})^2 + (3 - \\frac{2(-1223 - 13\\sqrt{8881})}{3(477 + 5\\sqrt{8881})} - \\frac{(-1015 - 11\\sqrt{8881})}{3(477 + 5\\sqrt{8881})})^2}} & \\frac{3 - \\frac{2(1223 - 13\\sqrt{8881})}{3(5\\sqrt{8881} - 477)} - \\frac{(1015 - 11\\sqrt{8881})}{3(5\\sqrt{8881} - 477)}}{\\sqrt{(9 - \\frac{8(1223 - 13\\sqrt{8881})}{3(5\\sqrt{8881} - 477)} - \\frac{7(1015 - 11\\sqrt{8881})}{3(5\\sqrt{8881} - 477)})^2 + (6 - \\frac{5(1223 - 13\\sqrt{8881})}{3(5\\sqrt{8881} - 477)} - \\frac{4(1015 - 11\\sqrt{8881})}{3(5\\sqrt{8881} - 477)})^2 + (3 - \\frac{2(1223 - 13\\sqrt{8881})}{3(5\\sqrt{8881} - 477)} - \\frac{(1015 - 11\\sqrt{8881})}{3(5\\sqrt{8881} - 477)})^2}} & \\frac{1}{\\sqrt{6}} \\\\ \\\\ \\frac{6 - \\frac{5(-1223 - 13\\sqrt{8881})}{3(477 + 5\\sqrt{8881})} - \\frac{4(-1015 - 11\\sqrt{8881})}{3(477 + 5\\sqrt{8881})}}{\\sqrt{(9 - \\frac{8(-1223 - 13\\sqrt{8881})}{3(477 + 5\\sqrt{8881})} - \\frac{7(-1015 - 11\\sqrt{8881})}{3(477 + 5\\sqrt{8881})})^2 + (6 - \\frac{5(-1223 - 13\\sqrt{8881})}{3(477 + 5\\sqrt{8881})} - \\frac{4(-1015 - 11\\sqrt{8881})}{3(477 + 5\\sqrt{8881})})^2 + (3 - \\frac{2(-1223 - 13\\sqrt{8881})}{3(477 + 5\\sqrt{8881})} - \\frac{(-1015 - 11\\sqrt{8881})}{3(477 + 5\\sqrt{8881})})^2}} & \\frac{6 - \\frac{5(1223 - 13\\sqrt{8881})}{3(5\\sqrt{8881} - 477)} - \\frac{4(1015 - 11\\sqrt{8881})}{3(5\\sqrt{8881} - 477)}}{\\sqrt{(9 - \\frac{8(1223 - 13\\sqrt{8881})}{3(5\\sqrt{8881} - 477)} - \\frac{7(1015 - 11\\sqrt{8881})}{3(5\\sqrt{8881} - 477)})^2 + (6 - \\frac{5(1223 - 13\\sqrt{8881})}{3(5\\sqrt{8881} - 477)} - \\frac{4(1015 - 11\\sqrt{8881})}{3(5\\sqrt{8881} - 477)})^2 + (3 - \\frac{2(1223 - 13\\sqrt{8881})}{3(5\\sqrt{8881} - 477)} - \\frac{(1015 - 11\\sqrt{8881})}{3(5\\sqrt{8881} - 477)})^2}} & \\frac{1}{\\sqrt{6}} \\\\ \\\\ \\frac{9 - \\frac{8(-1223 - 13\\sqrt{8881})}{3(477 + 5\\sqrt{8881})} - \\frac{7(-1015 - 11\\sqrt{8881})}{3(477 + 5\\sqrt{8881})}}{\\sqrt{(9 - \\frac{8(-1223 - 13\\sqrt{8881})}{3(477 + 5\\sqrt{8881})} - \\frac{7(-1015 - 11\\sqrt{8881})}{3(477 + 5\\sqrt{8881})})^2 + (6 - \\frac{5(-1223 - 13\\sqrt{8881})}{3(477 + 5\\sqrt{8881})} - \\frac{4(-1015 - 11\\sqrt{8881})}{3(477 + 5\\sqrt{8881})})^2 + (3 - \\frac{2(-1223 - 13\\sqrt{8881})}{3(477 + 5\\sqrt{8881})} - \\frac{(-1015 - 11\\sqrt{8881})}{3(477 + 5\\sqrt{8881})})^2}} & \\frac{9 - \\frac{8(1223 - 13\\sqrt{8881})}{3(5\\sqrt{8881} - 477)} - \\frac{7(1015 - 11\\sqrt{8881})}{3(5\\sqrt{8881} - 477)}}{\\sqrt{(9 - \\frac{8(1223 - 13\\sqrt{8881})}{3(5\\sqrt{8881} - 477)} - \\frac{7(1015 - 11\\sqrt{8881})}{3(5\\sqrt{8881} - 477)})^2 + (6 - \\frac{5(1223 - 13\\sqrt{8881})}{3(5\\sqrt{8881} - 477)} - \\frac{4(1015 - 11\\sqrt{8881})}{3(5\\sqrt{8881} - 477)})^2 + (3 - \\frac{2(1223 - 13\\sqrt{8881})}{3(5\\sqrt{8881} - 477)} - \\frac{(1015 - 11\\sqrt{8881})}{3(5\\sqrt{8881} - 477)})^2}} & \\frac{1}{\\sqrt{6}} \\\\ \\end{bmatrix}} \\) </p>\r\n            <p> \\(Σ = \\begin{bmatrix} \\sqrt{\\frac{3}{2}(95 + \\sqrt{8881})} & 0 & 0 \\\\ 0 & \\sqrt{\\frac{3}{2}(95 - \\sqrt{8881})} & 0 \\\\ 0 & 0 & 0 \\\\ \\end{bmatrix} \\) \\(V = \\begin{bmatrix} -\\frac{-1015 - 11\\sqrt{8881}}{3(477 + 5\\sqrt{8881})\\sqrt{1 + \\frac{(-1223 - 13\\sqrt{8881})^2}{9(477 + 5\\sqrt{8881})^2} + \\frac{(-1015 - 11\\sqrt{8881})^2}{9(477 + 5\\sqrt{8881})^2}}} & -\\frac{1015 - 11\\sqrt{8881}}{3(5\\sqrt{8881} - 477)\\sqrt{1 + \\frac{(1223 - 13\\sqrt{8881})^2}{9(5\\sqrt{8881} - 477)^2} + \\frac{(1015 - 11\\sqrt{8881})^2}{9(5\\sqrt{8881} - 477)^2}}} & \\frac{1}{\\sqrt{6}} \\\\ -\\frac{-1223 - 13\\sqrt{8881}}{3(477 + 5\\sqrt{8881})\\sqrt{1 + \\frac{(-1223 - 13\\sqrt{8881})^2}{9(477 + 5\\sqrt{8881})^2} + \\frac{(-1015 - 11\\sqrt{8881})^2}{9(477 + 5\\sqrt{8881})^2}}} & -\\frac{1223 - 13\\sqrt{8881}}{3(5\\sqrt{8881} - 477)\\sqrt{1 + \\frac{(1223 - 13\\sqrt{8881})^2}{9(5\\sqrt{8881} - 477)^2} + \\frac{(1015 - 11\\sqrt{8881})^2}{9(5\\sqrt{8881} - 477)^2}}} & -\\sqrt{\\frac{2}{3}} \\\\ \\frac{1}{\\sqrt{1 + \\frac{(-1223 - 13\\sqrt{8881})^2}{9(477 + 5\\sqrt{8881})^2} + \\frac{(-1015 - 11\\sqrt{8881})^2}{9(477 + 5\\sqrt{8881})^2}}} & \\frac{1}{\\sqrt{1 + \\frac{(1223 - 13\\sqrt{8881})^2}{9(5\\sqrt{8881} - 477)^2} + \\frac{(1015 - 11\\sqrt{8881})^2}{9(5\\sqrt{8881} - 477)^2}}} & 0 \\\\ \\end{bmatrix}^†\\) </p>\r\n            <p>Note: † denotes the conjugate transpose</p>\r\n            <sub>I also found this one with Wolfram &#9825;</sub>\r\n          </div>\r\n        </div>\r\n        <br>\r\n        <div>\r\n          <p>INSTEAD, below I transcribed and explained <a href=\"https://www.d.umn.edu/~mhampton/m4326svd_example.pdf\">a better example</a> from <b>Prof. Marshall Hampton</b> at University of Minnesota Duluth (I merely trascribed & explained <em>Prof. Marshall Hampton's</em> idea &mdash; no plagiarism intended)! </p>\r\n          <p> Find the SVD of \\(A, U S V^{T}\\), where \\[A=\\begin{bmatrix}3 & 2 & 2 \\\\ 2 & 3 & -2\\end{bmatrix}\\] </p>\r\n          <p> First, we compute the singular values \\(\\sigma_{i}\\) by finding the eigenvalues of \\(A A^{T}\\). </p>\r\n          <p> \\[ A A^{T}=\\begin{bmatrix}17 & 8 \\\\ 8 & 17\\end{bmatrix} \\] </p>\r\n          <div class=\"card\">\r\n            <div class=\"card-body\">\r\n              <p> Multiply the following matrices: </p>\r\n              <p> \\[ \\begin{bmatrix} 3 & 2 & 2 \\\\ 2 & 3 & -2 \\end{bmatrix} \\cdot \\begin{bmatrix} 3 & 2 \\\\ 2 & 3 \\\\ 2 & -2 \\end{bmatrix} \\] </p>\r\n              <p> Determine the dimension of the product. The dimensions of the first matrix are \\(2 \\times 3\\) and the dimensions of the second matrix are \\(3 \\times 2\\). This means the dimensions of the product are \\(2 \\times 2\\): </p>\r\n              <p> \\[ \\begin{bmatrix} 3 & 2 & 2 \\\\ 2 & 3 & -2 \\end{bmatrix} \\cdot \\begin{bmatrix} 3 & 2 \\\\ 2 & 3 \\\\ 2 & -2 \\end{bmatrix} = \\begin{bmatrix} \\_ & \\_ \\\\ \\_ & \\_ \\end{bmatrix} \\] </p>\r\n              <p> Find the entry in the 1\\(^{\\text{st}}\\) row and 1\\(^{\\text{st}}\\) column of the product matrix. First look at the 1\\(^{\\text{st}}\\) row of the first matrix and the 1\\(^{\\text{st}}\\) column of the second matrix. </p>\r\n              <p> Highlight the 1\\(^{\\text{st}}\\) row and the 1\\(^{\\text{st}}\\) column: </p>\r\n              <p> \\[ \\begin{bmatrix} 3 & 2 & 2 \\\\ 2 & 3 & -2 \\end{bmatrix} \\cdot \\begin{bmatrix} 3 & 2 \\\\ 2 & 3 \\\\ 2 & -2 \\end{bmatrix} = \\begin{bmatrix} \\_ & \\_ \\\\ \\_ & \\_ \\end{bmatrix} \\] </p>\r\n              <p> Multiply corresponding components of the highlighted row and highlighted column, then add. </p>\r\n              <p> Multiply corresponding components and add: \\(3 \\cdot 3 + 2 \\cdot 2 + 2 \\cdot 2 = 17\\). </p>\r\n              <p> Place this number into the 1\\(^{\\text{st}}\\) row and 1\\(^{\\text{st}}\\) column of the product: </p>\r\n              <p> \\[ \\begin{bmatrix} 3 & 2 & 2 \\\\ 2 & 3 & -2 \\end{bmatrix} \\cdot \\begin{bmatrix} 3 & 2 \\\\ 2 & 3 \\\\ 2 & -2 \\end{bmatrix} = \\begin{bmatrix} 17 & \\_ \\\\ \\_ & \\_ \\end{bmatrix} \\] </p>\r\n              <p> Find the entry in the 1\\(^{\\text{st}}\\) row and 2\\(^{\\text{nd}}\\) column of the product matrix. First look at the 1\\(^{\\text{st}}\\) row of the first matrix and the 2\\(^{\\text{nd}}\\) column of the second matrix. </p>\r\n              <p> Highlight the 1\\(^{\\text{st}}\\) row and the 2\\(^{\\text{nd}}\\) column: </p>\r\n              <p> \\[ \\begin{bmatrix} 3 & 2 & 2 \\\\ 2 & 3 & -2 \\end{bmatrix} \\cdot \\begin{bmatrix} 3 & 2 \\\\ 2 & 3 \\\\ 2 & -2 \\end{bmatrix} = \\begin{bmatrix} 17 & \\_ \\\\ \\_ & \\_ \\end{bmatrix} \\] </p>\r\n              <p> Multiply corresponding components of the highlighted row and highlighted column, then add. </p>\r\n              <p> Multiply corresponding components and add: \\(3 \\cdot 2 + 2 \\cdot 3 + 2 \\cdot (-2) = 8\\). </p>\r\n              <p> Place this number into the 1\\(^{\\text{st}}\\) row and 2\\(^{\\text{nd}}\\) column of the product: </p>\r\n              <p> \\[ \\begin{bmatrix} 3 & 2 & 2 \\\\ 2 & 3 & -2 \\end{bmatrix} \\cdot \\begin{bmatrix} 3 & 2 \\\\ 2 & 3 \\\\ 2 & -2 \\end{bmatrix} = \\begin{bmatrix} 17 & 8 \\\\ \\_ & \\_ \\end{bmatrix} \\] </p>\r\n              <p> Find the entry in the 2\\(^{\\text{nd}}\\) row and 1\\(^{\\text{st}}\\) column of the product matrix. First look at the 2\\(^{\\text{nd}}\\) row of the first matrix and the 1\\(^{\\text{st}}\\) column of the second matrix. </p>\r\n              <p> Highlight the 2\\(^{\\text{nd}}\\) row and the 1\\(^{\\text{st}}\\) column: </p>\r\n              <p> \\[ \\begin{bmatrix} 3 & 2 & 2 \\\\ 2 & 3 & -2 \\end{bmatrix} \\cdot \\begin{bmatrix} 3 & 2 \\\\ 2 & 3 \\\\ 2 & -2 \\end{bmatrix} = \\begin{bmatrix} 17 & 8 \\\\ 8 & \\_ \\end{bmatrix} \\] </p>\r\n              <p> Multiply corresponding components of the highlighted row and highlighted column, then add. </p>\r\n              <p> Multiply corresponding components and add: \\(2 \\cdot 3 + 3 \\cdot 2 + (-2) \\cdot 2 = 8\\). </p>\r\n              <p> Place this number into the 2\\(^{\\text{nd}}\\) row and 1\\(^{\\text{st}}\\) column of the product: </p>\r\n              <p> \\[ \\begin{bmatrix} 3 & 2 & 2 \\\\ 2 & 3 & -2 \\end{bmatrix} \\cdot \\begin{bmatrix} 3 & 2 \\\\ 2 & 3 \\\\ 2 & -2 \\end{bmatrix} = \\begin{bmatrix} 17 & 8 \\\\ 8 & \\_ \\end{bmatrix} \\] </p>\r\n              <p> Find the entry in the 2\\(^{\\text{nd}}\\) row and 2\\(^{\\text{nd}}\\) column of the product matrix. First look at the 2\\(^{\\text{nd}}\\) row of the first matrix and the 2\\(^{\\text{nd}}\\) column of the second matrix. </p>\r\n              <p> Highlight the 2\\(^{\\text{nd}}\\) row and the 2\\(^{\\text{nd}}\\) column: </p>\r\n              <p> \\[ \\begin{bmatrix} 3 & 2 & 2 \\\\ 2 & 3 & -2 \\end{bmatrix} \\cdot \\begin{bmatrix} 3 & 2 \\\\ 2 & 3 \\\\ 2 & -2 \\end{bmatrix} = \\begin{bmatrix} 17 & 8 \\\\ 8 & \\_ \\end{bmatrix} \\] </p>\r\n              <p> Multiply corresponding components of the highlighted row and highlighted column, then add. </p>\r\n              <p> Multiply corresponding components and add: \\(2 \\cdot 2 + 3 \\cdot 3 + (-2) \\cdot (-2) = 17\\). </p>\r\n              <p> Place this number into the 2\\(^{\\text{nd}}\\) row and 2\\(^{\\text{nd}}\\) column of the product: </p>\r\n              <p> Answer: \\[ \\begin{bmatrix} 3 & 2 & 2 \\\\ 2 & 3 & -2 \\end{bmatrix} \\cdot \\begin{bmatrix} 3 & 2 \\\\ 2 & 3 \\\\ 2 & -2 \\end{bmatrix} = \\begin{bmatrix} 17 & 8 \\\\ 8 & 17 \\end{bmatrix} \\] </p>\r\n              <sub>Again, &#9825; lovingly &#9825; transcribed from Wolfram &#9825;</sub>\r\n            </div>\r\n          </div>\r\n          <br>\r\n          <p> The characteristic polynomial is \\(\\det(A A^{T}-\\lambda I)=\\lambda^{2}-34 \\lambda+225= \\) \\( (\\lambda-25)(\\lambda-9)\\), so the singular values are \\[\\sigma_{1}=\\sqrt{25}=5\\] \\[\\sigma_{2}=\\sqrt{9}=3\\] These singular values are important to finding \\(U\\) and \\(V^T\\), but let's pause to think about that last step. </p>\r\n          <div class=\"pl-5\">\r\n          <p>For this sixe theorems page, I do not want to get too much in to determinates (i.e. \\(\\det(A)\\)). Besides, the cross product we did for the <em>Fundamental Theorem</em> already demonstrated the \\(2\\times2\\) determinate. Recall, <em>the determinant of the matrix \\(\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\\) is given by \\(ad - bc\\)</em>.</p>\r\n          <p>The beauty and horror of determinats aside, quickly recall the <em>identity matrix</em>, which is just like the number \\(1\\) but for matrices.</p>\r\n          <p>\r\n            The general identity matrix, denoted as \\(I_n\\), is a square matrix of size \\(n \\times n\\) with ones on the main diagonal (from the top-left to the bottom-right) and zeros elsewhere.\r\n          </p>\r\n          <p>\r\n            The general identity matrix \\(I_n\\) can be represented as:\r\n          </p>\r\n          <p>\r\n            \\[\r\n            I_n = \\begin{bmatrix}\r\n              1 & 0 & \\cdots & 0 \\\\\r\n              0 & 1 & \\cdots & 0 \\\\\r\n              \\vdots & \\vdots & \\ddots & \\vdots \\\\\r\n              0 & 0 & \\cdots & 1\r\n            \\end{bmatrix}\r\n            \\]\r\n          </p>\r\n          <p>\r\n            In this matrix, the element at the \\(i^\\text{th}\\) row and \\(j^\\text{th}\\) column is denoted as \\((I_n)_{ij}\\). It has the value:\r\n          </p>\r\n          <p>\r\n            \\[\r\n            (I_n)_{ij} = \\begin{cases}\r\n              1 & \\text{if } i = j \\\\\r\n              0 & \\text{if } i \\neq j\r\n            \\end{cases}\r\n            \\]\r\n          </p>\r\n          <p>Now, before getting back to <em>Prof. Marshall Hampton</em> is to recall that matrix multiplication is not commutatitive. Generally, \\(AB \\ne BA \\)</p>\r\n        </div>\r\n        <p>Ok, here's another small explanation, but this isn't a mere note. It is important! The original from <em>Prof. Marshall Hampton</em> also does not mention \\(S = \\text{diag}_{n\\times p} (\\sigma) \\) meaning, the singualar value matrix is just a diagonal where \\(n\\times p\\) are just the row and column lengths for the original matrix.</p>\r\n        <p>Thus, \\( \\begin{bmatrix}\\sigma_1 & 0 & 0 \\\\ 0 & \\sigma_2 & 0\\end{bmatrix} = \\begin{bmatrix}3 & 0 & 0 \\\\ 0 & 5 & 0\\end{bmatrix} \\)</p>\r\n          <p> Now we find the right singular vectors (the columns of \\(V\\)) by finding an orthonormal set of eigenvectors of \\(A^{T} A\\). It is also possible to proceed by finding the left singular vectors (columns of \\(U\\)) instead. The eigenvalues of \\(A^{T} A\\) are 25, 9, and 0, and since \\(A^{T} A\\) is symmetric, we know that the eigenvectors will be orthogonal. </p>\r\n          <div class=\"card\"><div class=\"card-body\">\r\n            <p>\r\n              Multiply the following matrices:\r\n              \\[\r\n              \\begin{bmatrix}\r\n                3 & 2 \\\\\r\n                2 & 3 \\\\\r\n                2 & -2\r\n              \\end{bmatrix} \\cdot \\begin{bmatrix}\r\n                3 & 2 & 2 \\\\\r\n                2 & 3 & -2\r\n              \\end{bmatrix}\r\n              \\]\r\n            </p>\r\n            <p>\r\n              Determine the dimension of the product. The dimensions of the first matrix are 3x2 and the dimensions of the second matrix are 2x3.\r\n              This means the dimensions of the product are 3x3:\r\n              \\[\r\n              \\begin{bmatrix}\r\n                3 & 2 \\\\\r\n                2 & 3 \\\\\r\n                2 & -2\r\n              \\end{bmatrix} \\cdot \\begin{bmatrix}\r\n                3 & 2 & 2 \\\\\r\n                2 & 3 & -2\r\n              \\end{bmatrix} = \\begin{bmatrix}\r\n                \\_ & \\_ & \\_ \\\\\r\n                \\_ & \\_ & \\_ \\\\\r\n                \\_ & \\_ & \\_\r\n              \\end{bmatrix}\r\n              \\]\r\n            </p>\r\n            <p>\r\n              Find the entry in the \\(1^\\text{st}\\) row and \\(1^\\text{st}\\) column of the product matrix. First look at the \\(1^\\text{st}\\) row of the first matrix and the \\(1^\\text{st}\\) column of the second matrix.\r\n            </p>\r\n            <p>\r\n              Highlight the \\(1^\\text{st}\\) row and the \\(1^\\text{st}\\) column:\r\n            </p>\r\n            <p>\r\n              \\[\r\n              \\begin{bmatrix}\r\n                3 & 2 \\\\\r\n                2 & 3 \\\\\r\n                2 & -2\r\n              \\end{bmatrix} \\cdot \\begin{bmatrix}\r\n                3 & 2 & 2 \\\\\r\n                2 & 3 & -2\r\n              \\end{bmatrix} = \\begin{bmatrix}\r\n                \\_ & \\_ & \\_ \\\\\r\n                \\_ & \\_ & \\_ \\\\\r\n                \\_ & \\_ & \\_\r\n              \\end{bmatrix}\r\n              \\]\r\n            </p>\r\n            <p>\r\n              Multiply corresponding components of the highlighted row and highlighted column, then add.\r\n            </p>\r\n            <p>\r\n              Multiply corresponding components and add: \\(3 \\cdot 3 + 2 \\cdot 2 = 13\\).\r\n            </p>\r\n            <p>\r\n              Place this number into the \\(1^\\text{st}\\) row and \\(1^\\text{st}\\) column of the product:\r\n            </p>\r\n            <p>\r\n              \\[\r\n              \\begin{bmatrix}\r\n                3 & 2 \\\\\r\n                2 & 3 \\\\\r\n                2 & -2\r\n              \\end{bmatrix} \\cdot \\begin{bmatrix}\r\n                3 & 2 & 2 \\\\\r\n                2 & 3 & -2\r\n              \\end{bmatrix} = \\begin{bmatrix}\r\n                13 & \\_ & \\_ \\\\\r\n                \\_ & \\_ & \\_ \\\\\r\n                \\_ & \\_ & \\_\r\n              \\end{bmatrix}\r\n              \\]\r\n            </p>\r\n            <p>\r\n              Repeat the same process to find the remaining entries of the product matrix:\r\n            </p>\r\n            <p>\r\n              \\[\r\n              \\begin{bmatrix}\r\n                3 & 2 \\\\\r\n                2 & 3 \\\\\r\n                2 & -2\r\n              \\end{bmatrix} \\cdot \\begin{bmatrix}\r\n                3 & 2 & 2 \\\\\r\n                2 & 3 & -2\r\n              \\end{bmatrix} = \\begin{bmatrix}\r\n                13 & 12 & 2 \\\\\r\n                12 & 13 & -2 \\\\\r\n                2 & -2 & 8\r\n              \\end{bmatrix}\r\n              \\]\r\n            </p>\r\n            <p>\r\n              Therefore, the product of the matrices is:\r\n            </p>\r\n            <p>\r\n              \\[A^{T} A =\r\n              \\begin{bmatrix}\r\n                3 & 2 \\\\\r\n                2 & 3 \\\\\r\n                2 & -2\r\n              \\end{bmatrix} \\cdot \\begin{bmatrix}\r\n                3 & 2 & 2 \\\\\r\n                2 & 3 & -2\r\n              \\end{bmatrix} = \\begin{bmatrix}\r\n                13 & 12 & 2 \\\\\r\n                12 & 13 & -2 \\\\\r\n                2 & -2 & 8\r\n              \\end{bmatrix}\r\n              \\]\r\n            </p>\r\n            <sub>Yes, Wolfram :)</sub>\r\n          </div></div><br>\r\n          <p> For \\(\\lambda=25\\), we have </p>\r\n          <p> \\[ A^{T} A-25 I\\] \\[=\\begin{bmatrix}-12 & 12 & 2 \\\\ 12 & -12 & -2 \\\\ 2 & -2 & -17\\end{bmatrix} \\] </p>\r\n          <p> This row-reduces to the matrix \\(\\begin{bmatrix}1 & -1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0\\end{bmatrix}\\) giving the unit-length vector in the kernel of that matrix of \\(v_{1}=\\begin{bmatrix}\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0\\end{bmatrix}\\) </p>\r\n          <div class=\"card\">\r\n            <div class=\"card-body\">\r\n              <h5 class=\"card-title\">Convert \\(A^{T} A-25 I\\) to reduced row echelon form:</h5>\r\n              <p> \\[ \\begin{bmatrix} -12 & 12 & 2 \\\\ 12 & -12 & -2 \\\\ 2 & -2 & -17 \\end{bmatrix} \\] </p>\r\n              <p> Add one row to another: </p>\r\n              <p> \\[ \\begin{bmatrix} -12 & 12 & 2 \\\\ 0 & 0 & 0 \\\\ 2 & -2 & -17 \\end{bmatrix} \\] </p>\r\n              <p> Add a multiple of one row to another: </p>\r\n              <p> \\[ \\begin{bmatrix} -12 & 12 & 2 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & -\\frac{50}{3} \\end{bmatrix} \\] </p>\r\n              <p> Swap two rows: </p>\r\n              <p> \\[ \\begin{bmatrix} -12 & 12 & 2 \\\\ 0 & 0 & -\\frac{50}{3} \\\\ 0 & 0 & 0 \\end{bmatrix} \\] </p>\r\n              <p> Multiply row 2 by a scalar: </p>\r\n              <p> \\[ \\begin{bmatrix} -12 & 12 & 2 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} \\] </p>\r\n              <p> Subtract a multiple of one row from another: </p>\r\n              <p> \\[ \\begin{bmatrix} -12 & 12 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} \\] </p>\r\n              <p> Divide row 1 by a scalar: </p>\r\n              <p> \\[ \\begin{bmatrix} 1 & -1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} \\] </p>\r\n              <p> Verify matrix is reduced: </p>\r\n              <p> This matrix is now in reduced row echelon form. All nonzero rows are above rows of all zeros: </p>\r\n              <p> \\[ \\begin{bmatrix} 1 & -1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} \\] </p>\r\n              <p> Verify pivots and their positions: </p>\r\n              <p> Each pivot is 1 and is strictly to the right of every pivot above it: </p>\r\n              <p> \\[ \\begin{bmatrix} 1 & -1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} \\] </p>\r\n              <p> Verify all non-pivot elements in pivot columns are zeros: </p>\r\n              <p> Each pivot is the only nonzero entry in its column: </p>\r\n              <p> Answer: </p>\r\n              <p> \\[ \\begin{bmatrix} 1 & -1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} \\] </p>\r\n              <sub>Yes, I transcribed from Wolfram &#9825; &#9825;</sub>\r\n            </div>\r\n          </div>\r\n          <br>\r\n          <p> For \\(\\lambda=9\\), we have \\(A^{T} A-9 I=\\begin{bmatrix}4 & 12 & 2 \\\\ 12 & 4 & -2 \\\\ 2 & -2 & -1\\end{bmatrix}\\) which row-reduces to \\(\\begin{bmatrix}1 & 0 & -\\frac{1}{4} \\\\ 0 & 1 & \\frac{1}{4} \\\\ 0 & 0 & 0\\end{bmatrix}\\). </p>\r\n          <p> A unit-length vector in the kernel is \\(v_{2}=\\begin{bmatrix}\\frac{1}{\\sqrt{18}} \\\\ -\\frac{1}{\\sqrt{18}} \\\\ \\frac{4}{\\sqrt{18}}\\end{bmatrix}\\). </p>\r\n          <div class=\"card\">\r\n            <div class=\"card-body\">\r\n              <h5 class=\"card-title\">Find the kernel of the matrix M:</h5>\r\n              <p>\r\n                \\( M = \\begin{bmatrix} 1 & 0 & -\\frac{1}{4} \\\\ 0 & 1 & \\frac{1}{4} \\\\ 0 & 0 & 0 \\end{bmatrix} \\)\r\n              </p>\r\n              <p>\r\n                The kernel of a matrix \\( M \\) is the set of solutions \\( v \\) to the homogeneous equation \\( M \\cdot v = 0 \\).\r\n              </p>\r\n              <p>\r\n                The kernel of matrix \\( M = \\begin{bmatrix} 1 & 0 & -\\frac{1}{4} \\\\ 0 & 1 & \\frac{1}{4} \\\\ 0 & 0 & 0 \\end{bmatrix} \\) is the set of all vectors \\( v = (x_1, x_2, x_3) \\) such that \\( M \\cdot v = 0 \\):\r\n              </p>\r\n              <p>\r\n                \\( \\begin{bmatrix} 1 & 0 & -\\frac{1}{4} \\\\ 0 & 1 & \\frac{1}{4} \\\\ 0 & 0 & 0 \\end{bmatrix} \\cdot (x_1, x_2, x_3) = (0, 0, 0) \\)\r\n              </p>\r\n              <p>\r\n                Identify free variables. Free variables in the kernel \\((x_1, x_2, x_3)\\) correspond to the columns in \\( \\begin{bmatrix} 1 & 0 & -\\frac{1}{4} \\\\ 0 & 1 & \\frac{1}{4} \\\\ 0 & 0 & 0 \\end{bmatrix} \\) which have no pivot.\r\n              </p>\r\n              <p>\r\n                Column 3 is the only column with no pivot, so we may take \\( x_3 \\) to be the only free variable.\r\n              </p>\r\n              <p>\r\n                Perform matrix multiplication. Multiply out the reduced matrix \\( \\begin{bmatrix} 1 & 0 & -\\frac{1}{4} \\\\ 0 & 1 & \\frac{1}{4} \\\\ 0 & 0 & 0 \\end{bmatrix} \\) with the proposed solution vector \\( (x_1, x_2, x_3) \\):\r\n              </p>\r\n              <p>\r\n                \\( \\begin{bmatrix} 1 & 0 & -\\frac{1}{4} \\\\ 0 & 1 & \\frac{1}{4} \\\\ 0 & 0 & 0 \\end{bmatrix} \\cdot (x_1, x_2, x_3) = (x_1 - \\frac{x_3}{4}, x_2 + \\frac{x_3}{4}, 0) = (0, 0, 0) \\)\r\n              </p>\r\n              <p>\r\n                Convert to a system and solve in terms of the free variables. Solve the equations \\( \\begin{cases} x_1 - \\frac{x_3}{4} = 0 \\\\ x_2 + \\frac{x_3}{4} = 0 \\\\ 0 = 0 \\end{cases} \\) for \\( x_1 \\) and \\( x_2 \\):\r\n              </p>\r\n              <p>\r\n                \\( \\begin{cases} x_1 = \\frac{x_3}{4} \\\\ x_2 = -\\frac{x_3}{4} \\end{cases} \\)\r\n              </p>\r\n              <p>\r\n                Replace the pivot variables with free variable expressions. Rewrite \\( v \\) in terms of the free variable \\( x_3 \\), and assign it an arbitrary real value of \\( x \\):\r\n              </p>\r\n              <p>\r\n                \\( v = \\left(\\frac{x_3}{4}, -\\frac{x_3}{4}, x_3\\right) = \\left(\\frac{x}{4}, -\\frac{x}{4}, x\\right) \\) for \\( x \\in \\mathbb{R} \\)\r\n              </p>\r\n              <p>\r\n                Rewrite the solution vector without using fractions. Since \\( x \\) is taken from \\( \\mathbb{R} \\), we can replace it with \\( 4x \\):\r\n              </p>\r\n              <p>\r\n                \\( \\left(\\frac{x}{4}, -\\frac{x}{4}, x\\right) \\rightarrow \\left(\\frac{4x}{4}, -\\frac{1}{4}(4x), 4x\\right) = (x, -x, 4x) \\) for \\( x \\in \\mathbb{R} \\)\r\n              </p>\r\n              <p>\r\n                Convert to set-builder notation. Rewrite the solution vector \\( v = (x, -x, 4x) \\) in set notation:\r\n              </p>\r\n              <p>\r\n                Answer: \\( \\left\\{ (x, -x, 4x) : x \\in \\mathbb{R} \\right\\} \\)\r\n              </p>\r\n              <sub>Yes, yes... &#9825; &#9825; Wolfram &#9825;</sub>\r\n            </div>\r\n          </div><br>\r\n          <p> For the last eigenvector, we could (option 1) compute the kernel of \\(A^{T} A\\) or (option 2) find a unit vector perpendicular to \\(v_{1}\\) and \\(v_{2}\\). </p>\r\n          <p><em>Prof. Marshall Hampton</em> only uses option 2, which requires more reasoning and abstraction but is far less computational. Still, I like option 1 too.</p>\r\n          <div class=\"card\"><div class=\"card-body\">\r\n            <h5 class=\"card-title\">Option 1: \\( \\quad \\text{ker}(A^{T}A)\\)</h5>\r\n            <p>Let \\(M\\) denote \\(A^{T}A = \\begin{bmatrix}\r\n            13 & 12 & 2 \\\\\r\n            12 & 13 & -2 \\\\\r\n            2 & -2 & 8\r\n          \\end{bmatrix}\\)</p>\r\n          <p>\r\n            \\( M = \\begin{bmatrix} 13 & 12 & 2 \\\\ 12 & 13 & -2 \\\\ 2 & -2 & 8 \\end{bmatrix} \\)\r\n          </p>\r\n          <p>\r\n            The kernel of a matrix \\( M \\) is the set of solutions \\( v \\) to the homogeneous equation \\( M \\cdot v = 0 \\).\r\n          </p>\r\n          <p>\r\n            The kernel of matrix \\( M = \\begin{bmatrix} 13 & 12 & 2 \\\\ 12 & 13 & -2 \\\\ 2 & -2 & 8 \\end{bmatrix} \\) is the set of all vectors \\( v = (x_1, x_2, x_3) \\) such that \\( M \\cdot v = 0 \\):\r\n          </p>\r\n          <p>\r\n            \\( \\begin{bmatrix} 13 & 12 & 2 \\\\ 12 & 13 & -2 \\\\ 2 & -2 & 8 \\end{bmatrix} \\cdot (x_1, x_2, x_3) = (0, 0, 0) \\)\r\n          </p>\r\n          <p>\r\n            The kernel of a matrix is equal to the kernel of the row echelon form of the matrix.\r\n          </p>\r\n          <p>\r\n            Reduce the matrix \\( \\begin{bmatrix} 13 & 12 & 2 \\\\ 12 & 13 & -2 \\\\ 2 & -2 & 8 \\end{bmatrix} \\) to row echelon form:\r\n          </p>\r\n          <p>\r\n            \\( \\begin{bmatrix} 13 & 12 & 2 \\\\ 12 & 13 & -2 \\\\ 2 & -2 & 8 \\end{bmatrix} \\)\r\n          </p>\r\n          <p>\r\n            Subtract a multiple of one row from another.\r\n            Subtract \\( \\frac{12}{13} \\times \\) (row 1) from row 2:\r\n          </p>\r\n          <p>\r\n            \\( \\begin{bmatrix} 13 & 12 & 2 \\\\ 0 & \\frac{25}{13} & -\\frac{50}{13} \\\\ 2 & -2 & 8 \\end{bmatrix} \\)\r\n          </p>\r\n          <p>\r\n            Subtract a multiple of one row from another.\r\n            Subtract \\( \\frac{2}{13} \\times \\) (row 1) from row 3:\r\n          </p>\r\n          <p>\r\n            \\( \\begin{bmatrix} 13 & 12 & 2 \\\\ 0 & \\frac{25}{13} & -\\frac{50}{13} \\\\ 0 & -\\frac{50}{13} & \\frac{100}{13} \\end{bmatrix} \\)\r\n          </p>\r\n          <p>\r\n            Swap two rows.\r\n            Swap row 2 with row 3:\r\n          </p>\r\n          <p>\r\n            \\( \\begin{bmatrix} 13 & 12 & 2 \\\\ 0 & -\\frac{50}{13} & \\frac{100}{13} \\\\ 0 & \\frac{25}{13} & -\\frac{50}{13} \\end{bmatrix} \\)\r\n          </p>\r\n          <p>\r\n            Add a multiple of one row to another.\r\n            Add \\( \\frac{1}{2} \\times \\) (row 2) to row 3:\r\n          </p>\r\n          <p>\r\n            \\( \\begin{bmatrix} 13 & 12 & 2 \\\\ 0 & -\\frac{50}{13} & \\frac{100}{13} \\\\ 0 & 0 & 0 \\end{bmatrix} \\)\r\n          </p>\r\n          <p>\r\n            Multiply row 2 by a scalar.\r\n            Multiply row 2 by \\( -\\frac{13}{50} \\):\r\n          </p>\r\n          <p>\r\n            \\( \\begin{bmatrix} 13 & 12 & 2 \\\\ 0 & 1 & -2 \\\\ 0 & 0 & 0 \\end{bmatrix} \\)\r\n          </p>\r\n          <p>\r\n            Subtract a multiple of one row from another.\r\n            Subtract \\( 12 \\times \\) (row 2) from row 1:\r\n          </p>\r\n          <p>\r\n            \\( \\begin{bmatrix} 13 & 0 & 26 \\\\ 0 & 1 & -2 \\\\ 0 & 0 & 0 \\end{bmatrix} \\)\r\n          </p>\r\n          <p>\r\n            Divide row 1 by a scalar.\r\n            Divide row 1 by 13:\r\n          </p>\r\n          <p>\r\n            \\( \\begin{bmatrix} 1 & 0 & 2 \\\\ 0 & 1 & -2 \\\\ 0 & 0 & 0 \\end{bmatrix} \\)\r\n          </p>\r\n          <p>\r\n            Identify free variables.\r\n            Free variables in the kernel \\( (x_1, x_2, x_3) \\) correspond to the columns in \\( \\begin{bmatrix} 1 & 0 & 2 \\\\ 0 & 1 & -2 \\\\ 0 & 0 & 0 \\end{bmatrix} \\) which have no pivot.\r\n            Column 3 is the only column with no pivot, so we may take \\( x_3 \\) to be the only free variable.\r\n          </p>\r\n          <p>\r\n            Perform matrix multiplication.\r\n            Multiply out the reduced matrix \\( \\begin{bmatrix} 1 & 0 & 2 \\\\ 0 & 1 & -2 \\\\ 0 & 0 & 0 \\end{bmatrix} \\) with the proposed solution vector \\( (x_1, x_2, x_3) \\):\r\n          </p>\r\n          <p>\r\n            \\( \\begin{bmatrix} 1 & 0 & 2 \\\\ 0 & 1 & -2 \\\\ 0 & 0 & 0 \\end{bmatrix} \\cdot (x_1, x_2, x_3) = (x_1 + 2x_3, x_2 - 2x_3, 0) = (0, 0, 0) \\)\r\n          </p>\r\n          <p>\r\n            Convert to a system and solve in terms of the free variables.\r\n            Solve the equations \\( \\{ x_1 + 2x_3 = 0, x_2 - 2x_3 = 0, 0 = 0 \\} \\) for \\( x_1 \\) and \\( x_2 \\):\r\n          </p>\r\n          <p>\r\n            \\( \\{ x_1 = -2x_3, x_2 = 2x_3 \\} \\)\r\n          </p>\r\n          <p>\r\n            Replace the pivot variables with free variable expressions.\r\n            Rewrite \\( v \\) in terms of the free variable \\( x_3 \\), and assign it an arbitrary real value of \\( x \\):\r\n          </p>\r\n          <p>\r\n            \\( v = (x_1, x_2, x_3) = (-2x_3, 2x_3, x_3) = (-2x, 2x, x) \\) for \\( x \\in \\mathbb{R} \\)\r\n          </p>\r\n          <p>\r\n            Convert to set builder notation.\r\n            Rewrite the solution vector \\( v = (-2x, 2x, x) \\) in set notation:\r\n          </p>\r\n          <p>\r\n            Answer: \\( \\{ (-2x, 2x, x) : x \\in \\mathbb{R} \\} \\)\r\n          </p>\r\n          <sub>Wolfram, Wolfram, etc &#9825;</sub>\r\n          </div></div><br>\r\n          <div class=\"pl-5\">\r\n          <p> <b>Option 2:</b> To be perpendicular to \\(v_{1}=\\begin{bmatrix}a & b & c\\end{bmatrix}\\), we need \\(-a=b\\). Then the condition that \\(v_{2}^{T} v_{3}=0\\) becomes \\(\\frac{2a}{\\sqrt{18}}+\\frac{4c}{\\sqrt{18}}=0\\) or \\(-a=2c\\).</p><p>Thus, \\(v_{3}=\\begin{bmatrix}a & -a & -\\frac{a}{2}\\end{bmatrix}\\) &mdash; which satsfies the <em>option 1</em> result factoring out \\(-2\\) from \\( \\ker (A)=\\) \\( \\{ (-2x, 2x, x) : x \\in \\mathbb{R} \\} \\) &mdash; and for it to be unit-length, we need \\(a=\\frac{2}{3}\\) which gives \\(v_{3}=\\begin{bmatrix}\\frac{2}{3} & -\\frac{2}{3} & -\\frac{1}{3}\\end{bmatrix}\\).</p>\r\n          </div>\r\n          <p>At this point, we know that </p>\r\n          <p class=\"overflow-auto\"> \\[ A=U S V^{T}=U\\begin{bmatrix}5 & 0 & 0 \\\\ 0 & 3 & 0\\end{bmatrix}\\begin{bmatrix}\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0 \\\\ \\frac{1}{\\sqrt{18}} & -\\frac{1}{\\sqrt{18}} & \\frac{4}{\\sqrt{18}} \\\\ \\frac{2}{3} & -\\frac{2}{3} & -\\frac{1}{3}\\end{bmatrix} \\] </p>\r\n\r\n          \r\n          <p> Finally, we can compute \\(U\\) (by the product of the eigenvectors \\(v_i\\) and the original matrix \\(A\\) and also the reciprocals of the singular values\\(\\frac{1}{\\sigma_i}\\)) all via the formula \\(\\sigma u_{i}=A v_{i}\\) or \\(u_{i}=\\frac{1}{\\sigma} A v_{i}\\). This gives \\(U=\\begin{bmatrix}\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}}\\end{bmatrix}\\). </p>\r\n          <p>As <em>Prof. Marshall Hampton</em> puts it: \"So, in its full glory, the SVD is:\"</p>\r\n          <p class=\"overflow-auto\"> \\[ A=U S V^{T}=\\begin{bmatrix}\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}}\\end{bmatrix}\\begin{bmatrix}5 & 0 & 0 \\\\ 0 & 3 & 0\\end{bmatrix}\\begin{bmatrix}\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0 \\\\ \\frac{1}{\\sqrt{18}} & -\\frac{1}{\\sqrt{18}} & \\frac{4}{\\sqrt{18}} \\\\ \\frac{2}{3} & -\\frac{2}{3} & -\\frac{1}{3}\\end{bmatrix} \\] </p>\r\n        </div>\r\n      </div>\r\n      <hr>\r\n      <!-- SPECTRAL THEOREM -->\r\n      <h2 id=\"spectral-theorem\">Spectral Theorem</h2>\r\n      <p>If \\( A^T = A \\), there are orthonormal \\( q \\)'s so that \\( Aq_i = \\lambda_iq_i \\) and \\( A = Q\\Lambda Q^T \\).</p>\r\n      <p>Mathematically: If \\( A \\) is symmetric, there exist orthonormal eigenvectors (\\( q \\)'s) and eigenvalues (\\( \\lambda \\)'s) such that \\( Aq_i = \\lambda_iq_i \\).</p>\r\n      <p>Now we're really talking.</p>\r\n      <p>Here is another explanation, this time explaining a lovely example from <em>Prof. Bruce Ikenaga</em> transcribed from <a href=\"https://sites.millersville.edu/bikenaga/linear-algebra/spectral-theorem/spectral-theorem.html\">here</a>.</p>\r\n      <p class=\"pl-5\">Also, <a href=\"https://mast.queensu.ca/~br66/419/spectraltheoremproof.pdf\">here</a> is a shorter proof from <em>Prof. Brad Rodgers</em> at an undergraduate level, and <a href=\"https://math.dartmouth.edu/~m113s19/ln-spec-thm.pdf\">here</a> is a longer proof from <em>Prof. Dana Williams</em> at a graduate level applying this theorem to bounded and unbounded operators on an infinite dimensional complex Hilbert space, all culminating in Stone's Theorem.</p>\r\n      <p>Finally, <em>Prof. Ikenaga's</em> example requires little explanation or calculation, but there are two points to touch on first.</p>\r\n      <p>First, each column has unit length and is perpendicular to every other column, so the inverse is the transpose. \"If \\(A^{-1}=A^T\\), then \\(A^TA=I\\). This means that each column has unit length and is perpendicular to every other column. That means it is an orthonormal matrix. [...] Think of \\(A\\) as an arrangement of \\(n\\) columns (each \\(n\\) elements tall). Then the \\((i, j)\\) element of \\(A^TA\\) is the dot product of the \\(i\\)th and \\(j\\)th columns of \\(A\\) since the \\(i\\)th row of \\(A^T\\) is the \\(i\\)th column of \\(A\\).\" (<a href=\"https://math.stackexchange.com/a/156742/1098426\">Math Stack Exchange</a>)</p>\r\n      <p>Second, eigenvectors from different eigenvalues are linearly independent.</p>\r\n      <p>The best explanation I could find of this is from <a href=\"https://math.stackexchange.com/a/29374/1098426\">Math Stack Exchange</a>:</p>\r\n      <p class=\"pl-5 overflow-auto\">Suppose \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\) correspond to distinct eigenvalues \\(\\lambda_1\\) and \\(\\lambda_2\\), respectively. \r\n        \r\n        Take a linear combination that is equal to \\(0\\), \\(\\alpha_1\\mathbf{v}_1+\\alpha_2\\mathbf{v}_2 = \\mathbf{0}\\). We need to show that \\(\\alpha_1=\\alpha_2=0\\).\r\n        \r\n        Applying \\(T\\) to both sides, we get\r\n        \\[\\mathbf{0} = T(\\mathbf{0}) = T(\\alpha_1\\mathbf{v}_1+\\alpha_2\\mathbf{v}_2) = \\alpha_1\\lambda_1\\mathbf{v}_1 + \\alpha_2\\lambda_2\\mathbf{v}_2.\\]\r\n        Now, instead, multiply the original equation by \\(\\lambda_1\\):\r\n        \\[\\mathbf{0} = \\lambda_1\\alpha_1\\mathbf{v}_1 + \\lambda_1\\alpha_2\\mathbf{v}_2.\\]\r\n        Now take the two equations,\r\n        \\[\\begin{align*}\r\n        \\mathbf{0} &= \\alpha_1\\lambda_1\\mathbf{v}_1 + \\alpha_2\\lambda_2\\mathbf{v}_2\\\\\r\n        \\mathbf{0} &= \\alpha_1\\lambda_1\\mathbf{v}_1 + \\alpha_2\\lambda_1\\mathbf{v}_2\r\n        \\end{align*}\\]\r\n        and taking the difference, we get:\r\n        \\[\\mathbf{0} = 0\\mathbf{v}_1 + \\alpha_2(\\lambda_2-\\lambda_1)\\mathbf{v}_2 = \\alpha_2(\\lambda_2-\\lambda_1)\\mathbf{v}_2.\\]\r\n        \r\n        Since \\(\\lambda_2-\\lambda_1\\neq 0\\), and since \\(\\mathbf{v}_2\\neq\\mathbf{0}\\) (because \\(\\mathbf{v}_2\\) is an eigenvector), then \\(\\alpha_2=0\\). Using this on the original linear combination \\(\\mathbf{0} = \\alpha_1\\mathbf{v}_1 + \\alpha_2\\mathbf{v}_2\\), we conclude that \\(\\alpha_1=0\\) as well (since \\(\\mathbf{v}_1\\neq\\mathbf{0}\\)).\r\n        \r\n        So \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\) are linearly independent.\r\n        \r\n        Now try using induction on \\(n\\) for the general case.</p>\r\n      <div class=\"card\"><div class=\"card-body\">\r\n        <p>Now, take a symmetric matrix \\(\r\n          A=\\left[\\begin{array}{ccc}\r\n          -1 & 2 & 0 \\\\\r\n          2 & 2 & 0 \\\\\r\n          0 & 0 & 3\r\n          \\end{array}\\right]\r\n          \\)</p>\r\n          \r\n          <p>Find an orthogonal matrix \\(\\mathrm{O}\\) which diagonalizes \\(\\mathrm{A}\\). Find \\(\\mathrm{O}^{-1}\\) and the corresponding diagonal matrix.</p>\r\n          \r\n          <p>The characteristic polynomial is</p>\r\n          \r\n          <p>\\[\r\n          \\det{(A - xI)}\r\n          \\]\r\n        \\[ = (x-3)[(x-2)(x+1)-(2)(2)]\\]\r\n        \\[=-(x-3)^2(x+2)\\]\r\n        </p>\r\n          \r\n          <p>The eigenvalues are \\(x=3\\) and \\(x=-2\\).</p>\r\n          \r\n          <p>For \\(x=3\\), the eigenvector matrix is</p>\r\n          \r\n          <p>\\[\r\n          A-3 I=\\left[\\begin{array}{ccc}\r\n          -4 & 2 & 0 \\\\\r\n          2 & -1 & 0 \\\\\r\n          0 & 0 & 0\r\n          \\end{array}\\right] \\rightarrow\\left[\\begin{array}{ccc}\r\n          2 & -1 & 0 \\\\\r\n          0 & 0 & 0 \\\\\r\n          0 & 0 & 0\r\n          \\end{array}\\right]\r\n          \\]</p>\r\n          \r\n          <p>This gives the independent eigenvectors \\((1,2,0)\\) and \\((0,0,1)\\). Dividing them by their lengths, I get \\(\\frac{1}{\\sqrt{5}}(1,2,0)\\) and \\((0,0,1)\\).</p>\r\n          \r\n          <p>For \\(x=-2\\), the eigenvector matrix is</p>\r\n          \r\n          <p>\\[\r\n          A+2 I=\\left[\\begin{array}{lll}\r\n          1 & 2 & 0 \\\\\r\n          2 & 4 & 0 \\\\\r\n          0 & 0 & 5\r\n          \\end{array}\\right] \\rightarrow\\left[\\begin{array}{lll}\r\n          1 & 2 & 0 \\\\\r\n          0 & 0 & 1 \\\\\r\n          0 & 0 & 0\r\n          \\end{array}\\right]\r\n          \\]</p>\r\n          \r\n          <p>This gives the independent eigenvector \\((-2,1,0)\\). Dividing it by its length, I get \\(\\frac{1}{\\sqrt{5}}(-2,1,0)\\).</p>\r\n          \r\n          <p>Thus, the orthogonal diagonalizing matrix is</p>\r\n          \r\n          <p>\\[\r\n          O=\\left[\\begin{array}{ccc}\r\n          \\frac{1}{\\sqrt{5}} & 0 & -\\frac{2}{\\sqrt{5}} \\\\\r\n          \\frac{2}{\\sqrt{5}} & 0 & \\frac{1}{\\sqrt{5}} \\\\\r\n          0 & 1 & 0\r\n          \\end{array}\\right]\r\n          \\]</p>\r\n          \r\n          <p>Then (note again: each column has unit length and is perpendicular to every other column, so the inverse is the transpose)</p>\r\n          \r\n          <p>\\[\r\n          O^{-1}=O^T=\\left[\\begin{array}{ccc}\r\n          \\frac{1}{\\sqrt{5}} & \\frac{2}{\\sqrt{5}} & 0 \\\\\r\n          0 & 0 & 1 \\\\\r\n          -\\frac{2}{\\sqrt{5}} & \\frac{1}{\\sqrt{5}} & 0\r\n          \\end{array}\\right]\r\n          \\]</p>\r\n          \r\n          <p>The diagonal matrix is</p>\r\n          \r\n          <p>\\[\r\n          O^T A O=\\left[\\begin{array}{ccc}\r\n          \\frac{1}{\\sqrt{5}} & \\frac{2}{\\sqrt{5}} & 0 \\\\\r\n          0 & 0 & 1 \\\\\r\n          -\\frac{2}{\\sqrt{5}} & \\frac{1}{\\sqrt{5}} & 0\r\n          \\end{array}\\right]\r\n          \r\n          \\left[\\begin{array}{ccc}\r\n          -1 & 2 & 0 \\\\\r\n          2 & 2 & 0 \\\\\r\n          0 & 0 & 3\r\n          \\end{array}\\right]\r\n\r\n          \\left[\\begin{array}{ccc}\r\n          \\frac{1}{\\sqrt{5}} & 0 & -\\frac{2}{\\sqrt{5}} \\\\\r\n          \\frac{2}{\\sqrt{5}} & 0 & \\frac{1}{\\sqrt{5}} \\\\\r\n          0 & 1 & 0\r\n          \\end{array}\\right]\r\n          \\] \\[ =\\left[\\begin{array}{ccc}\r\n          3 & 0 & 0 \\\\\r\n          0 & 3 & 0 \\\\\r\n          0 & 0 & -2\r\n          \\end{array}\\right] \\]</p>\r\n          \r\n        <sup>We all &#9825; Wolfram</sup></div></div><br>\r\n      <hr>\r\n      <h2 id=\"nutshell\">Linear Algebra in a Nutshell</h2>\r\n      <p>As a bonus, here is the other part of the sections from from Gilbert Strang's <em>Introduction to Linear Algebra, 5th Ed</em>. I was tempted not to include this because I cannot think of a sufficient example. But, I was struck by the similarity with \"The Key Theorem of Linear Algebra\" from Prof. Thomas Garrity's <em>All the Math you Missed, 2nd Ed</em>. Prof. Garrity lays out his version after giving three definitions for the determinant (basically from induction, from linear rules, & from signed volume), but Prof. Garrity does so before defining eigenvectors, as his intro to matrices as linear transformations.</p>\r\n      <p><b>Let \\(A\\) be a \\(n\\times n\\) matrix...</b></p>\r\n      <div class=\"table-responsive\">\r\n      <table class=\"table\">\r\n        <thead class=\"thead-light\">\r\n        <tr>\r\n          <th scope=\"col\" class=\"col-3\">Singular</th>\r\n          <th scope=\"col\" class=\"col-3\">Nonsingular</th>\r\n        </tr>\r\n      </thead>\r\n      <tbody>\r\n        <tr>\r\n          <td>\\(A\\) is not invertible</td>\r\n          <td>\\(A\\) is invertible</td>\r\n        </tr>\r\n        <tr>\r\n          <td>The columns are dependent</td>\r\n          <td>The columns are independent</td>\r\n        </tr>\r\n        <tr>\r\n          <td>The rows are dependent</td>\r\n          <td>The rows are independent</td>\r\n        </tr>\r\n        <tr>\r\n          <td>The determinant is zero</td>\r\n          <td>The determinant is not zero</td>\r\n        </tr>\r\n        <tr>\r\n          <td>\\(Ax=0\\) has infinitely many solutions</td>\r\n          <td>\\(Ax=0\\) has one solution \\(x=0\\)</td>\r\n        </tr>\r\n        <tr>\r\n          <td>\\(Ax=b\\) has no solution or \\(\\infty\\) many</td>\r\n          <td>\\(Ax=b\\) has one solution \\(x=A^{-1}b\\)</td>\r\n        </tr>\r\n        <tr>\r\n          <td>\\(A\\) has \\(r < n\\) pivots</td>\r\n          <td>\\(A\\) has \\(n\\) (nonzero) pivots</td>\r\n        </tr>\r\n        <tr>\r\n          <td>\\(A\\) has rank \\(r < n\\)</td>\r\n          <td>\\(A\\) has full rank \\(r=n\\)</td>\r\n        </tr>\r\n        <tr>\r\n          <td>Reduced row echelon form isn't \\(R=I\\)</td>\r\n          <td>Reduced row echelon form is \\(R=I\\)</td>\r\n        </tr>\r\n        <tr>\r\n          <td>The column space has dimension \\(r < n\\)</td>\r\n          <td>The column space is all of \\(R^n\\)</td>\r\n        </tr>\r\n        <tr>\r\n          <td>The row space has dimension \\(r < n\\)</td>\r\n          <td>The row space is all of \\(R^n\\)</td>\r\n        </tr>\r\n        <tr>\r\n          <td>Zero is an eigenvalue of \\(A\\)</td>\r\n          <td>All eigenvalues are nonzero</td>\r\n        </tr>\r\n        <tr>\r\n          <td>\\(A^TA\\) is only semidefinite</td>\r\n          <td>\\(A^TA\\) is symmetric positive definite</td>\r\n        </tr>\r\n        <tr>\r\n          <td>\\(A\\) has \\(r < n\\) singular values</td>\r\n          <td>\\(A\\) has \\(n\\) (positive) singular values</td>\r\n        </tr>\r\n      </tbody>\r\n      </table>\r\n    </div>\r\n      <br>\r\n      <div class=\"card\"><div class=\"card-body\">\r\n        <p><b>Now &#9825; transcribed &#9825; from <em>All the Math you Missed</em> by Prof. Garrity &#9825; &#9825;</b></p>\r\n        <p><em>Theorem 1.6.1 (Key Theorem)</em></p>\r\n        <p>Let \\(A\\) be an \\(n \\times n\\) matrix. Then the following are equivalent:</p>\r\n        <ol>\r\n          <li>\\(A\\) is invertible.</li>\r\n          <li>\\(\\det(A) \\neq 0\\).</li>\r\n          <li>\\(\\text{ker}(A) = \\{0\\}\\).</li>\r\n          <li>If \\(b\\) is a column vector in \\(\\mathbb{R}^n\\), there is a unique column vector \\(x\\) in \\(\\mathbb{R}^n\\) satisfying \\(Ax = b\\).</li>\r\n          <li>The columns of \\(A\\) are linearly independent \\(n \\times 1\\) column vectors.</li>\r\n          <li>The rows of \\(A\\) are linearly independent \\(1 \\times n\\) row vectors.</li>\r\n          <li>The transpose \\(A^T\\) of \\(A\\) is invertible. (Here, if \\(A = (a_{ij})\\), then \\(A^T = (a_{ji})\\)).</li>\r\n          <li>All of the eigenvalues of \\(A\\) are non-zero.</li>\r\n        </ol>\r\n        \r\n        <p><em>Theorem 1.6.2 (Key Theorem)</em></p>\r\n        <p>Let \\(T : V \\rightarrow V\\) be a linear transformation. Then the following are equivalent:</p>\r\n        <ol>\r\n          <li>\\(T\\) is invertible.</li>\r\n          <li>\\(\\det(T) \\neq 0\\), where the determinant is defined by a choice of basis on \\(V\\).</li>\r\n          <li>\\(\\text{ker}(T) = \\{0\\}\\).</li>\r\n          <li>If \\(b\\) is a vector in \\(V\\), there is a unique vector \\(v\\) in \\(V\\) satisfying \\(T(v) = b\\).</li>\r\n          <li>For any basis \\(v_1, \\ldots, v_n\\) of \\(V\\), the image vectors \\(T(v_1), \\ldots, T(v_n)\\) are linearly independent.</li>\r\n          <li>For any basis \\(v_1, \\ldots, v_n\\) of \\(V\\), if \\(S\\) denotes the transpose linear transformation of \\(T\\), then the image vectors \\(S(v_1), \\ldots, S(v_n)\\) are linearly independent.</li>\r\n          <li>The transpose of \\(T\\) is invertible. (Here the transpose is defined by a choice of basis on \\(V\\).)</li>\r\n          <li>All of the eigenvalues of \\(T\\) are non-zero.</li>\r\n        </ol>\r\n      </div></div><br>\r\n      <hr>\r\n      <p>I hope you <em>enjoyed</em> because I sure did.</p>\r\n\r\n    </div><!-- bootstrap wrapper container closing div tag-->\r\n    <!-- <footer class=\"bg-light pt-5 pb-2 px-5\">\r\n      <hr> \r\n      <a href=\"/\">Home</a>\r\n    </footer> -->\r\n    <script src=\"https://code.jquery.com/jquery-3.4.1.min.js\"></script>\r\n    <script src=\"https://code.jquery.com/ui/1.12.1/jquery-ui.js\"></script>\r\n    <script src=\"https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js\" integrity=\"sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo\" crossorigin=\"anonymous\"></script>\r\n    <script src=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js\" integrity=\"sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6\" crossorigin=\"anonymous\"></script>\r\n    <!--<script src=\"js/main.js\"></script>-->\r\n  </body>\r\n</html>",
      "date_published": "2023-05-31T17:00:00-07:00"
    },{
      "id": "https://ischmidls.github.io/posts/pages/heyting/",
      "url": "https://ischmidls.github.io/posts/pages/heyting/",
      "title": "heyting",
      "content_html": "<!DOCTYPE html>\r\n<html>\r\n<head>\r\n    <meta charset=\"UTF-8\" />\r\n    <title>Heyting</title>\r\n    <!--MATH SUPPORT-->\r\n    <script type=\"text/javascript\" id=\"MathJax-script\" async src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"></script>\r\n    <!--STYLE-->\r\n    <link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css\" integrity=\"sha384-B0vP5xmATw1+K9KRQjQERJvTumQW0nPEzvF6L/Z6nronJ3oUOFUFpCjEUQouq2+l\" crossorigin=\"anonymous\">\r\n    <!--BACKWARDS COMPATIBILITY-->\r\n    <script src=\"https://polyfill.io/v3/polyfill.min.js?features=es6\"></script>\r\n</head>\r\n<body>\r\n    <header><br></header>\r\n    <div class=\"container\" id=\"bsr-wrapper\">\r\n        <h1>Heyting Algebras</h1>\r\n        <p>Izak, May 2023</p>\r\n      <!-- <p class=\"text-secondary\">Estimated reading time: ~20 min. <br> \r\n        Approx. word count: ~3,700 words <br> \r\n        Approx math terms: ~300 LaTeX blocks</p> -->\r\n\r\n        <h2>Table of Contents</h2>\r\n        <ul>\r\n            <li><a href=\"#heyting-algebra\">Heyting Algebra</a></li>\r\n            <li><a href=\"#topologize-this\">Topologize This</a></li>\r\n            <li><a href=\"#adjunction-boolean\">Adjunction Boolean</a></li>\r\n            <li><a href=\"#quantifiers-as-adjoints\">Quantifiers as Adjoints</a></li>\r\n            <li><a href=\"#topoi-of-heyting-algebra-models\">Topoi of ʜᴇʏᴛɪɴɢ ᴀʟɢᴇʙʀᴀ Models</a></li>\r\n          </ul>\r\n          \r\n          <h2 id=\"heyting-algebra\">Heyting Algebra</h2>\r\n          \r\n          \r\n<p>Now, Heyting algebras often take the unhelpful label—for those who are not yet students of topology—that: \"Every topology provides a complete Heyting algebra in the form of its open set lattice.\"</p>\r\n<p>NOTE: An \"open\" set is defined as a set where each point is surrounded by other members of the set. To provide a precise definition, one begins with a \"basic open set\" and forms other open sets from it. In the presence of a metric, basic open sets can be represented by \\(ϵ\\)-neighborhoods (points are less than \\(ϵ\\) away from some point for \\(ϵ > 0\\)). \\(A\\) set \\(S\\) is considered open if, for every point \\(P ∈ S\\), there exists a basic open set \\(U\\) containing \\(P\\) such that \\(U ⊆ S\\). \\(A\\) set is classified as closed if it either has no complement or if its complement is open.</p>\r\n<p>A subset lattice (also called powerset lattice) is prototypical. Where join is disjunction, union, and inclusion, and meet is conjunction, intersection, and (something like) includes. In the case of Heyting algebras, these might also be maximum and minimum respectively.</p>\r\n<img alt=\"\" src=\"images/image2.png\" style=\"width: 100%; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);\" title=\"\">\r\n<p>Source: <a href=\"https://en.wikipedia.org/wiki/Power_set\">Wikipedia Power set</a></p>\r\n<p>For a more concrete example of a Heyting algebra, let us look at the next Figure where is is clear conjunction \\(A \\land B\\) is \\(min(A, B)\\) and the disjunction \\(A \\lor B\\) is \\(max(A, B)\\) but less clear is what Wikipedia kindly spells out: \"Every totally ordered set that has a least element \\(0\\)and a greatest element \\(1\\) is a Heyting algebra (if viewed as a lattice). In this case \\(A \\rightarrow B\\) equals to \\(1\\)(or \\(T\\) for those generalized fans out there) when \\(A \\leq B\\), and \\(B\\) otherwise.\"</p>\r\n<img alt=\"\" src=\"images/image6.png\" style=\"width: 100%; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);\" title=\"\">\r\n<p>Fig: Source: <a href=\"https://en.wikipedia.org/wiki/Heyting_algebra\">Wikipedia Heyting algebra</a></p>\r\n<p>With all that aside, it becomes somewhat clearer how anybody would evaluate truth via a Heyting algebra. But what of intuitionistic logic? Well, Wikipedia touches on that too!</p>\r\n<p>Picture this: We've got this formula \\((P \\lor Q) \\rightarrow P\\), and by the definition of the pseudo-complement (fancy term alert!), it's all about finding the largest element \\(x\\) such that \\(P \\land Q \\land x \\leq P\\). And guess what? We hit the jackpot! We find an \\(x\\) that satisfies the equation, and you know what that means? It's \\(1\\),baby! That's the biggest \\(x\\) we can get.</p>\r\n<p>But wait, there's more! The rule of modus ponens swoops in like a superhero, allowing us to derive this formula from the formulas \\(P\\) and \\(P \\rightarrow Q\\). Now, here's where things get interesting. In any Heyting algebra, if \\(P\\) has that value of \\(1\\),and \\(P \\rightarrow Q\\) is strutting with a value of \\(1\\)too, it can only mean one thing: \\(P \\land 1 \\leq Q\\). And when we crunch the numbers, \\(1 \\land 1 \\leq Q\\), it's crystal clear—\\(Q\\) has got to be \\(1\\)too!</p>\r\n<p>So here's the deal: If a formula can be deduced from the laws of intuitionistic logic using our trusty modus ponens rule, it's a winner in all Heyting algebras. No matter how you assign those variable values, it'll always score a perfect \\(1\\).Talk about consistency!</p>\r\n<p>We can whip up a Heyting algebra where Peirce's law doesn't always hit that \\(1\\)mark. Let's take a peek at a 3-element algebra \\(\\{0, \\frac{1}{2}, 1\\}\\). If we play around and give \\(P\\) a value of \\(\\frac{1}{2}\\) and \\(Q\\) a sad \\(0\\),the value of Peirce's law \\(((P \\rightarrow Q) \\rightarrow P) \\rightarrow P\\) turns out to be \\(\\frac{1}{2}\\). Peirce's law (excluded middle via conditionals) just can't quite make the intuitionistic cut.</p>\r\n<p>But fear not. There's a flip side to this story. If a formula always takes a value of \\(1\\),it can be deduced from the laws of intuitionistic logic. The intuitionistically valid formulas always have that \\(1\\)value. It's just like those classically valid formulas that score a perfect \\(1\\)in the two-element Boolean algebra, no matter what true and false you throw their way.</p>\r\n<p>So, my friends, a Heyting algebra is like a wild and wonderful playground for logic. It's a grand generalization of the usual truth values system, and the biggest element in the algebra is like the ultimate \"true.\" And guess what? Our usual two-valued logic system is just a special case of a Heyting algebra—the baby version, where we only have \\(1\\) (true) and \\(0\\) (false) hanging out.</p>\r\n<p>Let’s take a turn from the big boys and churn out another spindle on the so-called Computational Trilogy.</p>\r\n\r\n\r\n<h2 id=\"topologize-this\">Topologize This</h2>\r\n\r\n<p>The question now: how do elementary topoi relate to Heyting algebras? Here is my transposition of knowledge dependencies—mostly from Wikipedia.\r\n    The following sparse, definitional style is one of many manners for illustrating how these concepts hang together.\r\n    As a preliminary, some chapter zero jargon, (monics are injective homomorphisms—injective meaning all inputs used \\( \\forall x \\exists f(x) \\)—homomorphisms satisfying \\( f(ab) = f(a)f(b) \\)) and isomorphisms. That being said, there are several layers of definitions to peel away, at least to recall: global element, and subobject classifier, and elementary topos. With that out of the way: recall, the global element of an object \\(A\\) from a category is a morphism \\(h:1 \\rightarrow A\\), where \\(1\\) is a terminal object of the category. (recall: a terminal object is the object of the category that is the target of morphisms from all objects in the category) Also recall, the subobject classifier relies on two other definitions, at least: subobject & pullback. \\(A\\) subobject is analogous to a subset, but formally: Some object \\(A\\) in some category has two monics denoted \\(u:S \\rightarrow A\\) and \\(v:T \\rightarrow A\\) (satisfying an equivalence relation \\(uRv\\) with isomorphism \\(f:S \\rightarrow T\\) where \\(u = v \\circ f\\)), and the subobjects of \\(A\\) are the equivalence classes of those monics that can be expressed as the equalizer of two morphisms. An equalizer is analogous to the set-theoretic notion: let \\(X\\) and \\(Y\\) be sets. Let \\(f\\) and \\(g\\) be functions, both from \\(X\\) to \\(Y\\). Then the equalizer of \\(f\\) and \\(g\\) is the set of elements \\(x\\) of \\(X\\) such that \\(f(x)\\) equals \\(g(x)\\) in \\(Y\\). This is also denoted \\(Eq(f,g) := \\{x \\in X \\mid f(x) = g(x)\\}\\). This generalizes to any set of functions \\(f,g \\in F\\), but is degenerate (e.g., singletons and empties). Categorically, the equalizer is the limit of the diagram (note: a diagram is analogous to an indexed family of sets, except morphisms are indexed too, i.e., a diagram with shape \\(J\\) in category \\(C\\) is a functor \\(F:J \\rightarrow C\\) for morphisms \\(f,g:X \\rightarrow Y\\)). The limit is a cone (note: a cone to \\(F\\) is an object \\(N\\) of \\(C\\) together with a family \\(p(X):N \\rightarrow F(X)\\) of morphisms indexed by objects \\(X\\) of \\(J\\) such that every morphism \\(f:X \\rightarrow Y\\) in \\(J\\) requires \\(F(f) \\circ p(X) = p(Y)\\)) where this specific cone is denoted \\((L,t)\\) to \\(F:J \\rightarrow C\\) such that for every other cone denoted \\((N,p)\\) to \\(F:J \\rightarrow C\\) there exists a unique morphism \\(u:N \\rightarrow L\\) such that \\(t(X) \\circ u = p(X)\\) for all \\(X\\) in \\(J\\).</p>\r\n    <img alt=\"\" src=\"images/image1.png\" style=\"width: 40%; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);\" title=\"\">\r\n    <p>Source: <a href=\"https://en.wikipedia.org/wiki/Limit_(category_theory)\">Wikipedia Limit (category theory)</a></p>\r\n\r\n<p>Now, just as \"the equalizer is the limit of the diagram,\" and every \"subobjects of some object are the equivalence classes of those monics that can be expressed as the equalizer of two morphisms\": for each monic \\(j:U \\rightarrow\r\n\r\n X\\) there is a unique morphism \\(h(j):X \\rightarrow \\Omega\\) where \\(\\Omega\\) is the subobject classifier for some category \\(C\\) with a terminal object satisfying \\(1 \\rightarrow \\Omega\\) in the commutative diagram of a pullback such that U is the limit.</p>\r\n <img alt=\"\" src=\"images/image3.png\" style=\"width: 15%; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);\" title=\"\">\r\n<p>Source: <a href=\"https://en.wikipedia.org/wiki/Subobject_classifier\">Wikipedia Subobject classifier</a></p>\r\n\r\n<p>At risk of redundancy, a pullback is the limit of a diagram with two morphisms \\(f:X \\rightarrow Z\\) and \\(g:Y \\rightarrow Z\\) where an object P and two morphisms \\(p_1:P \\rightarrow X\\) and \\(p_2:P \\rightarrow Y\\) where \\(fp_1 = gp_2\\), all denoted \\((P, p_1, p_2)\\), which is a universal property, meaning for any other triple denoted \\((Q, q_1, q_2)\\) where \\(q_1:Q \\rightarrow X\\) and \\(q_2:Q \\rightarrow Y\\) and \\(fq_1 = gq_2\\) there exists a unique \\(u:Q \\rightarrow P\\) such that \\(p_1 \\circ u = q_1\\) and \\(p_2 \\circ u = q_2\\). This is unique up to isomorphism.</p>\r\n<img alt=\"\" src=\"images/image4.png\" style=\"width: 40%; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);\" title=\"\">\r\n<p>Source: <a href=\"https://en.wikipedia.org/wiki/Pullback_(category_theory)\">Wikipedia Pullback (category theory)</a></p>\r\n\r\n<p>Finally, elementary topoi have many definitions. (To avoid beginning the question, this definition allows one to derive the existence of the subobject classifier rather than positing the subobject classifier to derive other properties.) \\(A\\) topos is a category with two properties: (1) every limit over a finite index category exists, (2) every object has a power object. \\(A\\) power object of an object \\(X\\) is a pair \\((P_X, \\ni_x)\\) with \\(\\ni_x \\subseteq P_X \\times X\\). Now, for every object I, a morphism \\(r:I \\rightarrow P_X\\) induces a subobject \\(\\{(i, x) \\mid x \\in r(i)\\} \\subseteq I \\times X\\). \\(A\\) pullback denoted \\(\\ni_x\\) defines this along \\(r \\times X : I \\times X \\rightarrow P_X \\times X\\).</p>\r\n\r\n<p>So, an isomorphic correspondence between relations \\(R \\subseteq I \\times X\\) and morphisms \\(r: I \\rightarrow P_X\\) is the universal property of a power object. From finite limits and power objects one can derive that:</p>\r\n<ol>\r\n<li>All colimits taken over finite index categories exist.</li>\r\n<li>The category has a subobject classifier.</li>\r\n<li>The category is Cartesian closed.</li>\r\n</ol>\r\n<p>A category \\(C\\) is Cartesian closed if it:</p>\r\n<ol>\r\n<li>Has a terminal object.</li>\r\n<li>Any two objects \\(X\\) and \\(Y\\) in \\(C\\) have a product \\(X \\times Y\\) in \\(C\\).</li>\r\n<li>Any two objects \\(Z\\) and \\(Y\\) in \\(C\\) have an exponential denoted \\(Z(Y)\\) in \\(C\\).</li>\r\n</ol>\r\n<p>Recall: an exponential is an object \\(Z(Y)\\) with a morphism \\((Z(Y) \\times Y) \\rightarrow Z\\) if for any object \\(X\\) and morphism \\(g: X \\times Y \\rightarrow Z\\) there is a unique morphism \\(\\lambda g: X \\rightarrow Z(Y)\\), and thus an isomorphism \\(g \\cong \\lambda g\\) often denoted \\(\\text{Hom}(X \\times Y, Z) \\cong \\text{Hom}(X, Z(Y))\\).</p>\r\n<img alt=\"\" src=\"images/image5.png\" style=\"width: 40%; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);\" title=\"\">\r\n<p>Source: <a href=\"https://en.wikipedia.org/wiki/Exponential_object\">Wikipedia Exponential object</a></p>\r\n<p>With all of this, we can see that the global elements of the subobject classifier denoted \\(\\Omega\\) of an elementary topos form a Heyting algebra (of truth values of the intuitionistic higher-order logic induced by the topos). More generally, the set of subobjects of any object \\(X\\) in a topos forms a Heyting algebra.</p>\r\n<p>The question: how to relate \"(1) every limit over a finite index category exists, (2) every object has a power object\" of my definition for elementary topos with the lattices and logical connectives described for Heyting algebras. Well, it is just a list of verifiable algebraic properties according to my GPT query for Lean code, for explications of tactics, and for a natural language explanation. Here's a list:</p>\r\n<ol>\r\n<li>Limits of finite diagrams exist in the category  \\(C\\) .</li>\r\n<li>Pullbacks exist in the category  \\(C\\) .</li>\r\n<li>Products exist in the category  \\(C\\) .</li>\r\n<li>Equalizers exist in the category  \\(C\\) .</li>\r\n<li>The infimum of subobjects  \\(P\\)  and  \\(Q\\)  is lower or equal to  \\(P\\) .</li>\r\n<li>If a subobject  \\(R\\)  is lower or equal to both  \\(P\\)  and  \\(Q\\) , it is also lower or equal to their infimum.</li>\r\n<li>There is an equivalence between \\(a ≤ b\\) and the implication \\(a ⊓ c ≤ b\\) for any subobject  \\(C\\) .</li>\r\n<li>The infimum of subobjects  \\(P\\)  and  \\(Q\\)  is lower or equal to  \\(Q\\) .</li>\r\n<li>Subobject  \\(P\\)  is lower or equal to the supremum of  \\(P\\)  and  \\(Q\\) .</li>\r\n<li>Subobject  \\(Q\\)  is lower or equal to the supremum of  \\(P\\)  and  \\(Q\\) .</li>\r\n<li>If a subobject  \\(R\\)  is greater or equal to both  \\(P\\)  and  \\(Q\\) , it is also greater or equal to their supremum.</li>\r\n</ol>\r\n\r\n\r\n<h2 id=\"adjunction-boolean\">Adjunction Boolean</h2>\r\n\r\n<p>(See nLab <a href=\"https://ncatlab.org/nlab/show/Heyting+algebra\">ʜᴇʏᴛɪɴɢ ᴀʟɢᴇʙʀᴀ</a> for source)</p>\r\n<div class=\"card\"><div class=\"card-body\">\r\n<p>There are several ways of passing back and forth between Boolean algebras and ʜᴇʏᴛɪɴɢ ᴀʟɢᴇʙʀʌs, having to do with the double negation operator. Recall, double negation \\(¬¬: L → L\\) is a monad. Further, it preserves finite meets.</p>\r\n<p>Now let \\(L(\\neg\\neg)\\) denote the poset of regular elements of \\(L\\), that is, those elements \\(x\\) such that \\(\\neg\\neg x = x\\). (When \\(L\\) is the topology of a space, an open set \\(U\\) is regular if and only if it is the interior of its closure, that is if it a regular element of the ʜᴇʏᴛɪɴɢ ᴀʟɢᴇʙʀʌ of open sets described above.)</p>\r\n<p>A Theorem: The poset \\(L(\\neg\\neg)\\) is a Boolean algebra. Moreover, the assignment \\(L \\mapsto L(\\neg\\neg)\\) is the object part of a functor \\(F: \\text{Heyt} \\to \\text{Bool}\\) called Booleanization, which is left adjoint to the full and faithful inclusion \\(i: \\text{Bool} \\rightarrow \\text{Heyt}\\)</p>\r\n<p>The unit of the adjunction, applied to a ʜᴇʏᴛɪɴɢ ᴀʟɢᴇʙʀʌ \\(L\\), is the map \\(L \\to L(\\neg\\neg)\\) which maps each element \\(x\\) to its regularization \\(\\neg\\neg x\\).</p>\r\n<p><em>Proof.</em> To avoid confusion from two different maps called \\(\\neg\\neg\\) that differ only in their codomain, write \\(M: L \\to L\\) for the monad and \\(U: L \\to L(\\neg\\neg)\\) for the left adjoint. Write \\(\\iota: L(\\neg\\neg) \\to L\\) for the right adjoint, so that \\(M = \\iota \\circ U\\).</p>\r\n<p>We first show that \\(L(\\neg\\neg)\\) is a ʜᴇʏᴛɪɴɢ ᴀʟɢᴇʙʌ and \\(U\\) is a Heyting-algebra map. Because \\(U\\) is surjective (and monotone), it suffices to show that it preserves the Heyting-algebra operators: finite meets, finite joins, and implication.</p>\r\n<p>Because \\(\\iota\\) is full, it reflects meets. Therefore, because \\(\\neg\\neg: L \\to L\\) preserves finite meets, \\(M = \\iota \\circ U\\) preserves finite meets, so too does \\(U\\), and as a left adjoint, it preserves joins.</p>\r\n<p>Finally, for \\(a, b \\in L\\), we show that \\(\\neg\\neg(a \\to b)\\), which (because \\(\\neg\\neg: L \\to L\\) preserves finite meets) is the \\(L\\)-implication \\(\\neg\\neg a \\to \\neg\\neg b\\), satisfies the universal property (\\(1\\)), also in \\(L(\\neg\\neg)\\). For any \\(x \\in L(\\neg\\neg)\\), \\(\\iota(x) \\leq \\neg\\neg a \\to_\\mathrm{L} \\neg\\neg b\\) just if \\(\\iota(x) \\land_\\mathrm{L} \\neg\\neg a \\leq \\neg\\neg b\\) by the universal property in \\(L\\); but because \\(\\iota\\) reflects meets, this is equivalent to \\(x \\land_\\mathrm{L(\\neg\\neg)} (\\neg\\neg a) \\leq \\neg\\neg b\\), completing the universal property.</p>\r\n<p>Now because \\(U\\) preserves implication and (the empty join) \\(0\\), it preserves negation. Therefore, \\(\\neg\\neg\\) in \\(L(\\neg\\neg)\\) is the identity, so the latter is a Boolean algebra.</p>\r\n<p>Therefore \\(U = \\neg\\neg: L \\to L(\\neg\\neg)\\) is a ʜᴇʏᴛɪɴɢ ᴀʟɢᴇʙʌ quotient which is the coequalizer of \\(1\\), \\(\\neg\\neg: L \\to L\\). It follows that a ʜᴇʏᴛɪɴɢ ᴀʟɢᴇʙʌ map \\(L \\to B\\) to any Boolean algebra \\(B\\), i.e., any ʜᴇʏᴛɪɴɢ ᴀʟɢᴇʙʌ where \\(1\\) and \\(\\neg\\neg\\) coincide, factors uniquely through this coequalizer, and the induced map \\(L(\\neg\\neg) \\to B\\) is a Boolean algebra map. In other words, \\(\\neg\\neg: L \\to L(\\neg\\neg)\\) is the universal ʜᴇʏᴛɪɴɢ ᴀʟɢᴇʙʌ map to a Boolean algebra, which establishes the adjunction.</p>\r\n</div></div><br>\r\n\r\n<h2 id=\"quantifiers-as-adjoints\">Quantifiers as Adjoints</h2>\r\n<p>\r\n  (See\r\n  <a href=\"https://www.google.com/url?q=https://math.stackexchange.com/q/452795/1098426&amp;sa=D&amp;source=editors&amp;ust=1688321707808674&amp;usg=AOvVaw2DDZ2-k228Ur5-IiVPwFxy\">\"Quantifiers as Adjoints...\". Math Stack Exchange</a>\r\n  for source)\r\n</p>\r\n<div class=\"card\"><div class=\"card-body\">\r\n<p>\r\n  What is a predicate \\(P(x)\\), with the variable \\(x\\) coming from the sort \\(X\\)? Viewing \\(X\\) as a set, we can interpret \\(P(x)\\) as a subset of \\(X\\). The definable subsets of \\(X\\) naturally form a partially ordered category, where an arrow \\(P(x) \\rightarrow Q(x)\\) exists if and only if \\(P \\subseteq Q\\). This category structure can also be seen as the partial order of implications between predicates. Let's denote this category as Pred(\\(X\\)).\r\n</p>\r\n<p>\r\n  Now, there is a functor \\(Dy: \\text{Pred}(X) \\rightarrow \\text{Pred}(X \\times Y)\\) that corresponds to adding a dummy variable \\(y\\) of type \\(Y\\). So, \\(Dy(P(x)) = \\{(x, y) \\in X \\times Y \\mid x \\in P(x)\\}\\). We can write this as \\(P(x, y)\\), where the dot denotes that \\(y\\) is a dummy variable. This is indeed a functor, as if \\(P(x) \\rightarrow Q(x)\\), then \\(P(x, y) \\rightarrow Q(x, y)\\) (again, you can interpret this as containment or implication).\r\n</p>\r\n<p>\r\n  There are also functors \\(\\exists y: \\text{Pred}(X \\times Y) \\rightarrow \\text{Pred}(X)\\) and \\(\\forall y: \\text{Pred}(X \\times Y) \\rightarrow \\text{Pred}(X)\\) that behave as follows:\r\n</p>\r\n<p>\r\n  - \\(\\exists yP(x, y) = \\{x \\in X \\mid \\exists y(x, y) \\in P(x, y)\\}\\).\r\n</p>\r\n<p>\r\n  - \\(\\forall yP(x, y) = \\{x \\in X \\mid \\forall y(x, y) \\in P(x, y)\\}\\).\r\n</p>\r\n<p>\r\n  Now, we have the adjunctions \\(\\exists y \\dashv Dy \\dashv \\forall y\\). These adjunctions correspond to the following statements:\r\n</p>\r\n<p>\r\n  - \\(\\exists yP(x, y) \\rightarrow Q(x) \\iff P(x, y) \\rightarrow Q(x, y)\\).\r\n</p>\r\n<p>\r\n  - \\(P(x) \\rightarrow \\forall yQ(x, y) \\iff P(x, y) \\rightarrow Q(x, y)\\).\r\n</p>\r\n<p>\r\n  Moving on to viewing predicates as maps \\(X \\rightarrow \\{0,1\\}\\), we can define a corresponding partial order structure on maps \\(X \\rightarrow \\{0,1\\}\\). Specifically, \\(P(x) \\rightarrow Q(x)\\) if and only if for all \\(x \\in X\\), \\(P(x) \\leq Q(x)\\). In other words, the partial order is defined by pointwise comparison of functions.\r\n</p>\r\n<p>\r\n  The functor \\(Dy\\) now takes a map \\(X \\rightarrow \\{0,1\\}\\) and composes it with the projection \\(X \\times Y \\rightarrow X\\), and its adjoints are given by:\r\n</p>\r\n<p>\r\n  - \\((\\exists yP(x, y))(x) = \\{1 \\text{ if } \\exists yP(x, y) = 1, 0 \\text{ otherwise}\\} = \\sup_{y \\in Y} P(x, y)\\).\r\n</p>\r\n<p>\r\n  - \\((\\forall yP(x, y))(x) = \\{1 \\text{ if } \\forall yP(x, y) = 1, 0 \\text{ otherwise}\\} = \\inf_{y \\in Y} P(x, y)\\).\r\n</p>\r\n<p>\r\n  By replacing \\(\\{0,1\\}\\) with \\([0,1]\\), we can generalize this concept further. We take the same category structure on predicates (pointwise \\(\\leq\\) in \\([0,1]\\)) and obtain the quantifiers in continuous model theory as adjoints to the \"dummy variable functor,\" replacing max and min with sup and inf.\r\n</p>\r\n<p>\r\n  Regarding your second question, if the compact Hausdorff space is not ordered, it is not clear what the appropriate category structure should be since the one where the quantifiers naturally appear stems directly from the order. However, it is easier to generalize this situation by replacing \\(\\{0,1\\}\\) and \\([0,1]\\) with any complete lattice, which is what happens in topos theory.\r\n</p>\r\n</div></div><br>\r\n\r\n<h2 id=\"topoi-of-heyting-algebra-models\">Topoi of ʜᴇʏᴛɪɴɢ ᴀʟɢᴇʙʀᴀ Models</h2>\r\n<p>(See <a href=\"https://www.google.com/url?q=https://math.stackexchange.com/q/4506152/1098426&amp;sa=D&amp;source=editors&amp;ust=1688321707815342&amp;usg=AOvVaw31M2RPlEaOas6XPwEkKAzC\">&quot;Is there a topos of Heyting...&quot;,Math Stack Exchange</a> for source)</p>\r\n<div class=\"card\"><div class=\"card-body\">\r\n    \r\n    <p>Let \\(Heyt\\) be the category of Heyting algebras. We first describe a functor \\(F: Set^{op} \\rightarrow Heyt\\). This functor is simply the functor sending \\(X\\) to \\(H^X\\). In fact, \\(F\\) always outputs a complete Heyting algebra since \\(H\\) is complete.</p>\r\n    <p>Let us note that for all \\(f: A \\rightarrow B\\), the map \\(Ff: FB \\rightarrow FA\\) preserves all meets and all joins. Therefore, \\(Ff\\) has a left adjoint \\(\\exists f\\) and a right adjoint \\(\\forall f: FA \\rightarrow FB\\). Moreover, these adjoints satisfy the Beck-Chevalley condition. This gives us a \"first-order hyperdoctrine with equality\".</p>\r\n    <p>From here, we can construct the category of partial equivalence relations.</p>\r\n    <p>In a bit more generality, consider a category \\(C\\) with finite limits and a functor \\(F: C^{op} \\rightarrow Heyt\\), where for all \\(f: A \\rightarrow B\\), \\(Ff\\) has a left and a right adjoint which satisfy the Beck-Chevalley condition. Such a functor is known as a first-order hyperdoctrine with equality. We write \\(f^{-1}\\) instead of \\(Ff\\).</p>\r\n    <p>A partial equivalence relation consists of an object \\(A \\in C\\), together with some predicate \\(P \\in F(A \\times A)\\), which satisfies the following conditions:</p>\r\n    <p>1. Consider the \"swap map\" \\(swap = (p2, p1): A^2 \\rightarrow A^2\\). Then \\(swap^{-1}(P) = P\\). This is \"symmetry\". It corresponds to saying \\(\\forall x, y \\in A (P(x, y) \\iff P(y, x))\\).</p>\r\n    <p>2. Consider the three maps \\((p1, p2), (p1, p3), (p2, p3): A^3 \\rightarrow A^2\\). We have \\((p1, p2)^{-1}(P) \\land (p2, p3)^{-1}(P) \\leq (p1, p3)^{-1}(P)\\). This is \"transitivity\". It corresponds to saying \\(\\forall x, y, z \\in A (P(x, y) \\land P(y, z) \\rightarrow P(x, z))\\).</p>\r\n    <p>The arrows from \\((A, P)\\) to \\((B, Q)\\) will consist of \\(f \\in F(A \\times B)\\) which satisfies the following conditions:</p>\r\n    <p>1. Consider \\(p1: A \\times B \\rightarrow A\\). Then \\(\\exists p1f = P\\). This is \"having the correct domain\" - it corresponds to saying \\(\\forall x \\in A (P(x) \\rightarrow \\exists y \\in B (f(x, y)))\\).</p>\r\n    <p>\r\n        2. Consider \\(p_2: A \\times B \\rightarrow B\\). Then \\(\\exists p_2f \\leq Q\\). This is \"having the correct codomain\". It corresponds to saying \\(\\forall y \\in B (\\exists x \\in A (f(x, y)) \\rightarrow Q(y, y))\\).\r\n        </p>\r\n        <p>\r\n        3. Consider \\((p_1, p_2), (p_1, p_3): A \\times B^2 \\rightarrow A \\times B\\) and \\((p_2, p_3): A \\times B^2 \\rightarrow B^2\\). Then \\((p_1, p_2)^{-1}f \\land (p_1, p_3)^{-1}f \\leq (p_2, p_3)^{-1}(Q)\\). This is known as \"being single-valued\" and corresponds to saying \\(\\forall x \\in A \\forall y, z \\in B (f(x, y) \\land f(x, z) \\rightarrow Q(y, z))\\).\r\n        </p>\r\n        <p>\r\n        Using these tools, we can fairly easily define composition. Given \\(f: (A, P) \\rightarrow (B, Q)\\) and \\(g: (B, Q) \\rightarrow (C, R)\\), we can define \\(g \\circ f: (A, P) \\rightarrow (C, R)\\) as follows. We have\r\n        </p>\r\n        <p>\r\n        &nbsp;&nbsp;maps \\((p_1, p_2): A \\times B \\times C \\rightarrow A \\times B\\), \\((p_1, p_3): A \\times B \\times C \\rightarrow A \\times C\\), and \\((p_2, p_3): A \\times B \\times C \\rightarrow B \\times C\\). Then \\(g \\circ f = \\exists (p_1, p_2)((p_1, p_2)^{-1}(f) \\land (p_2, p_3)^{-1}(g))\\). This corresponds to defining \\(g \\circ f = \\{(x, z) \\in A \\times C | \\exists y \\in B (f(x, y) \\land g(y, z))\\}\\).\r\n        </p>\r\n        <p>\r\n        One can easily show that this gives us a category. In fact, it gives us a Heyting category - a category in which we can interpret first-order intuitionist logic. This type of category has finite limits, an initial object, Heyting algebra structures for subobjects, and left and right adjoints to pullbacks for monos.\r\n        </p>\r\n        <p>\r\n        We now add one final condition on \\(F\\). The condition is:\r\n        </p>\r\n        <p>\r\n        For every object \\(X\\) of \\(C\\), there exists an object \\(PX\\) and an element \\(in_X \\in F(X \\times PX)\\) such that for all \\(Y \\in C\\) and \\(Q \\in F(X \\times Y)\\), there exists some (not necessarily unique) \\(f: Y \\rightarrow PX\\) with \\((1_X \\times f)^{-1}(in_X) = Q\\).\r\n        </p>\r\n        <p>\r\n        This makes \\(F\\) into a \"tripos\". In the particular case \\(FX = HX\\), we see that we can construct \\(P(X) = H^X\\) with \\(in_X: X \\times HX \\rightarrow H\\) being the obvious map.\r\n        </p>\r\n        <p>\r\n        Using the tripos condition, we can prove that the resulting category of partial equivalence relations is\r\n        \r\n        , in fact, a topos. Finally, we have the following theorem:\r\n        </p>\r\n        <p>\r\n        Thm: The tripos-to-topos construction applied to the functor \\(X \\mapsto HX\\) produces a topos which is equivalent to \\(Sh(H)\\).\r\n        </p>\r\n        <p>\r\n        So the \"correct\" way of constructing \\(H\\)-valued types just gives us the topos of sheaves on the locale \\(H\\).\r\n        </p>\r\n        <p>\r\n        There are a few other interesting ways of getting triposes. The first is the \"syntactic tripos\". Let's say we're working with intuitionist higher-order logic. Then let \\(C\\) be the category of types, where the objects are (products of) types, the arrows are function terms, and \\(F(T)\\) is the Heyting algebra of predicates on \\(T\\). The tripos-to-topos construction then produces a topos where the true statements in the internal logic exactly correspond to provable statements.\r\n        </p>\r\n        <p>\r\n        The second is, of course, starting with \\(C\\) being the topos and taking the sub-object functor. The tripos-to-topos construction will simply produce a topos which is equivalent to \\(C\\) - this shouldn't be terribly surprising.\r\n        </p>\r\n        <p>\r\n        The third is using a partial combinatory algebra. This is related to Kleene's realizability models for arithmetic and can be used to construct a topos where, for example, we can show in the internal logic that all functions \\(N \\rightarrow N\\) are computable. It can also be used to model Brouwer's intuitionism.\r\n        </p>\r\n        \r\n\r\n</div></div><br>\r\n</div>\r\n<!-- <footer  class=\"bg-light pt-5 pb-2 px-5\"><a href=\"/\">Home</a><hr><p>Copyright © 2023 Izak</p></footer> -->\r\n\r\n<!--HONESTLY, I DO NOT KNOW WHICH OF THESE SCRIPTS ARE NECESSARY DEPENDENCIES-->\r\n<script src=\"https://code.jquery.com/jquery-3.4.1.min.js\"></script>\r\n<script src=\"https://code.jquery.com/ui/1.12.1/jquery-ui.js\"></script>\r\n<script src=\"https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js\" integrity=\"sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo\" crossorigin=\"anonymous\"></script>\r\n<script src=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js\" integrity=\"sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6\" crossorigin=\"anonymous\"></script>\r\n</body>\r\n</html>",
      "date_published": "2023-04-30T17:00:00-07:00"
    },{
      "id": "https://ischmidls.github.io/posts/pages/translate/",
      "url": "https://ischmidls.github.io/posts/pages/translate/",
      "title": "translate",
      "content_html": "<!DOCTYPE html>\r\n<html>\r\n<head>\r\n  <title>Language Translation</title>\r\n</head>\r\n<body>\r\n  <label for=\"input-text\">Enter text to translate:</label>\r\n  <textarea id=\"input-text\" rows=\"4\" cols=\"50\"></textarea><br><br>\r\n  <button onclick=\"strTranslate()\">Translate</button>\r\n  <h2>Translations:</h2>\r\n<table>\r\n    <tr>\r\n      <th><span>German:</span></th>\r\n      <th><span>French:</span></th>\r\n      <th><span>Japanese:</span></th>\r\n      <th><span>Russian:</span></th>\r\n      <th><span>Arabic:</span></th>\r\n      <th><span>Mandarin:</span></th>\r\n      <th><span>Spanish:</span></th>\r\n      <th><span>Italian:</span></th>\r\n    </tr>\r\n    <tr>\r\n      <td><span id=\"output-de\"></span></td>\r\n      <td><span id=\"output-fr\"></span></td>\r\n      <td><span id=\"output-ja\"></span></td>\r\n      <td><span id=\"output-ru\"></span></td>\r\n      <td><span id=\"output-ar\"></span></td>\r\n      <td><span id=\"output-zh\"></span></td>\r\n      <td><span id=\"output-es\"></span></td>\r\n      <td><span id=\"output-it\"></span></td>\r\n    </tr>\r\n  </table>\r\n\r\n  <script>\r\n    function strTranslate() {\r\n      const input = document.getElementById(\"input-text\").value;\r\n\r\n      // German\r\n      fetch(`https://translate.googleapis.com/translate_a/single?client=gtx&sl=auto&tl=de&dt=t&q=${encodeURIComponent(input)}`)\r\n        .then(response => response.json())\r\n        .then(data => document.getElementById(\"output-de\").innerHTML = data[0][0][0]);\r\n\r\n      // French\r\n      fetch(`https://translate.googleapis.com/translate_a/single?client=gtx&sl=auto&tl=fr&dt=t&q=${encodeURIComponent(input)}`)\r\n        .then(response => response.json())\r\n        .then(data => document.getElementById(\"output-fr\").innerHTML = data[0][0][0]);\r\n\r\n      // Japanese\r\n      fetch(`https://translate.googleapis.com/translate_a/single?client=gtx&sl=auto&tl=ja&dt=t&q=${encodeURIComponent(input)}`)\r\n        .then(response => response.json())\r\n        .then(data => document.getElementById(\"output-ja\").innerHTML = data[0][0][0]);\r\n\r\n      // Russian\r\n      fetch(`https://translate.googleapis.com/translate_a/single?client=gtx&sl=auto&tl=ru&dt=t&q=${encodeURIComponent(input)}`)\r\n        .then(response => response.json())\r\n        .then(data => document.getElementById(\"output-ru\").innerHTML = data[0][0][0]);\r\n      // Arabic\r\n      fetch(`https://translate.googleapis.com/translate_a/single?client=gtx&sl=auto&tl=ar&dt=t&q=${encodeURIComponent(input)}`)\r\n        .then(response => response.json())\r\n        .then(data => document.getElementById(\"output-ar\").innerHTML = data[0][0][0]);\r\n\r\n      // Mandarin\r\n      fetch(`https://translate.googleapis.com/translate_a/single?client=gtx&sl=auto&tl=zh&dt=t&q=${encodeURIComponent(input)}`)\r\n        .then(response => response.json())\r\n        .then(data => document.getElementById(\"output-zh\").innerHTML = data[0][0][0]);\r\n\r\n      // Spanish\r\n      fetch(`https://translate.googleapis.com/translate_a/single?client=gtx&sl=auto&tl=es&dt=t&q=${encodeURIComponent(input)}`)\r\n        .then(response => response.json())\r\n        .then(data => document.getElementById(\"output-es\").innerHTML = data[0][0][0]);\r\n      \r\n      // Italian\r\n      fetch(`https://translate.googleapis.com/translate_a/single?client=gtx&sl=auto&tl=it&dt=t&q=${encodeURIComponent(input)}`)\r\n        .then(response => response.json())\r\n        .then(data => document.getElementById(\"output-it\").innerHTML = data[0][0][0]);\r\n    }\r\n  </script>\r\n</body>\r\n</html>\r\n",
      "date_published": "2023-01-31T16:00:00-08:00"
    },{
      "id": "https://ischmidls.github.io/posts/pages/occurcount/",
      "url": "https://ischmidls.github.io/posts/pages/occurcount/",
      "title": "occurance count",
      "content_html": "<!doctype html>\r\n<html lang=\"en\" dir=\"ltr\">\r\n  <head>\r\n    <meta charset=\"utf-8\">\r\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\r\n    <link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css\" integrity=\"sha384-B0vP5xmATw1+K9KRQjQERJvTumQW0nPEzvF6L/Z6nronJ3oUOFUFpCjEUQouq2+l\" crossorigin=\"anonymous\">\r\n    <!--<link rel=\"stylesheet\" href=\"css/main.css\">-->\r\n    \r\n        <title>Word Count</title>\r\n    <script src=\"https://cdn.plot.ly/plotly-2.20.0.min.js\"></script>\r\n    <script>\r\n      function countWords() {\r\n        // Get the input string\r\n        var inputString = document.getElementById(\"inputString\").value.toLowerCase();\r\n        // Split the input string into an array of words\r\n        var wordsArray = inputString.match(/[a-zA-Z0-9]+/g);\r\n        // Create an empty object to hold the word counts\r\n        var wordCounts = {};\r\n        // Loop through the words and count their occurrences\r\n        if (wordsArray === null) {\r\n          alert(\"Silly, you didn't put any words in!\")\r\n          throw new Error(\"user gave no words\")\r\n        }\r\n        for (var i = 0; i < wordsArray.length; i++) {\r\n          if (wordCounts[wordsArray[i]]) {\r\n            wordCounts[wordsArray[i]]++;\r\n          } else {\r\n            wordCounts[wordsArray[i]] = 1;\r\n          }\r\n        }\r\n        // Convert the word counts object into an array of pairs\r\n        var pairsArray = [];\r\n        for (var word in wordCounts) {\r\n          pairsArray.push([word, wordCounts[word]]);\r\n        }\r\n        // Return the array of pairs\r\n        return pairsArray;\r\n      }\r\n\r\n      function displayWordCounts() {\r\n        // Call the countWords function and get the array of pairs\r\n        try {\r\n          var pairsArray = countWords();\r\n        }\r\n        catch(err) { console.log(err.message)}\r\n        // Get the output element\r\n        var outputElement = document.getElementById(\"output\");\r\n        // Clear the output element\r\n        outputElement.innerHTML = \"<h2>Counts</h2>\";\r\n        // Loop through the pairs array and add each pair to the output element\r\n        for (var i = 0; i < pairsArray.length; i++) {\r\n          var pair = pairsArray[i];\r\n          var word = pair[0];\r\n          var count = pair[1];\r\n          outputElement.innerHTML += word + \": \" + count + \" <br> \";\r\n        }\r\n        displayPlotlyPairs(pairsArray);\r\n      }\r\n\r\n      function displayPlotlyPairs(pairsArray) {\r\n        // Extract the words and counts from the pairs array\r\n        var words = pairsArray.map(pair => pair[0]);\r\n        var counts = pairsArray.map(pair => pair[1]);\r\n        // Create the trace for the bar chart\r\n        var trace = {\r\n          x: words,\r\n          y: counts,\r\n          type: \"bar\"\r\n        };\r\n        // Create the layout for the chart\r\n        var layout = {\r\n          title: \"Word Frequencies\",\r\n          xaxis: {\r\n            title: \"Words\",\r\n            categoryorder:'total descending'\r\n          },\r\n          yaxis: {\r\n            title: \"Counts\"\r\n          },\r\n          width: 2000,\r\n          height: 1500,\r\n        };\r\n        // Create the data array\r\n        var data = [trace];\r\n        // Display the chart using Plotly.newPlot\r\n        Plotly.newPlot(\"chartDiv\", data, layout);\r\n      }\r\n    </script>\r\n    <style> \r\n    textarea {\r\n      width: 90%;\r\n      height: 80vh;\r\n    }\r\n    #output, #chartDiv {\r\n      outline:auto;\r\n      padding: 3em;\r\n      overflow: auto;\r\n    }\r\n    </style>\r\n  \r\n  </head>\r\n  <body>\r\n    <div class=\"container\" id=\"bsr-wrapper\">\r\n      <h1>Word Count</h1>\r\n    <p>Enter some text:</p>\r\n    <textarea id=\"inputString\"></textarea>\r\n    <br>\r\n    <button onclick=\"displayWordCounts()\">Count Words</button>\r\n    <br>\r\n    <h2>Plot & List!</h2>\r\n    <div id=\"chartDiv\"></div>\r\n    <div id=\"output\"></div>\r\n      \r\n    </div>\r\n    <!-- <footer  class=\"bg-light pt-5 pb-2 px-5\"><a href=\"/\">Home</a></footer> -->\r\n    \r\n    <script src=\"https://code.jquery.com/jquery-3.4.1.min.js\"></script>\r\n    <script src=\"https://code.jquery.com/ui/1.12.1/jquery-ui.js\"></script>\r\n    <script src=\"https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js\" integrity=\"sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo\" crossorigin=\"anonymous\"></script>\r\n    <script src=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js\" integrity=\"sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6\" crossorigin=\"anonymous\"></script>\r\n    <!--<script src=\"js/main.js\"></script>-->\r\n  </body>\r\n</html>\r\n",
      "date_published": "2023-01-31T16:00:00-08:00"
    },{
      "id": "https://ischmidls.github.io/posts/pages/tabaway/",
      "url": "https://ischmidls.github.io/posts/pages/tabaway/",
      "title": "tab away",
      "content_html": "<!DOCTYPE html>\r\n<html>\r\n<head>\r\n  <title>Remove Tabs</title>\r\n</head>\r\n<body>\r\n  <label for=\"input-text\">Enter text:</label>\r\n  <textarea id=\"input-text\" rows=\"4\" cols=\"50\"></textarea><br><br>\r\n  <button onclick=\"removeTabs()\">Remove Tabs</button>\r\n  <h2>Result:</h2>\r\n  <div id=\"output\"></div>\r\n\r\n  <script>\r\n    function removeTabs() {\r\n      const input = document.getElementById(\"input-text\").value;\r\n      const output = input.replace(/\\t/g, \"\");\r\n      const formattedOutput = output.replace(/\\n/g, \"<br>\");\r\n      document.getElementById(\"output\").innerHTML = formattedOutput;\r\n    }\r\n  </script>\r\n</body>\r\n</html>\r\n",
      "date_published": "2022-12-31T16:00:00-08:00"
    },{
      "id": "https://ischmidls.github.io/posts/pages/mix/",
      "url": "https://ischmidls.github.io/posts/pages/mix/",
      "title": "mix list",
      "content_html": "<!DOCTYPE html>\r\n<html>\r\n<head>\r\n  <title>Randomly Mix Elements</title>\r\n  <script>\r\n    function mixElements() {\r\n      var inputString = document.getElementById(\"inputString\").value;\r\n      \r\n      // Split the input string into an array\r\n      var elementsArray = inputString.split(\",\");\r\n      \r\n      // Randomly shuffle the elements array\r\n      for (var i = elementsArray.length - 1; i > 0; i--) {\r\n        var j = Math.floor(Math.random() * (i + 1));\r\n        var temp = elementsArray[i];\r\n        elementsArray[i] = elementsArray[j];\r\n        elementsArray[j] = temp;\r\n      }\r\n      \r\n      // Join the elements array into a mixed-up string\r\n      var mixedString = elementsArray.join(\",\");\r\n      \r\n      // Display the mixed-up string\r\n      document.getElementById(\"result\").innerHTML = mixedString;\r\n    }\r\n  </script>\r\n</head>\r\n<body>\r\n  <label for=\"inputString\">Enter a comma-delimited string:</label>\r\n  <input type=\"text\" id=\"inputString\">\r\n  <button onclick=\"mixElements()\">Mix Elements</button>\r\n  <p id=\"result\"></p>\r\n</body>\r\n</html>\r\n",
      "date_published": "2022-12-31T16:00:00-08:00"
    },{
      "id": "https://ischmidls.github.io/posts/pages/first%20sentence/",
      "url": "https://ischmidls.github.io/posts/pages/first%20sentence/",
      "title": "first sentence",
      "content_html": "<!DOCTYPE html>\r\n<html>\r\n  <head>\r\n    <meta charset=\"UTF-8\">\r\n    <title>Wikipedia Page First Sentence</title>\r\n  </head>\r\n  <body>\r\n    <form id=\"myForm\">\r\n      <label for=\"url\">Enter Wikipedia page URL:</label>\r\n      <input type=\"text\" id=\"url\" name=\"url\">\r\n      <input type=\"submit\" value=\"Get First Sentence\">\r\n    </form>\r\n    <br>\r\n    <p>BONUS: If you know the page title, the program works with that too.</p>\r\n    <p>(e.g.) \"Gilles Deleuze\" instead of \"https://en.wikipedia.org/wiki/Gilles_Deleuze\"</p>\r\n    <br>\r\n    <div id=\"result\"></div>\r\n\r\n    <script>\r\n      const form = document.getElementById('myForm');\r\n      const resultDiv = document.getElementById('result');\r\n\r\n      form.addEventListener('submit', async (event) => {\r\n        event.preventDefault();\r\n        const urlInput = document.getElementById('url');\r\n        const url = urlInput.value;\r\n        const queryArray = url.split('/')\r\n\r\n        const proxy = new Proxy(queryArray, {\r\n            get(target, prop) {\r\n                if (!isNaN(prop)) {\r\n                    prop = parseInt(prop, 10);\r\n                    if (prop < 0) {\r\n                        prop += target.length;\r\n                    }\r\n                }\r\n                return target[prop];\r\n            }\r\n        });\r\n\r\n        const query = proxy[-1]\r\n        \r\n        const apiEndpoint = `https://en.wikipedia.org/api/rest_v1/page/summary/${query}`;\r\n        try {\r\n          const response = await fetch(apiEndpoint);\r\n          const data = await response.json();\r\n          // resultDiv.innerText = 'result'+ data.stringify\r\n          const firstSentence = data.extract.split('.')[0] + '.';\r\n          resultDiv.innerText = firstSentence;\r\n        } catch (error) {\r\n          console.error(error);\r\n          resultDiv.innerText = 'Error retrieving first sentence';\r\n        }\r\n      });\r\n    </script>\r\n  </body>\r\n</html>\r\n",
      "date_published": "2022-12-31T16:00:00-08:00"
    },{
      "id": "https://ischmidls.github.io/posts/pages/countsplit/",
      "url": "https://ischmidls.github.io/posts/pages/countsplit/",
      "title": "count split",
      "content_html": "<!DOCTYPE html>\r\n<html>\r\n  <head>\r\n    <title>Substring Splitter</title>\r\n  </head>\r\n  <body>\r\n    <label for=\"inputString\">Input string:</label>\r\n    <input type=\"text\" id=\"inputString\"><br><br>\r\n    <label for=\"maxChar\">Max characters:</label>\r\n    <input type=\"number\" id=\"maxChar\"><br><br>\r\n    <button onclick=\"splitString()\">Split string</button><br><br>\r\n    <label for=\"output\">Output:</label>\r\n    <textarea id=\"output\" rows=\"10\" cols=\"50\"></textarea>\r\n\r\n    <script>\r\n      function splitString() {\r\n        var input = document.getElementById(\"inputString\").value;\r\n        var max = parseInt(document.getElementById(\"maxChar\").value);\r\n        var substrings = [];\r\n\r\n        for (var i = 0; i < input.length; i += max) {\r\n          substrings.push(input.substring(i, i + max));\r\n        }\r\n\r\n        document.getElementById(\"output\").value = substrings.join(\"\\n\\n\\n\");\r\n      }\r\n    </script>\r\n  </body>\r\n</html>\r\n",
      "date_published": "2022-12-31T16:00:00-08:00"
    },{
      "id": "https://ischmidls.github.io/posts/pages/countlines/",
      "url": "https://ischmidls.github.io/posts/pages/countlines/",
      "title": "count lines",
      "content_html": "<!DOCTYPE html>\r\n<html>\r\n<head>\r\n\t<title>Print Substring</title>\r\n</head>\r\n<body>\r\n\t<label for=\"end\">Enter an integer (N):</label>\r\n\t<input type=\"number\" id=\"end\">\r\n\t<button onclick=\"printSubstring()\">Print Substring</button>\r\n    <p><em>Enumerate paragraphs with (§N) where N ∈ 0, 1, 2, 3, ...</em></p>\r\n\t<p id=\"output\"></p>\r\n\t<script>\r\n\t\tfunction printSubstring() {\r\n\t\t\tconst end = document.getElementById(\"end\").value;\r\n\t\t\tlet output = \"\";\r\n\t\t\tfor (let i = 1; i <= end; i++) {\r\n\t\t\t\toutput += `(§${i})<br>`;\r\n\t\t\t}\r\n\t\t\tdocument.getElementById(\"output\").innerHTML = output;\r\n\t\t}\r\n\t</script>\r\n</body>\r\n</html>\r\n",
      "date_published": "2022-12-31T16:00:00-08:00"
    },{
      "id": "https://ischmidls.github.io/posts/pages/mathtax/",
      "url": "https://ischmidls.github.io/posts/pages/mathtax/",
      "title": "math education taxonomy",
      "content_html": "<!doctype html>\r\n<html lang=\"en\" dir=\"ltr\">\r\n  <head>\r\n    <meta charset=\"utf-8\">\r\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\r\n    <link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css\" integrity=\"sha384-B0vP5xmATw1+K9KRQjQERJvTumQW0nPEzvF6L/Z6nronJ3oUOFUFpCjEUQouq2+l\" crossorigin=\"anonymous\">\r\n    <title>Math Taxonomy</title>\r\n    <style> img {width: 90vw;}</style>\r\n  </head>\r\n  <body>\r\n    <div class=\"container\" id=\"bsr-wrapper\">\r\n        <main  class=\"tmpl-post\" >\r\n    <h1>Math Taxonomy</h1>\r\n\r\n<p>Izak, <time datetime=\"2022-07-16\">16 Jul 2022</time></p>\r\n\r\n\r\n      <hr>\r\n        <h2>My Final Visual</h2>\r\n        <p>Tap or click to interact!</p>\r\n        <!-- PLOTLY OUTPUT -->\r\n        <div>                        <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\r\n            <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"a9240c6c-37ff-4f48-b622-c68e751e2bd8\" class=\"plotly-graph-div\" style=\"height:100%; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"a9240c6c-37ff-4f48-b622-c68e751e2bd8\")) {                    Plotly.newPlot(                        \"a9240c6c-37ff-4f48-b622-c68e751e2bd8\",                        [{\"ids\":[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"293\",\"294\",\"295\",\"296\",\"297\",\"13\",\"91\",\"298\",\"299\",\"300\",\"301\",\"302\",\"303\",\"304\",\"305\",\"306\",\"92\",\"307\",\"308\",\"309\",\"310\",\"311\",\"312\",\"93\",\"313\",\"314\",\"315\",\"316\",\"317\",\"94\",\"95\",\"318\",\"319\",\"320\",\"321\",\"322\",\"14\",\"96\",\"97\",\"98\",\"99\",\"15\",\"100\",\"323\",\"324\",\"325\",\"101\",\"326\",\"327\",\"328\",\"102\",\"329\",\"330\",\"331\",\"332\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"16\",\"110\",\"111\",\"112\",\"17\",\"113\",\"114\",\"115\",\"116\",\"18\",\"19\",\"20\",\"117\",\"118\",\"119\",\"333\",\"334\",\"335\",\"336\",\"337\",\"338\",\"339\",\"340\",\"120\",\"341\",\"342\",\"343\",\"344\",\"345\",\"346\",\"347\",\"121\",\"122\",\"123\",\"124\",\"21\",\"125\",\"126\",\"127\",\"128\",\"129\",\"130\",\"131\",\"22\",\"132\",\"133\",\"134\",\"135\",\"136\",\"23\",\"137\",\"138\",\"348\",\"349\",\"350\",\"139\",\"140\",\"141\",\"142\",\"143\",\"144\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"145\",\"146\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"037\",\"38\",\"39\",\"147\",\"148\",\"149\",\"150\",\"351\",\"352\",\"353\",\"354\",\"151\",\"355\",\"356\",\"357\",\"358\",\"359\",\"152\",\"153\",\"360\",\"361\",\"362\",\"363\",\"154\",\"364\",\"365\",\"366\",\"367\",\"40\",\"155\",\"156\",\"157\",\"158\",\"159\",\"160\",\"161\",\"41\",\"162\",\"163\",\"164\",\"165\",\"166\",\"167\",\"168\",\"169\",\"170\",\"42\",\"43\",\"44\",\"45\",\"171\",\"172\",\"173\",\"174\",\"46\",\"175\",\"176\",\"177\",\"178\",\"179\",\"180\",\"181\",\"47\",\"48\",\"182\",\"183\",\"184\",\"185\",\"186\",\"187\",\"49\",\"188\",\"189\",\"190\",\"191\",\"192\",\"193\",\"50\",\"194\",\"195\",\"196\",\"197\",\"198\",\"199\",\"200\",\"201\",\"51\",\"52\",\"53\",\"54\",\"202\",\"203\",\"204\",\"205\",\"206\",\"207\",\"55\",\"208\",\"209\",\"210\",\"211\",\"212\",\"213\",\"214\",\"56\",\"215\",\"216\",\"217\",\"218\",\"219\",\"220\",\"221\",\"222\",\"57\",\"223\",\"224\",\"225\",\"226\",\"227\",\"58\",\"228\",\"229\",\"230\",\"231\",\"232\",\"59\",\"233\",\"234\",\"235\",\"236\",\"237\",\"238\",\"239\",\"60\",\"61\",\"62\",\"240\",\"241\",\"242\",\"243\",\"244\",\"245\",\"246\",\"247\",\"248\",\"63\",\"249\",\"250\",\"251\",\"252\",\"253\",\"254\",\"255\",\"256\",\"257\",\"64\",\"258\",\"259\",\"260\",\"261\",\"262\",\"65\",\"263\",\"264\",\"265\",\"266\",\"267\",\"268\",\"269\",\"270\",\"271\",\"66\",\"272\",\"273\",\"274\",\"67\",\"275\",\"368\",\"369\",\"276\",\"370\",\"371\",\"68\",\"277\",\"278\",\"279\",\"280\",\"281\",\"282\",\"283\",\"284\",\"285\",\"69\",\"286\",\"372\",\"373\",\"374\",\"375\",\"287\",\"376\",\"377\",\"378\",\"288\",\"379\",\"380\",\"289\",\"381\",\"382\",\"383\",\"290\",\"384\",\"385\",\"386\",\"291\",\"292\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\"],\"labels\":[\"Math Topics\",\"Numbers<br>and<br>Computation\",\"Logic<br>and<br>Foundations\",\"Algebra<br>and<br>Number<br>Theory\",\"Discrete<br>Mathematics\",\"Geometry<br>and<br>Topology\",\"Calculus\",\"Analysis\",\"Differential<br>and<br>Difference<br>Equations\",\"Statistics<br>and<br>Probability\",\"Applied<br>Mathematics\",\"Mathematics<br>History\",\"Number<br>Concepts\",\"Natural\",\"Integers\",\"Rational\",\"Irrational\",\"Algebraic\",\"Real\",\"Complex\",\"Famous<br>Numbers\",\"zero\",\"pi\",\"e\",\"i\",\"Golden<br>Mean\",\"Arithmetic\",\"Operations\",\"Addition\",\"Subtraction\",\"Multiplication\",\"Division\",\"Roots\",\"Factorials\",\"Factoring\",\"Properties<br>of<br>Operations\",\"Estimation\",\"Fractions\",\"Addition\",\"Subtraction\",\"Multiplication\",\"Division\",\"Ratio<br>and<br>Proportion\",\"Equivalent<br>Fractions\",\"Decimals\",\"Addition\",\"Subtraction\",\"Multiplication\",\"Division\",\"Percents\",\"Comparison<br>of<br>numbers\",\"Exponents\",\"Multiplication\",\"Division\",\"Powers\",\"Integer<br>Exponents\",\"Rational<br>Exponents\",\"Patterns<br>and<br>Sequences\",\"Number<br>Patterns\",\"Fibonacci<br>Sequence\",\"Arithmetic<br>Sequence\",\"Geometric<br>Sequence\",\"Measurement\",\"Units<br>of<br>Measurement\",\"Metric<br>System\",\"Standard<br>Units\",\"Nonstandard<br>Units\",\"Linear<br>Measure\",\"Distance\",\"Circumference\",\"Perimeter\",\"Area\",\"Area<br>of<br>Polygons\",\"Area<br>of<br>Circles\",\"Surface<br>Area\",\"Nonstandard<br>Shapes\",\"Volume\",\"Weight<br>and<br>Mass\",\"Temperature\",\"Time\",\"Speed\",\"Money\",\"Scale\",\"Logic\",\"Venn<br>Diagrams\",\"Propositional<br>and<br>Predicate<br>Logic\",\"Methods<br>of<br>Proof\",\"Set<br>Theory\",\"Sets<br>and<br>Set<br>Operations\",\"Relations<br>and<br>Functions\",\"Cardinality\",\"Axiom<br>of<br>Choice\",\"Computability<br>and<br>Decidability\",\"Model<br>Theory\",\"Algebra\",\"Graphing<br>Techniques\",\"Algebraic<br>Manipulation\",\"Functions\",\"Linear\",\"Quadratic\",\"Polynomial\",\"Rational\",\"Exponential\",\"Logarithmic\",\"Piece-wise\",\"Step\",\"Equations\",\"Linear\",\"Quadratic\",\"Polynomial\",\"Rational\",\"Exponential\",\"Logarithmic\",\"Systems\",\"Inequalities\",\"Matrices\",\"Sequences<br>and<br>Series\",\"Algebraic<br>Proof\",\"Linear<br>Algebra\",\"Systems<br>of<br>Linear<br>Equations\",\"Matrix<br>algebra\",\"Vectors<br>in<br>R\",\"Vector<br>Spaces\",\"Linear<br>Transformations\",\"Eigenvalues<br>and<br>Eigenvectors\",\"Inner<br>Product<br>Spaces\",\"Abstract<br>Algebra\",\"Groups\",\"Rings<br>and<br>Ideals\",\"Fields\",\"Galois<br>Theory\",\"Multilinear<br>Algebra\",\"Number<br>Theory\",\"Integers\",\"Primes\",\"Divisibility\",\"Factorization\",\"Distributions<br>of<br>Primes\",\"Congruences\",\"Diophantine<br>Equations\",\"Irrational<br>Numbers\",\"Famous<br>Problems\",\"Coding<br>Theory\",\"Cryptography\",\"Category<br>Theory\",\"K-Theory\",\"Homological<br>Algebra\",\"Modular<br>Arithmetic\",\"Cellular<br>Automata\",\"Combinatorics\",\"Combinations\",\"Permutations\",\"Game<br>Theory\",\"Algorithms\",\"Recursion\",\"Graph<br>Theory\",\"Linear<br>Programming\",\"Order<br>and<br>Lattices\",\"Theory<br>of<br>Computation\",\"<br>Chaos\",\"Geometric<br>Proof\",\"Plane<br>Geometry\",\"Plane<br>Measurement\",\"Lines<br>and<br>Planes\",\"Angles\",\"Triangles\",\"Properties\",\"Congruence\",\"Similarity\",\"Pythagorean<br>Theorem\",\"Polygons\",\"Properties\",\"Regular\",\"Irregular\",\"Congruence\",\"Similarity\",\"Circles\",\"Patterns\",\"Geometric<br>Patterns\",\"Tilings<br>and<br>Tessellations\",\"Symmetry\",\"Golden<br>Ratio\",\"Transformations\",\"Translation\",\"Rotation\",\"Reflection\",\"Scaling\",\"Solid<br>Geometry\",\"Dihedral<br>Angles\",\"Spheres\",\"Cones\",\"Cylinders\",\"Pyramids\",\"Prisms\",\"Polyhedra\",\"Analytic<br>Geometry\",\"Cartesian<br>Coordinates\",\"Lines\",\"Circles\",\"Planes\",\"Conics\",\"Polar<br>Coordinates\",\"Parametric<br>Curves\",\"Surfaces\",\"Distance<br>Formula\",\"Projective<br>Geometry\",\"Differential<br>Geometry\",\"Algebraic<br>Geometry\",\"Topology\",\"Point<br>Set<br>Topology\",\"General<br>Topology\",\"Differential<br>Topology\",\"Algebraic<br>Topology\",\"Trigonometry\",\"Angles\",\"Trigonometric<br>Functions\",\"Inverse<br>Trigonometric<br>Functions\",\"Trigonometric<br>Identities\",\"Trigonometric<br>Equations\",\"Roots<br>of<br>Unity\",\"Spherical<br>Trigonometry\",\"Fractal<br>Geometry\",\"Single<br>Variable\",\"(Single)<br>Functions\",\"(Single)<br>Limits\",\"(Single)<br>Continuity\",\"(Single)<br>Differentiation\",\"(Single)<br>Integration\",\"(Single)<br>Series\",\"Several<br>Variables\",\"Functions<br>of<br>Several<br>Variables\",\"(Several)<br>Limits\",\"(Several)<br>Continuity\",\"Partial<br>Derivatives\",\"Multiple<br>integrals\",\"Taylor<br>Series\",\"Advanced<br>Calculus\",\"Vector<br>Valued<br>Functions\",\"Line<br>Integrals\",\"Surface<br>Integrals\",\"Stokes<br>Theorem\",\"Curvilinear<br>Coordinates\",\"Linear<br>spaces\",\"Fourier<br>Series\",\"Orthogonal<br>Functions\",\"Tensor<br>Calculus\",\"Calculus<br>of<br>Variations\",\"Operational<br>Calculus\",\"Real<br>Analysis\",\"(Analysis)<br>Metric<br>Spaces\",\"(Analysis)<br>Convergence\",\"(Analysis)<br>Continuity\",\"(Analysis)<br>Differentiation\",\"(Analysis)<br>Integration\",\"Measure<br>Theory\",\"Complex<br>Analysis\",\"(Complex)<br>Convergence\",\"Infinite<br>Series\",\"Analytic<br>Functions\",\"Integration\",\"Contour<br>Integrals\",\"Conformal<br>Mappings\",\"Several<br>Complex<br>Variables\",\"Numerical<br>Analysis\",\"Computer<br>Arithmetic\",\"Solutions<br>of<br>Equations\",\"Solutions<br>of<br>Systems\",\"Interpolation\",\"Numerical<br>Differentiation\",\"Numerical<br>Integration\",\"Numerical<br>Solutions<br>of<br>ODEs\",\"Numerical<br>Solutions<br>of<br>PDEs\",\"Integral<br>Transforms\",\"Fourier<br>Transforms\",\"Laplace<br>Transforms\",\"Hankel<br>Transforms\",\"Wavelets\",\"Other<br>Transforms\",\"Signal<br>Analysis\",\"Sampling<br>Theory\",\"Filters\",\"Noise\",\"Data<br>Compression\",\"Image<br>Processing\",\"Functional<br>Analysis\",\"Hilbert<br>Spaces\",\"Banach<br>Spaces\",\"Topological<br>Spaces\",\"Locally<br>Convex<br>Spaces\",\"Bounded<br>Operators\",\"Spectral<br>Theorem\",\"Unbounded<br>Operators\",\"Harmonic<br>Analysis\",\"Global<br>Analysis\",\"Ordinary<br>Differential<br>Equations\",\"First<br>Order\",\"Second<br>Order\",\"Linear<br>Oscillations\",\"Nonlinear<br>Oscillations\",\"Systems<br>of<br>Differential<br>Equations\",\"Sturm<br>Liouville<br>Problems\",\"Special<br>Functions\",\"Power<br>Series<br>Methods\",\"Laplace<br>Transforms\",\"Partial<br>Differential<br>Equations\",\"First<br>Order\",\"Elliptic\",\"Parabolic\",\"Hyperbolic\",\"Integral<br>Transforms\",\"Integral<br>Equations\",\"Potential<br>Theory\",\"Nonlinear<br>Equations\",\"Symmetries<br>and<br>Integrability\",\"Difference<br>Equations\",\"First<br>Order\",\"Second<br>Order\",\"Linear<br>Systems\",\"Z<br>Transforms\",\"Orthogonal<br>Polynomials\",\"Dynamical<br>Systems\",\"D<br>Maps\",\"D<br>Maps\",\"Lyapunov<br>Exponents\",\"Bifurcations\",\"Fractals\",\"Differentiable<br>Dynamics\",\"Conservative<br>Dynamics\",\"Chaos\",\"Complex<br>Dynamical<br>Systems\",\"Data<br>Collection\",\"Experimental<br>Design\",\"Sampling<br>and<br>Surveys\",\"Data<br>and<br>Measurement<br>Issues\",\"Data<br>Summary<br>and<br>Presentation\",\"Summary<br>Statistics\",\"Measures<br>of<br>Central<br>Tendencies\",\"Measures<br>of<br>Spread\",\"Data<br>Representation\",\"Graphs<br>and<br>Plots\",\"Tables\",\"Statistical<br>Inference<br>and<br>Techniques\",\"Sampling<br>Distributions\",\"Regression<br>and<br>Correlation\",\"Confidence<br>Intervals\",\"Hypothesis<br>Tests\",\"Statistical<br>Quality<br>Control\",\"Non-parametric<br>Techniques\",\"Multivariate<br>Techniques\",\"Survival<br>Analysis\",\"Bayesian<br>Statistics\",\"Probability\",\"Elementary<br>Probability\",\"Sample<br>Space<br>and<br>Sets\",\"General<br>Rules\",\"Combinations<br>and<br>Permutations\",\"Random<br>Variables\",\"Univariate<br>Distributions\",\"Discrete<br>Distributions\",\"Continuous<br>Distributions\",\"Expected<br>Value\",\"Limit<br>Theorems\",\"Central<br>Limit<br>Theorem\",\"Law<br>of<br>Large<br>Numbers\",\"Multivariate<br>Distributions\",\"Joint\",\"Conditional\",\"Expectations\",\"Stochastic<br>Processes\",\"Brownian<br>Motion\",\"Markov<br>Chains\",\"Queuing<br>Theory\",\"Probability<br>Measures\",\"Simulation\",\"Mathematical<br>Physics\",\"Mathematical<br>Economics\",\"Mathematical<br>Biology\",\"Mathematics<br>for<br>Business\",\"Engineering<br>Mathematics\",\"Mathematical<br>Sociology\",\"Mathematics<br>for<br>Social<br>Sciences\",\"Mathematics<br>for<br>Computer<br>Science\",\"Mathematics<br>for<br>Humanities\",\"Consumer<br>Mathematics\",\"General\",\"Famous<br>Problems\",\"Biographies<br>of<br>Mathematicians\"],\"parents\":[\"\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"1\",\"12\",\"12\",\"12\",\"12\",\"12\",\"12\",\"12\",\"12\",\"90\",\"90\",\"90\",\"90\",\"90\",\"1\",\"13\",\"91\",\"91\",\"91\",\"91\",\"91\",\"91\",\"91\",\"91\",\"91\",\"13\",\"92\",\"92\",\"92\",\"92\",\"92\",\"92\",\"13\",\"93\",\"93\",\"93\",\"93\",\"93\",\"13\",\"13\",\"95\",\"95\",\"95\",\"95\",\"95\",\"1\",\"14\",\"14\",\"14\",\"14\",\"1\",\"15\",\"100\",\"100\",\"100\",\"15\",\"101\",\"101\",\"101\",\"15\",\"102\",\"102\",\"102\",\"102\",\"15\",\"15\",\"15\",\"15\",\"15\",\"15\",\"15\",\"2\",\"16\",\"16\",\"16\",\"2\",\"17\",\"17\",\"17\",\"17\",\"2\",\"2\",\"3\",\"20\",\"20\",\"20\",\"119\",\"119\",\"119\",\"119\",\"119\",\"119\",\"119\",\"119\",\"20\",\"120\",\"120\",\"120\",\"120\",\"120\",\"120\",\"120\",\"20\",\"20\",\"20\",\"20\",\"3\",\"21\",\"21\",\"21\",\"21\",\"21\",\"21\",\"21\",\"3\",\"22\",\"22\",\"22\",\"22\",\"22\",\"3\",\"23\",\"23\",\"138\",\"138\",\"138\",\"23\",\"23\",\"23\",\"23\",\"23\",\"23\",\"3\",\"3\",\"3\",\"3\",\"4\",\"4\",\"29\",\"29\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"5\",\"5\",\"39\",\"39\",\"39\",\"39\",\"150\",\"150\",\"150\",\"150\",\"39\",\"151\",\"151\",\"151\",\"151\",\"151\",\"39\",\"39\",\"153\",\"153\",\"153\",\"153\",\"39\",\"154\",\"154\",\"154\",\"154\",\"5\",\"40\",\"40\",\"40\",\"40\",\"40\",\"40\",\"40\",\"5\",\"41\",\"41\",\"41\",\"41\",\"41\",\"41\",\"41\",\"41\",\"41\",\"5\",\"5\",\"5\",\"5\",\"45\",\"45\",\"45\",\"45\",\"5\",\"46\",\"46\",\"46\",\"46\",\"46\",\"46\",\"46\",\"5\",\"6\",\"48\",\"48\",\"48\",\"48\",\"48\",\"48\",\"6\",\"49\",\"49\",\"49\",\"49\",\"49\",\"49\",\"6\",\"50\",\"50\",\"50\",\"50\",\"50\",\"50\",\"50\",\"50\",\"6\",\"6\",\"6\",\"7\",\"54\",\"54\",\"54\",\"54\",\"54\",\"54\",\"7\",\"55\",\"55\",\"55\",\"55\",\"55\",\"55\",\"55\",\"7\",\"56\",\"56\",\"56\",\"56\",\"56\",\"56\",\"56\",\"56\",\"7\",\"57\",\"57\",\"57\",\"57\",\"57\",\"7\",\"58\",\"58\",\"58\",\"58\",\"58\",\"7\",\"59\",\"59\",\"59\",\"59\",\"59\",\"59\",\"59\",\"7\",\"7\",\"8\",\"62\",\"62\",\"62\",\"62\",\"62\",\"62\",\"62\",\"62\",\"62\",\"8\",\"63\",\"63\",\"63\",\"63\",\"63\",\"63\",\"63\",\"63\",\"63\",\"8\",\"64\",\"64\",\"64\",\"64\",\"64\",\"8\",\"65\",\"65\",\"65\",\"65\",\"65\",\"65\",\"65\",\"65\",\"65\",\"9\",\"66\",\"66\",\"66\",\"9\",\"67\",\"275\",\"275\",\"67\",\"276\",\"276\",\"9\",\"68\",\"68\",\"68\",\"68\",\"68\",\"68\",\"68\",\"68\",\"68\",\"9\",\"69\",\"286\",\"286\",\"286\",\"286\",\"69\",\"287\",\"287\",\"287\",\"69\",\"288\",\"288\",\"69\",\"289\",\"289\",\"289\",\"69\",\"290\",\"290\",\"290\",\"69\",\"69\",\"10\",\"10\",\"10\",\"10\",\"10\",\"10\",\"10\",\"10\",\"10\",\"10\",\"11\",\"11\",\"11\"],\"type\":\"treemap\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"margin\":{\"t\":50,\"l\":25,\"r\":25,\"b\":25}},                        {\"responsive\": true}                    )                };                            </script>        </div>   \r\n        <!-- END PLOTLY OUTPUT -->\r\n        <hr />\r\n        <h3>View my Code</h3>\r\n        <script src=\"https://gist.github.com/ischmidl-nd/c8036ea9b98557ca80bbf5e652d14655.js\"></script>\r\n        <hr />\r\n        <h3>View the Original Taxonomy</h3>\r\n        <div  style=\"border-style:inset; height: 75vh; overflow-y: auto;\">\r\n          <h3 class=\"book-heading\">Subject Taxonomy</h3>\r\n                <h3>Core Subject Taxonomy for Mathematical Sciences Education</h3>\r\n                <ul>\r\n                  <li><a href=\"https://www.math.duke.edu/education/ConferenceGroup04/\" target=\"_blank\">Mathematical Sciences Conference Group on Digital Educational Resources</a></li>\r\n                  <li><a href=\"/internal-archive?url=/gateway/index.html\" target=\"_blank\" title=\"This page has been archived internally\">Math Gateway Partners Group</a></li>\r\n                </ul>\r\n                <p>This taxonomy is based on the Math NSDL Taxonomy Committee Report, April 2, 2002, with draft changes proposed for Section 9 by CAUSE, May 16, 2004. Further changes to Section 9 were approved by the <a href=\"/internal-archive?url=/gateway/index.html\" target=\"_blank\" title=\"This page has been archived internally\">Math Gateway Partners Group</a>, April 29, 2005</p>\r\n                <p>The first two levels of this taxonomy are used for classifying JOMA documents by subject matter.&#160; A fuller indication of the meanings of those levels may be obtained by scanning the deeper levels.&#160;</p>\r\n                <ol style=\"list-style-type:none\">\r\n                  <li>1.0 Numbers and Computation\r\n                    <ol style=\"list-style-type:none\">\r\n                      <li>1.1 Number Concepts\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>1.1.1 Natural</li>\r\n                          <li>1.1.2 Integers</li>\r\n                          <li>1.1.3 Rational</li>\r\n                          <li>1.1.4 Irrational</li>\r\n                          <li>1.1.5 Algebraic</li>\r\n                          <li>1.1.6 Real</li>\r\n                          <li>1.1.7 Complex</li>\r\n                          <li>1.1.8 Famous Numbers\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>1.1.8.1 0</li>\r\n                              <li>1.1.8.2 pi</li>\r\n                              <li>1.1.8.3 e</li>\r\n                              <li>1.1.8.4 i</li>\r\n                              <li>1.1.8.5 Golden Mean</li>\r\n                            </ol>\r\n                          </li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>1.2 Arithmetic\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>1.2.1 Operations\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>1.2.1.1 Addition</li>\r\n                              <li>1.2.1.2 Subtraction</li>\r\n                              <li>1.2.1.3 Multiplication</li>\r\n                              <li>1.2.1.4 Division</li>\r\n                              <li>1.2.1.5 Roots</li>\r\n                              <li>1.2.1.6 Factorials</li>\r\n                              <li>1.2.1.7 Factoring</li>\r\n                              <li>1.2.1.8 Properties of Operations</li>\r\n                              <li>1.2.1.9 Estimation</li>\r\n                            </ol>\r\n                          </li>\r\n                          <li>1.2.2 Fractions\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>1.2.2.1 Addition</li>\r\n                              <li>1.2.2.2 Subtraction</li>\r\n                              <li>1.2.2.3 Multiplication</li>\r\n                              <li>1.2.2.4 Division</li>\r\n                              <li>1.2.2.5 Ratio and Proportion</li>\r\n                              <li>1.2.2.6 Equivalent Fractions</li>\r\n                            </ol>\r\n                          </li>\r\n                          <li>1.2.3 Decimals\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>1.2.3.1 Addition</li>\r\n                              <li>1.2.3.2 Subtraction</li>\r\n                              <li>1.2.3.3 Multiplication</li>\r\n                              <li>1.2.3.4 Division</li>\r\n                              <li>1.2.3.5 Percents</li>\r\n                            </ol>\r\n                          </li>\r\n                          <li>1.2.4 Comparison of numbers</li>\r\n                          <li>1.2.5 Exponents\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>1.2.5.1 Multiplication</li>\r\n                              <li>1.2.5.2 Division</li>\r\n                              <li>1.2.5.3 Powers</li>\r\n                              <li>1.2.5.4 Integer Exponents</li>\r\n                              <li>1.2.5.5 Rational Exponents</li>\r\n                            </ol>\r\n                          </li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>1.3 Patterns and Sequences\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>1.3.1 Number Patterns</li>\r\n                          <li>1.3.2 Fibonacci Sequence</li>\r\n                          <li>1.3.3 Arithmetic Sequence</li>\r\n                          <li>1.3.4 Geometric Sequence</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>1.4 Measurement\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>1.4.1 Units of Measurement\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>1.4.1.1 Metric System</li>\r\n                              <li>1.4.1.2 Standard Units</li>\r\n                              <li>1.4.1.3 Nonstandard Units</li>\r\n                            </ol>\r\n                          </li>\r\n                          <li>1.4.2 Linear Measure\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>1.4.2.1 Distance</li>\r\n                              <li>1.4.2.2 Circumference</li>\r\n                              <li>1.4.2.3 Perimeter</li>\r\n                            </ol>\r\n                          </li>\r\n                          <li>1.4.3 Area\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>1.4.3.1 Area of Polygons</li>\r\n                              <li>1.4.3.2 Area of Circles</li>\r\n                              <li>1.4.3.3 Surface Area</li>\r\n                              <li>1.4.3.4 Nonstandard Shapes</li>\r\n                            </ol>\r\n                          </li>\r\n                          <li>1.4.4 Volume</li>\r\n                          <li>1.4.5 Weight and Mass</li>\r\n                          <li>1.4.6 Temperature</li>\r\n                          <li>1.4.7 Time</li>\r\n                          <li>1.4.8 Speed</li>\r\n                          <li>1.4.9 Money</li>\r\n                          <li>1.4.10 Scale</li>\r\n                        </ol>\r\n                      </li>\r\n                    </ol>\r\n                  </li>\r\n                  <li>2.0 Logic and Foundations\r\n                    <ol style=\"list-style-type:none\">\r\n                      <li>2.1 Logic\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>2.1.1 Venn Diagrams</li>\r\n                          <li>2.1.2 Propositional and Predicate Logic</li>\r\n                          <li>2.1.3 Methods of Proof</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>2.2 Set Theory\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>2.2.1 Sets and Set Operations</li>\r\n                          <li>2.2.2 Relations and Functions</li>\r\n                          <li>2.2.3 Cardinality</li>\r\n                          <li>2.2.4 Axiom of Choice</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>2.3 Computability and Decidability</li>\r\n                      <li>2.4 Model Theory</li>\r\n                    </ol>\r\n                  </li>\r\n                  <li>3.0 Algebra and Number Theory\r\n                    <ol style=\"list-style-type:none\">\r\n                      <li>3.1 Algebra\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>3.1.1 Graphing Techniques</li>\r\n                          <li>3.1.2 Algebraic Manipulation</li>\r\n                          <li>3.1.3 Functions\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>3.1.3.1 Linear</li>\r\n                              <li>3.1.3.2 Quadratic</li>\r\n                              <li>3.1.3.3 Polynomial</li>\r\n                              <li>3.1.3.4 Rational</li>\r\n                              <li>3.1.3.5 Exponential</li>\r\n                              <li>3.1.3.6 Logarithmic</li>\r\n                              <li>3.1.3.7 Piece-wise</li>\r\n                              <li>3.1.3.8 Step</li>\r\n                            </ol>\r\n                          </li>\r\n                          <li>3.1.4 Equations\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>3.1.4.1 Linear</li>\r\n                              <li>3.1.4.2 Quadratic</li>\r\n                              <li>3.1.4.3 Polynomial</li>\r\n                              <li>3.1.4.4 Rational</li>\r\n                              <li>3.1.4.5 Exponential</li>\r\n                              <li>3.1.4.6 Logarithmic</li>\r\n                              <li>3.1.4.7 Systems</li>\r\n                            </ol>\r\n                          </li>\r\n                          <li>3.1.5 Inequalities</li>\r\n                          <li>3.1.6 Matrices</li>\r\n                          <li>3.1.7 Sequences and Series</li>\r\n                          <li>3.1.8 Algebraic Proof</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>3.2 Linear Algebra\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>3.2.1 Systems of Linear Equations</li>\r\n                          <li>3.2.2 Matrix algebra</li>\r\n                          <li>3.2.3 Vectors in R3</li>\r\n                          <li>3.2.4 Vector Spaces</li>\r\n                          <li>3.2.5 Linear Transformations</li>\r\n                          <li>3.2.6 Eigenvalues and Eigenvectors</li>\r\n                          <li>3.2.7 Inner Product Spaces</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>3.3 Abstract Algebra\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>3.3.1 Groups</li>\r\n                          <li>3.3.2 Rings and Ideals</li>\r\n                          <li>3.3.3 Fields</li>\r\n                          <li>3.3.4 Galois Theory</li>\r\n                          <li>3.3.5 Multilinear Algebra</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>3.4 Number Theory\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>3.4.1 Integers</li>\r\n                          <li>3.4.2 Primes</li>\r\n                          <li>3.4.2.1 Divisibility</li>\r\n                          <li>3.4.2.2 Factorization</li>\r\n                          <li>3.4.2.3 Distributions of Primes</li>\r\n                          <li>3.4.3 Congruences</li>\r\n                          <li>3.4.4 Diophantine Equations</li>\r\n                          <li>3.4.5 Irrational Numbers</li>\r\n                          <li>3.4.6 Famous Problems</li>\r\n                          <li>3.4.7 Coding Theory</li>\r\n                          <li>3.4.8 Cryptography</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>3.5 Category Theory</li>\r\n                      <li>3.6 K-Theory</li>\r\n                      <li>3.7 Homological Algebra</li>\r\n                      <li>3.8 Modular Arithmetic</li>\r\n                    </ol>\r\n                  </li>\r\n                  <li>4.0 Discrete Mathematics\r\n                    <ol style=\"list-style-type:none\">\r\n                      <li>4.1 Cellular Automata</li>\r\n                      <li>4.2 Combinatorics\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>4.2.1 Combinations</li>\r\n                          <li>4.2.2 Permutations</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>4.3 Game Theory</li>\r\n                      <li>4.4 Algorithms</li>\r\n                      <li>4.5 Recursion</li>\r\n                      <li>4.6 Graph Theory</li>\r\n                      <li>4.7 Linear Programming</li>\r\n                      <li>4.8 Order and Lattices</li>\r\n                      <li>4.9 Theory of Computation</li>\r\n                      <li>4.10 Chaos</li>\r\n                    </ol>\r\n                  </li>\r\n                  <li>5.0 Geometry and Topology\r\n                    <ol style=\"list-style-type:none\">\r\n                      <li>5.1 Geometric Proof</li>\r\n                      <li>5.2 Plane Geometry\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>5.2.1 Measurement</li>\r\n                          <li>5.2.2 Lines and Planes</li>\r\n                          <li>5.2.3 Angles</li>\r\n                          <li>5.2.4 Triangles\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>5.2.4.1 Properties</li>\r\n                              <li>5.2.4.2 Congruence</li>\r\n                              <li>5.2.4.3 Similarity</li>\r\n                              <li>5.2.4.4 Pythagorean Theorem</li>\r\n                            </ol>\r\n                          </li>\r\n                          <li>5.2.5 Polygons\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>5.2.5.1 Properties</li>\r\n                              <li>5.2.5.2 Regular</li>\r\n                              <li>5.2.5.3 Irregular</li>\r\n                              <li>5.2.5.4 Congruence</li>\r\n                              <li>5.2.5.5 Similarity</li>\r\n                            </ol>\r\n                          </li>\r\n                          <li>5.2.6 Circles</li>\r\n                          <li>5.2.7 Patterns\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>5.2.7.1 Geometric Patterns</li>\r\n                              <li>5.2.7.2 Tilings and Tessellations</li>\r\n                              <li>5.2.7.3 Symmetry</li>\r\n                              <li>5.2.7.4 Golden Ratio</li>\r\n                            </ol>\r\n                          </li>\r\n                          <li>5.2.8 Transformations\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>5.2.8.1 Translation</li>\r\n                              <li>5.2.8.2 Rotation</li>\r\n                              <li>5.2.8.3 Reflection</li>\r\n                              <li>5.2.8.4 Scaling</li>\r\n                            </ol>\r\n                          </li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>5.3 Solid Geometry\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>5.3.1 Dihedral Angles</li>\r\n                          <li>5.3.2 Spheres</li>\r\n                          <li>5.3.3 Cones</li>\r\n                          <li>5.3.4 Cylinders</li>\r\n                          <li>5.3.5 Pyramids</li>\r\n                          <li>5.3.6 Prisms</li>\r\n                          <li>5.3.7 Polyhedra</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>5.4 Analytic Geometry\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>5.4.1 Cartesian Coordinates</li>\r\n                          <li>5.4.2 Lines</li>\r\n                          <li>5.4.3 Circles</li>\r\n                          <li>5.4.4 Planes</li>\r\n                          <li>5.4.5 Conics</li>\r\n                          <li>5.4.6 Polar Coordinates</li>\r\n                          <li>5.4.7 Parametric Curves</li>\r\n                          <li>5.4.8 Surfaces</li>\r\n                          <li>5.4.9 Distance Formula</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>5.5 Projective Geometry</li>\r\n                      <li>5.6 Differential Geometry</li>\r\n                      <li>5.7 Algebraic Geometry</li>\r\n                      <li>5.8 Topology\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>5.8.1 Point Set Topology</li>\r\n                          <li>5.8.2 General Topology</li>\r\n                          <li>5.8.3 Differential Topology</li>\r\n                          <li>5.8.4 Algebraic Topology</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>5.9 Trigonometry\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>5.9.1 Angles</li>\r\n                          <li>5.9.2 Trigonometric Functions</li>\r\n                          <li>5.9.3 Inverse Trigonometric Functions</li>\r\n                          <li>5.9.4 Trigonometric Identities</li>\r\n                          <li>5.9.5 Trigonometric Equations</li>\r\n                          <li>5.9.6 Roots of Unity</li>\r\n                          <li>5.9.7 Spherical Trigonometry</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>5.10 Fractal Geometry</li>\r\n                    </ol>\r\n                  </li>\r\n                  <li>6.0 Calculus\r\n                    <ol style=\"list-style-type:none\">\r\n                      <li>6.1 Single Variable\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>6.1.1 Functions</li>\r\n                          <li>6.1.2 Limits</li>\r\n                          <li>6.1.3 Continuity</li>\r\n                          <li>6.1.4 Differentiation</li>\r\n                          <li>6.1.5 Integration</li>\r\n                          <li>6.1.6 Series</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>6.2 Several Variables\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>6.2.1 Functions of Several Variables</li>\r\n                          <li>6.2.2 Limits</li>\r\n                          <li>6.2.3 Continuity</li>\r\n                          <li>6.2.4 Partial Derivatives</li>\r\n                          <li>6.2.5 Multiple integrals</li>\r\n                          <li>6.2.6 Taylor Series</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>6.3 Advanced Calculus\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>6.3.1 Vector Valued Functions</li>\r\n                          <li>6.3.2 Line Integrals</li>\r\n                          <li>6.3.3 Surface Integrals</li>\r\n                          <li>6.3.4 Stokes Theorem</li>\r\n                          <li>6.3.5 Curvilinear Coordinates</li>\r\n                          <li>6.3.6 Linear spaces</li>\r\n                          <li>6.3.7 Fourier Series</li>\r\n                          <li>6.3.8 Orthogonal Functions</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>6.4 Tensor Calculus</li>\r\n                      <li>6.5 Calculus of Variations</li>\r\n                      <li>6.6 Operational Calculus</li>\r\n                    </ol>\r\n                  </li>\r\n                  <li>7.0 Analysis\r\n                    <ol style=\"list-style-type:none\">\r\n                      <li>7.1 Real Analysis\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>7.1.1 Metric Spaces</li>\r\n                          <li>7.1.2 Convergence</li>\r\n                          <li>7.1.3 Continuity</li>\r\n                          <li>7.1.4 Differentiation</li>\r\n                          <li>7.1.5 Integration</li>\r\n                          <li>7.1.6 Measure Theory</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>7.2 Complex Analysis\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>7.2.1 Convergence</li>\r\n                          <li>7.2.2 Infinite Series</li>\r\n                          <li>7.2.3 Analytic Functions</li>\r\n                          <li>7.2.4 Integration</li>\r\n                          <li>7.2.5 Contour Integrals</li>\r\n                          <li>7.2.6 Conformal Mappings</li>\r\n                          <li>7.2.7 Several Complex Variables</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>7.3 Numerical Analysis\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>7.3.1 Computer Arithmetic</li>\r\n                          <li>7.3.2 Solutions of Equations</li>\r\n                          <li>7.3.3 Solutions of Systems</li>\r\n                          <li>7.3.4 Interpolation</li>\r\n                          <li>7.3.5 Numerical Differentiation</li>\r\n                          <li>7.3.6 Numerical Integration</li>\r\n                          <li>7.3.7 Numerical Solutions of ODEs</li>\r\n                          <li>7.3.8 Numerical Solutions of PDEs</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>7.4 Integral Transforms\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>7.4.1 Fourier Transforms</li>\r\n                          <li>7.4.2 Laplace Transforms</li>\r\n                          <li>7.4.3 Hankel Transforms</li>\r\n                          <li>7.4.4 Wavelets</li>\r\n                          <li>7.4.5 Other Transforms</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>7.5 Signal Analysis\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>7.5.1 Sampling Theory</li>\r\n                          <li>7.5.2 Filters</li>\r\n                          <li>7.5.3 Noise</li>\r\n                          <li>7.5.4 Data Compression</li>\r\n                          <li>7.5.5 Image Processing</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>7.6 Functional Analysis\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>7.6.1 Hilbert Spaces</li>\r\n                          <li>7.6.2 Banach Spaces</li>\r\n                          <li>7.6.3 Topological Spaces</li>\r\n                          <li>7.6.4 Locally Convex Spaces</li>\r\n                          <li>7.6.5 Bounded Operators</li>\r\n                          <li>7.6.6 Spectral Theorem</li>\r\n                          <li>7.6.7 Unbounded Operators</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>7.7 Harmonic Analysis</li>\r\n                      <li>7.8 Global Analysis</li>\r\n                    </ol>\r\n                  </li>\r\n                  <li>8.0 Differential and Difference Equations\r\n                    <ol style=\"list-style-type:none\">\r\n                      <li>8.1 Ordinary Differential Equations\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>8.1.1 First Order</li>\r\n                          <li>8.1.2 Second Order</li>\r\n                          <li>8.1.3 Linear Oscillations</li>\r\n                          <li>8.1.4 Nonlinear Oscillations</li>\r\n                          <li>8.1.5 Systems of Differential Equations</li>\r\n                          <li>8.1.6 Sturm Liouville Problems</li>\r\n                          <li>8.1.7 Special Functions</li>\r\n                          <li>8.1.8 Power Series Methods</li>\r\n                          <li>8.1.9 Laplace Transforms</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>8.2 Partial Differential Equations\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>8.2.1 First Order</li>\r\n                          <li>8.2.2 Elliptic</li>\r\n                          <li>8.2.3 Parabolic</li>\r\n                          <li>8.2.4 Hyperbolic</li>\r\n                          <li>8.2.5 Integral Transforms</li>\r\n                          <li>8.2.6 Integral Equations</li>\r\n                          <li>8.2.7 Potential Theory</li>\r\n                          <li>8.2.8 Nonlinear Equations</li>\r\n                          <li>8.2.9 Symmetries and Integrability</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>8.3 Difference Equations\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>8.3.1 First Order</li>\r\n                          <li>8.3.2 Second Order</li>\r\n                          <li>8.3.3 Linear Systems</li>\r\n                          <li>8.3.4 Z Transforms</li>\r\n                          <li>8.3.5 Orthogonal Polynomials</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>8.4 Dynamical Systems\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>8.4.1 1D Maps</li>\r\n                          <li>8.4.2 2D Maps</li>\r\n                          <li>8.4.3 Lyapunov Exponents</li>\r\n                          <li>8.4.4 Bifurcations</li>\r\n                          <li>8.4.5 Fractals</li>\r\n                          <li>8.4.6 Differentiable Dynamics</li>\r\n                          <li>8.4.7 Conservative Dynamics</li>\r\n                          <li>8.4.8 Chaos</li>\r\n                          <li>8.4.9 Complex Dynamical Systems</li>\r\n                        </ol>\r\n                      </li>\r\n                    </ol>\r\n                  </li>\r\n                  <li>9.0 Statistics and Probability\r\n                    <ol style=\"list-style-type:none\">\r\n                      <li>9.1 Data Collection\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>9.1.1 Experimental Design</li>\r\n                          <li>9.1.2 Sampling and Surveys</li>\r\n                          <li>9.1.3 Data and Measurement Issues</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>9.2 Data Summary and Presentation\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>9.2.1 Summary Statistics\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>9.2.1.1 Measures of Central Tendencies</li>\r\n                              <li>9.2.1.2 Measures of Spread</li>\r\n                            </ol>\r\n                          </li>\r\n                          <li>9.2.2 Data Representation\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>9.2.2.1 Graphs and Plots</li>\r\n                              <li>9.2.2.2 Tables</li>\r\n                            </ol>\r\n                          </li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>9.3 Statistical Inference and Techniques\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>9.3.1 Sampling Distributions</li>\r\n                          <li>9.3.2 Regression and Correlation</li>\r\n                          <li>9.3.3 Confidence Intervals</li>\r\n                          <li>9.3.4 Hypothesis Tests</li>\r\n                          <li>9.3.5 Statistical Quality Control</li>\r\n                          <li>9.3.6 Non-parametric Techniques</li>\r\n                          <li>9.3.7 Multivariate Techniques</li>\r\n                          <li>9.3.8 Survival Analysis</li>\r\n                          <li>9.3.9 Bayesian Statistics</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>9.4 Probability\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>9.4.1 Elementary Probability\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>9.4.1.1 Sample Space and Sets</li>\r\n                              <li>9.4.1.2 General Rules</li>\r\n                              <li>9.4.1.3 Combinations and Permutations</li>\r\n                              <li>9.4.1.4 Random Variables</li>\r\n                            </ol>\r\n                          </li>\r\n                          <li>9.4.2 Univariate Distributions\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>9.4.2.1 Discrete Distributions</li>\r\n                              <li>9.4.2.2 Continuous Distributions</li>\r\n                              <li>9.4.2.3 Expected Value</li>\r\n                            </ol>\r\n                          </li>\r\n                          <li>9.4.3 Limit Theorems\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>9.4.3.1 Central Limit Theorem</li>\r\n                              <li>9.4.3.2 Law of Large Numbers</li>\r\n                            </ol>\r\n                          </li>\r\n                          <li>9.4.4 Multivariate Distributions\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>9.4.4.1 Joint</li>\r\n                              <li>9.4.4.2 Conditional</li>\r\n                              <li>9.4.4.3 Expectations</li>\r\n                            </ol>\r\n                          </li>\r\n                          <li>9.4.5 Stochastic Processes\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>9.4.5.1 Brownian Motion</li>\r\n                              <li>9.4.5.2 Markov Chains</li>\r\n                              <li>9.4.5.3 Queuing Theory</li>\r\n                            </ol>\r\n                          </li>\r\n                          <li>9.4.6 Probability Measures</li>\r\n                          <li>9.4.7 Simulation</li>\r\n                        </ol>\r\n                      </li>\r\n                    </ol>\r\n                  </li>\r\n                  <li>10.0 Applied Mathematics\r\n                    <ol style=\"list-style-type:none\">\r\n                      <li>10.1 Mathematical Physics</li>\r\n                      <li>10.2 Mathematical Economics</li>\r\n                      <li>10.3 Mathematical Biology</li>\r\n                      <li>10.4 Mathematics for Business</li>\r\n                      <li>10.5 Engineering Mathematics</li>\r\n                      <li>10.6 Mathematical Sociology</li>\r\n                      <li>10.7 Mathematics for Social Sciences</li>\r\n                      <li>10.8 Mathematics for Computer Science</li>\r\n                      <li>10.9 Mathematics for Humanities</li>\r\n                      <li>10.10 Consumer Mathematics</li>\r\n                    </ol>\r\n                  </li>\r\n                  <li>11.0 Mathematics History\r\n                    <ol style=\"list-style-type:none\">\r\n                      <li>11.1 General</li>\r\n                      <li>11.2 Famous Problems</li>\r\n                      <li>11.3 Biographies of Mathematicians</li>\r\n                    </ol>\r\n                  </li>\r\n                </ol>\r\n              </div>\r\n            <!--END CONTAINER FLUID-->\r\n            <script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.2.0-beta1/dist/js/bootstrap.bundle.min.js\" integrity=\"sha384-pprn3073KE6tl6bjs2QrFaJGz5/SUsLqktiwsUTF55Jfv3qYSDhgCecCxMW52nD2\" crossorigin=\"anonymous\"></script>\r\n\r\n<hr>\r\n<h1>Inspirations</h1>\r\n  <h2>Quanta's Map</h2>\r\n  <a href=\"https://mathmap.quantamagazine.org/map\">source</a><br>\r\n  <iframe height=600 width=1000 src=\"https://mathmap.quantamagazine.org/map\"></iframe>\r\n      \r\n  <h2>Domain of Science Map</h2>\r\n  <a data-flickr-embed=\"true\" href=\"https://www.flickr.com/photos/95869671@N08/32264483720\" title=\"Map of Mathematics Poster\"><img src=\"https://live.staticflickr.com/272/32264483720_0be41eb9d1_k.jpg\" width=\"1000\" alt=\"Map of Mathematics Poster\"></a>\r\n  <script async src=\"//embedr.flickr.com/assets/client-code.js\" charset=\"utf-8\"></script>\r\n\r\n  <h2>University of Michigan</h2>\r\n    <img width=\"1000\" alt=\"university of michigan math tracks\" src=\"/img/mathtax/university of michigan.png\">\r\n  <h2>Swarthmore</h2>\r\n    <img width=\"1000\" alt=\"swarthmore math tracks\" src=\"/img/mathtax/swarthmore.png\">\r\n  <h2>Binghamton</h2>\r\n    <img width=\"1000\" alt=\"binghamton math tracks\" src=\"/img/mathtax/binghamton.png\">\r\n  </main>\r\n\r\n      \r\n    </div>\r\n    <!-- <footer  class=\"bg-light pt-5 pb-2 px-5\"><hr><a href=\"/\">Home</a></footer> -->\r\n    \r\n    \r\n    <script src=\"https://code.jquery.com/jquery-3.4.1.min.js\"></script>\r\n    <script src=\"https://code.jquery.com/ui/1.12.1/jquery-ui.js\"></script>\r\n    <script src=\"https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js\" integrity=\"sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo\" crossorigin=\"anonymous\"></script>\r\n    <script src=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js\" integrity=\"sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6\" crossorigin=\"anonymous\"></script>\r\n\r\n  </body>\r\n</html>\r\n",
      "date_published": "2022-06-30T17:00:00-07:00"
    }
  ]
}
