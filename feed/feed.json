{
  "version": "https://jsonfeed.org/version/1",
  "title": "izak",
  "home_page_url": "https://ischmidls.github.io",
  "feed_url": "https://update-me.com/feed/feed.json",
  "description": "a tip of an iceberg",
  "author": {
    "name": "izak",
    "url": ""
  },
  "items": [{
      "id": "https://ischmidls.github.io/posts/ode/",
      "url": "https://ischmidls.github.io/posts/ode/",
      "title": "ordinary differential equations",
      "content_html": "<p>As with most of these posts, what I have included here closely follows and often directly copies or quotes a text written by someone more errudite than me. In this case thank you to <em>Prof. Peter Kuchment</em> for <a href=\"https://www.math.tamu.edu/~kuchment/ode.pdf\">A brief sketch of the main ODE theorems</a> written for Texas A&M University's Math 611, Fall 2017.</p>\r\n\r\n<h3>Main notions</h3>\r\n<p>\r\n    <b>Definition 1:</b>  An ODE of order \\( k \\) is an equation relating the values of one or more unknown functions of a single variable \\( t \\) (which we will call \"time\"), their derivatives up to the order \\( k \\), and the independent variable itself:\r\n  </p>\r\n  \r\n  \r\n\r\n<p style=\"overflow: auto;\">\\[\r\n\\Phi(t, x_1, x_2, \\ldots, x_n, x_i, x_i', \\ldots, x_i^{(k)}, x_{i+1}^{(1)}, \\ldots, x_{i+1}^{(n)}) = 0. \\quad (1)\r\n\\]</p>\r\n\r\n<p>\r\n  If more than one unknown function is involved, a system of such equations is usually needed. A system can be neatly written in vector form so that it looks like a single equation, for example:\r\n</p>\r\n\r\n<p style=\"overflow: auto;\">\\[\r\n\\Phi(t, \\mathbf{x}) = 0, \\quad (2)\r\n\\]</p>\r\n\r\n<p>\r\n  where boldface font is used to denote vectors.\r\n</p>\r\n\r\n<blockquote>\r\n<p>\r\n  Example 2:\r\n</p>\r\n<ol>\r\n  <li>\r\n    \\( x_i(t)x(t)^2 - 3t \\sin(x_{ii}(t)) = 8 \\) is an ODE (of what order? linear or non-linear?)\r\n  </li>\r\n  <li>\r\n    \\( x_i(t) = -5x(t + 7) \\) is NOT an ODE!\r\n  </li>\r\n  <li>\r\n    \\( x''(t) = -x(t) + \\int \\cos(x(\\tau)) \\, d\\tau \\) is NOT an ODE.\r\n  </li>\r\n</ol>\r\n</blockquote>\r\n\r\n<p>\r\n    Question:\r\n  </p>\r\n  <ul>\r\n    <li>Why aren't the latter two examples ODEs? If you read the definition, it looks at first glance like there is nothing wrong with these examples.</li>\r\n    <li>What was missing in the wording of the definition? How should it be changed to make sure we exclude such cases?</li>\r\n    <li>Do you know how the equations of the type shown in the last two examples are called?</li>\r\n  </ul>\r\n  \r\n  \r\n<p>I skipped section 2 because it's like three lines listing ODE vs PDE, Order, and Linear vs Non-linear.</p>\r\n<h3>Order reduction</h3>\r\n<p>TLDR; (<em>Prof. Peter Kuchment's</em>) idea: just use vectors to represent systems of equations, then its one row of vectors.</p>\r\n\r\n        <div class=\"text-secondary pl-5\">\r\n        <p>\r\n        Introducing new unknown functions, an ODE or a system \\(\\Phi(t, \\mathbf{x}) = 0, \\quad (2)\\) can be reduced to a first-order system:\r\n        </p>\r\n\r\n        <p style=\"overflow: auto;\">\\[\r\n        \\Phi(t, x_1, x_2, \\ldots, x_k, x_i) = 0 \\quad (3)\r\n        \\]</p>\r\n\r\n        <p>\r\n        where\r\n        </p>\r\n\r\n        <p style=\"overflow: auto;\">\\[\r\n        \\begin{align}\r\n        x_{1i} &= x_2 \\\\\r\n        x_{2i} &= x_3 \\\\\r\n        &\\ldots \\\\\r\n        x_{k-1i} &= x_k \\\\\r\n        \\end{align}\r\n        \\]</p>\r\n\r\n        <p>\r\n        So now we can always deal with the first-order systems:\r\n        </p>\r\n\r\n        <p style=\"overflow: auto;\">\\[\r\n        \\Phi(t, \\mathbf{x}, \\mathbf{x_i}) = 0 \\quad (4)\r\n        \\]</p>\r\n        </div>\r\n\r\n<p>\r\n  <b>Definition 3:</b> \r\n</p>\r\n<p>TLDR; with all due respect to <em>Prof. Peter Kuchment</em> his is just about whether the ODE does rely (non-autonomous) on stuff other than the initial condition \\(x=0\\), or if the ODE does no and only relies on the intial condition (autonomous).</p>\r\n    <div class=\"text-secondary pl-5\">\r\n    <ul>\r\n    <li>Normal:</li>\r\n    </ul>\r\n\r\n    <p style=\"overflow: auto;\">\\[\r\n    x_i = F(t, \\mathbf{x}) \\quad (5)\r\n    \\]</p>\r\n\r\n    <ul>\r\n    <li>Autonomous:</li>\r\n    </ul>\r\n\r\n    <p style=\"overflow: auto;\">\\[\r\n    x_i = F(\\mathbf{x}) \\quad (6)\r\n    \\]</p>\r\n\r\n    <p>\r\n    and non-autonomous (5) equations.\r\n    </p>\r\n    </div>\r\n\r\n<h4>Reduction of a non-autonomous equation to an autonomous one:</h4>\r\n<p>\r\n  \r\n  Introduce a new time \\( \\tau \\) and consider the autonomous system:\r\n</p>\r\n\r\n<p style=\"overflow: auto;\">\\[\r\n\\begin{align}\r\n  x'_0(\\tau) &= F(t(\\tau), x(\\tau)) \\\\\r\n  t'_0(\\tau) &= 1\r\n\\end{align}\r\n\\]</p>\r\n\r\n<p>\r\n  This autonomous system is equivalent to the non-autonomous (5).\r\n  In these notes, we will assume that \\( x(t) \\) is a differentiable function of \\( t \\in (a, b) \\subseteq \\mathbb{R}^n \\), and \\( F : \\mathbb{R}^{n+1} \\mapsto \\mathbb{R}^n \\).\r\n  (The complex case is possible, but we will not consider it here.)\r\n</p>\r\n<blockquote>\r\n    <p>\r\n        What evolutionary (i.e., depending on time \\( t \\)) process can be described by ODEs?\r\n        Let us have a process (mechanical, biological, etc.) whose instantaneous state can be described by some parameters \\( x \\).\r\n        We call the space of these parameters the phase space.\r\n        Since the system evolves with time, the parameters become functions of time as well: \\( x(t) \\).\r\n        When can such a process be described by an ODE?\r\n        Three conditions tell you when this is the case:\r\n      </p>\r\n      \r\n      \r\n\r\n<ul>\r\n  <li>\\(1\\) The system is finite-dimensional, i.e., it can be described by finitely many parameters \\( x_1, \\ldots, x_n \\).\r\n    This is not the case, for instance, in fluid dynamics, heat conduction, and quantum mechanics.</li>\r\n  <li>\\(2\\) Smoothness: the parameters change in a differentiable manner with time.\r\n    This is not the case with shock waves.</li>\r\n  <li>\\(3\\) The process is deterministic: the state of the system at a certain moment determines the whole future behavior of the system.\r\n    Indeed, due to the first condition, we can describe the evolution of the system by a finite-dimensional vector function \\( x(t) \\).\r\n    This function is differentiable, as the second condition tells us.\r\n    The third condition says that if we know \\( x(t) \\) for some moment \\( t \\), this determines all the future values of \\( x(\\tau) \\).\r\n    In particular, this determines the value of the derivative \\( x'_0 \\) at the moment \\( t \\).</li>\r\n</ul>\r\n</blockquote>\r\n\r\n<p>\r\n  Hence, \\( x'_0(t) \\) is determined by \\( t \\) and \\( x(t) \\). In mathematical notations, we write that \\( x'_0(t) \\) is a function of \\( t \\) and \\( x(t) \\): \\( x'_0(t) = F(t, x(t)) \\), which is an ODE (a system of ODEs).\r\n</p>\r\n\r\n<p>\r\n  IVP (Initial Value Problems):\r\n</p>\r\n\r\n<p style=\"overflow: auto;\">\\[\r\n\\begin{align}\r\n  \\frac{dx}{dt} &= F(t, x) \\\\\r\n  x(t_0) &= x_0 \\quad \\quad (8)\r\n\\end{align}\r\n\\]</p>\r\n<p>For those familiar with index notation (i.e., Kronecker delta, Levi-Civita, and generally \"linear algebra without matrices\"), skip the following four cards, and jump into <b><a href=\"#def4\">Definition 4</a></b></p>\r\n<div class=\"card\"><div class=\"card-body\">\r\n    <p>A note on index notation:</p>\r\n    <p>Thank you to <em>Prof. David Roylance</em> for his brief <a href=\"https://web.mit.edu/course/3/3.11/www/modules/index.pdf\">Matrix and Index Notation</a> from MIT September 18, 2000</p>\r\n    <p>A vector can be represented by its components along the Cartesian axes, denoted as \\(u_x, u_y, u_z\\) for the displacement vector \\(\\mathbf{u}\\). The components can also be indicated with numerical subscripts, such as \\(u_1, u_2, u_3\\), corresponding to the \\(x, y\\), and \\(z\\) directions. In a shorthand notation, the displacement vector can be written as \\(u_i\\), where the subscript \\(i\\) ranges over 1, 2, 3 (or 1 and 2 in two-dimensional problems). This is known as the range convention for index notation. With this convention, the vector equation \\(u_i = a\\) yields three scalar equations:</p>\r\n    </p><p style=\"overflow: auto;\">\\[\r\n    \\begin{aligned}\r\n    & u_{1}=a \\\\\r\n    & u_{2}=a \\\\\r\n    & u_{3}=a\r\n    \\end{aligned}\r\n    \\]\r\n    \r\n    </p><p>We will often find it convenient to denote a vector by listing its components in a vertical list enclosed in braces, and this form will help us keep track of matrix-vector multiplications a bit more easily. We therefore have the following equivalent forms of vector notation:\r\n    \r\n    </p><p style=\"overflow: auto;\">\\[\r\n    \\mathbf{u}=u_{i}=\\left\\{\\begin{array}{l}\r\n    u_{1} \\\\\r\n    u_{2} \\\\\r\n    u_{3}\r\n    \\end{array}\\right\\}=\\left\\{\\begin{array}{l}\r\n    u_{x} \\\\\r\n    u_{y} \\\\\r\n    u_{z}\r\n    \\end{array}\\right\\}\r\n    \\]\r\n    \r\n    </p><p>Second-rank quantities such as stress, strain, moment of inertia, and curvature can be denoted as \\(3 \\times 3\\) matrix arrays; for instance the stress can be written using numerical indices as\r\n    \r\n    </p><p style=\"overflow: auto;\">\\[\r\n    [\\sigma]=\\left[\\begin{array}{lll}\r\n    \\sigma_{11} & \\sigma_{12} & \\sigma_{13} \\\\\r\n    \\sigma_{21} & \\sigma_{22} & \\sigma_{23} \\\\\r\n    \\sigma_{31} & \\sigma_{32} & \\sigma_{33}\r\n    \\end{array}\\right]\r\n    \\]\r\n    \r\n    </p>\r\n    \r\n    <p>Here the first subscript index denotes the row and the second the column. The indices also have a physical meaning, for instance \\(\\sigma_{23}\\) indicates the stress on the 2 face (the plane whose normal is in the 2 , or \\(y\\), direction) and acting in the 3 , or \\(z\\), direction. To help distinguish them, we'll use brackets for second-rank tensors and braces for vectors.\r\n    \r\n    </p><p>Using the range convention for index notation, the stress can also be written as \\(\\sigma_{i j}\\), where both the \\(i\\) and the \\(j\\) range from 1 to 3 ; this gives the nine components listed explicitly above. (Since the stress matrix is symmetric, i.e. \\(\\sigma_{i j}=\\sigma_{j i}\\), only six of these nine components are independent.)\r\n    \r\n    </p><p>A subscript that is repeated in a given term is understood to imply summation over the range of the repeated subscript; this is the summation convention for index notation. For instance, to indicate the sum of the diagonal elements of the stress matrix we can write:\r\n    \r\n    </p>\r\n    \r\n    <p style=\"overflow: auto;\">\\[\r\n    \\sigma_{k k}=\\sum_{k=1}^{3} \\sigma_{k k}=\\sigma_{11}+\\sigma_{22}+\\sigma_{33}\r\n    \\]\r\n    \r\n    </p><p>The multiplication rule for matrices can be stated formally by taking \\(\\mathbf{A}=\\left(a_{i j}\\right)\\) to be an \\((M \\times N)\\) matrix and \\(\\mathbf{B}=\\left(b_{i j}\\right)\\) to be an \\((R \\times P)\\) matrix. The matrix product \\(\\mathbf{A B}\\) is defined only when \\(R=N\\), and is the \\((M \\times P)\\) matrix \\(\\mathbf{C}=\\left(c_{i j}\\right)\\) given by\r\n    \r\n    </p><p style=\"overflow: auto;\">\\[\r\n    c_{i j}=\\sum_{k=1}^{N} a_{i k} b_{k j}=a_{i 1} b_{1 j}+a_{i 2} b_{2 j}+\\cdots+a_{i N} b_{N k}\r\n    \\]\r\n    \r\n    </p><p>Using the summation convention, this can be written simply\r\n    \r\n    </p><p style=\"overflow: auto;\">\\[\r\n    c_{i j}=a_{i k} b_{k j}\r\n    \\]\r\n    \r\n    </p><p>where the summation is understood to be over the repeated index \\(k\\). In the case of a \\(3 \\times 3\\) matrix multiplying a \\(3 \\times 1\\) column vector we have\r\n    \r\n    </p><p style=\"overflow: auto;\">\\[\r\n    \\left[\\begin{array}{lll}\r\n    a_{11} & a_{12} & a_{13} \\\\\r\n    a_{21} & a_{22} & a_{23} \\\\\r\n    a_{31} & a_{32} & a_{33}\r\n    \\end{array}\\right]\\left\\{\\begin{array}{l}\r\n    b_{1} \\\\\r\n    b_{2} \\\\\r\n    b_{3}\r\n    \\end{array}\\right\\}=\\left\\{\\begin{array}{c}\r\n    a_{11} b_{1}+a_{12} b_{2}+a_{13} b_{3} \\\\\r\n    a_{21} b_{1}+a_{22} b_{2}+a_{23} b_{3} \\\\\r\n    a_{31} b_{1}+a_{32} b_{2}+a_{33} b_{3}\r\n    \\end{array}\\right\\}=a_{i j} b_{j}\r\n    \\]\r\n    \r\n    </p><p>The comma convention uses a subscript comma to imply differentiation with respect to the variable following, so \\(f_{, 2}=\\partial f / \\partial y\\) and \\(u_{i, j}=\\partial u_{i} / \\partial x_{j}\\). For instance, the expression \\(\\sigma_{i j, j}=0\\) uses all of the three previously defined index conventions: range on \\(\\mathrm{i}\\), sum on \\(\\mathrm{j}\\), and differentiate:\r\n    \r\n    </p><p style=\"overflow: auto;\">\\[\r\n    \\begin{aligned}\r\n    & \\frac{\\partial \\sigma_{x x}}{\\partial x}+\\frac{\\partial \\sigma_{x y}}{\\partial y}+\\frac{\\partial \\sigma_{x z}}{\\partial z}=0 \\\\\r\n    & \\frac{\\partial \\sigma_{y x}}{\\partial x}+\\frac{\\partial \\sigma_{y y}}{\\partial y}+\\frac{\\partial \\sigma_{y z}}{\\partial z}=0 \\\\\r\n    & \\frac{\\partial \\sigma_{z x}}{\\partial x}+\\frac{\\partial \\sigma_{z y}}{\\partial y}+\\frac{\\partial \\sigma_{z z}}{\\partial z}=0\r\n    \\end{aligned}\r\n    \\]\r\n    \r\n    </p><p>The Kronecker delta is a useful entity is defined as\r\n    \r\n    </p><p style=\"overflow: auto;\">\\[\r\n    \\delta_{i j}= \\begin{cases}0, & i \\neq j \\\\ 1, & i=j\\end{cases}\r\n    \\]\r\n    \r\n    </p><p>This is the index form of the unit matrix \\(\\mathbf{I}\\) :\r\n    \r\n    </p><p style=\"overflow: auto;\">\\[\r\n    \\delta_{i j}=\\mathbf{I}=\\left[\\begin{array}{ccc}\r\n    1 & 0 & 0 \\\\\r\n    0 & 1 & 0 \\\\\r\n    0 & 0 & 1\r\n    \\end{array}\\right]\r\n    \\]\r\n    \r\n    </p><p>So, for instance\r\n    \r\n    </p><p style=\"overflow: auto;\">\\[\r\n    \\sigma_{k k} \\delta_{i j}=\\left[\\begin{array}{ccc}\r\n    \\sigma_{k k} & 0 & 0 \\\\\r\n    0 & \\sigma_{k k} & 0 \\\\\r\n    0 & 0 & \\sigma_{k k}\r\n    \\end{array}\\right]\r\n    \\]\r\n    \r\n    </p><p>where \\(\\sigma_{k k}=\\sigma_{11}+\\sigma_{22}+\\sigma_{33}\\).\r\n    \r\n    </p>\r\n</div></div><br>\r\n\r\n<p>Now, before returning to <em>Prof. Peter Kuchment's</em> lovely review of the ODE, and for one last note beyond <em>Prof. David Roylance's</em> notes on index notation, (this time from the faceless physics department at <a href=\"https://bohr.physics.berkeley.edu/classes/209/f02/leviciv.pdf\">UC Berkeley, Fall 2002</a>): I want to address the Levi-Civita Symbol which is closely tied to index notation and the Kronecker delta.</em>\r\n\r\n    <div class=\"card\"><div class=\"card-body\"><br>\r\n    \r\n<p>The Levi-Civita symbol is useful for converting cross products and curls into the language of tensor analysis, and for many other purposes. The following is a summary of its most useful properties in three-dimensional Euclidean space.</p>\r\n<p>The Levi-Civita symbol is defined by</p>\r\n<p style=\"overflow: auto;\">\\[\\epsilon_{ijk}= \\begin{cases}1, & \\text{ if }(ijk) \\text{ is an even permutation of }(123) ; \\\\ -1, & \\text{ if }(ijk) \\text{ is an odd permutation of }(123) ; \\\\ 0, & \\text{ otherwise. }\\end{cases}\\]</p>\r\n<p>It has 27 components, of which only 6 are nonzero. It follows directly from this definition that \\(\\epsilon_{ijk}\\) changes sign if any two of its indices are exchanged,</p>\r\n<p style=\"overflow: auto;\">\\[\\epsilon_{ijk}=\\epsilon_{jki}=\\epsilon_{kij}=-\\epsilon_{jik}=-\\epsilon_{ikj}=-\\epsilon_{kji} .\\]</p>\r\n<p>The Levi-Civita symbol is convenient for expressing cross products and curls in tensor notation. For example, if \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are two vectors, then</p>\r\n<p style=\"overflow: auto;\">\\[(\\mathbf{A} \\times \\mathbf{B})_{i}=\\epsilon_{ijk} A_{j} B_{k}\\]</p>\r\n<p>and</p>\r\n<p style=\"overflow: auto;\">\\[(\\nabla \\times \\mathbf{B})_{i}=\\epsilon_{ijk} \\frac{\\partial B_{k}}{\\partial x_{j}}\\]</p>\r\n<p>Any combination of an even number of Levi-Civita symbols (or an even number of cross products and curls) can be reduced to dot products with the following system of identities. Similarly, any combination of an odd number of Levi-Civita symbols (or an odd number of cross products and curls) can be reduced to a single Levi-Civita symbol (or a cross product or a curl) plus dot products. The first is the most general:</p>\r\n<p style=\"overflow: auto;\">\\[\\epsilon_{ijk} \\epsilon_{\\ell mn}=\\left|\\begin{array}{ccc}\r\n\\delta_{i\\ell} & \\delta_{im} & \\delta_{in} \\\\\r\n\\delta_{j\\ell} & \\delta_{jm} & \\delta_{jn} \\\\\r\n\\delta_{k\\ell} & \\delta_{km} & \\delta_{kn}\r\n\\end{array}\\right|\\]</p><p>Notice that the indices \\((i j k)\\) label the rows, while ( \\(\\ell m n)\\) label the columns. If this is contracted on \\(i\\) and \\(l\\), we obtain</p>\r\n<p style=\"overflow: auto;\">\\[\\epsilon_{ijk} \\epsilon_{imn}=\\left|\\begin{array}{cc}\r\n\\delta_{jm} & \\delta_{jn} \\\\\r\n\\delta_{km} & \\delta_{kn}\r\n\\end{array}\\right|=\\delta_{jm} \\delta_{kn}-\\delta_{jn} \\delta_{km} .\\]</p>\r\n<p>This identity is the one used most often, for boiling down two cross products that have one index in common, such as \\(\\nabla \\times(\\mathbf{A} \\times \\mathbf{B})\\). By contracting \\(\\epsilon_{ijk} \\epsilon_{ijn}\\) in \\(j\\) and \\(m\\) we obtain</p>\r\n<p style=\"overflow: auto;\">\\[\\epsilon_{ijk} \\epsilon_{ijn}=2 \\delta_{kn} \\text {. }\\]</p>\r\n<p>Finally, contracting on \\(k\\) and \\(n\\) we obtain</p>\r\n<p style=\"overflow: auto;\">\\[\\epsilon_{ijk} \\epsilon_{ijk}=6\\]</p>\r\n<p>It should be clear how to generalize these identities to higher dimensions.</p>\r\n<p>If \\(A_{ij}=-A_{ji}\\) is an antisymmetric, \\(3 \\times 3\\) tensor, it has 3 independent components that we can associate with a 3 -vector \\(\\mathbf{A}\\), as follows:</p>\r\n<p style=\"overflow: auto;\">\\[A_{ij}=\\left(\\begin{array}{ccc}\r\n0 & A_{3} & -A_{2} \\\\\r\n-A_{3} & 0 & A_{1} \\\\\r\nA_{2} & -A_{1} & 0\r\n\\end{array}\\right)=\\epsilon_{ijk} A_{k} .\\]</p>\r\n<p>The inverse of this is</p><p style=\"overflow: auto;\">\\[A_{ij}=\\frac{1}{2} \\epsilon_{ijk} A_{k}\\]</p>\r\n<p>Using these identities, the multiplication of an antisymmetric matrix times a vector can be reexpressed in terms of a cross product. That is, if</p>\r\n<p style=\"overflow: auto;\">\\[X_{i}=A_{ij} Y_{j}\\]</p>\r\n<p>then</p>\r\n<p style=\"overflow: auto;\">\\[\\mathbf{X}=\\mathbf{Y} \\times \\mathbf{A}\\]</p>\r\n<p>Similarly, if \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are two vectors, then</p>\r\n<p style=\"overflow: auto;\">\\[A_{i} B_{j}-A_{j} B_{i}=\\epsilon_{ijk}(\\mathbf{A} \\times \\mathbf{B})_{k},\\]</p>\r\n<p>and</p>\r\n<p style=\"overflow: auto;\">\\[\\frac{\\partial B_{j}}{\\partial x_{i}}-\\frac{\\partial B_{i}}{\\partial x_{j}}=\\epsilon_{ijk}(\\nabla \\times \\mathbf{B})_{k}\\]</p>\r\n<p>Finally, if \\(M_{ij}\\) is a \\(3 \\times 3\\) matrix (or tensor), then</p>\r\n<p style=\"overflow: auto;\">\\[\\operatorname{det} M=\\epsilon_{ijk} M_{1i} M_{2j} M_{3k}=\\frac{1}{6} \\epsilon_{ijk} \\epsilon_{\\ell mn} M_{i\\ell} M_{jm} M_{kn} .\\]</p><p>The Levi-Civita symbol has been defined here only on <span class=\"math inline\">\\(\\mathbb{R}^{3}\\)</span>, but most of the properties above are easily generalized to <span class=\"math inline\">\\(\\mathbb{R}^{n}\\)</span> (including the case <span class=\"math inline\">\\(n=2\\)</span> ). It only transforms as a tensor under proper orthogonal changes of coordinates, which is why we are calling it a \"symbol\" instead of a \"tensor.\" It can, however, be used to create so-called tensor densities on arbitrary manifolds with a metric, and has fascinating applications in Hodge-de Rham theory in differential geometry.</p>\r\n</div></div><br>\r\n\r\n<p>As a postscript to that last note on the Levi-Civita symbol (you should be glad I have not touched on the dozens of notations for differential geometry, & all that.) I just wanted to give the definition and properties of the \\(n\\)-dimensional case for Levi-Civita from <a href=\"https://en.wikipedia.org/wiki/Levi-Civita_symbol\">Wikipedia: Levi-Civita symbol</a></p>\r\n\r\n<div class=\"card\"><div class=\"card-body\">\r\n    <p>\\(n\\)-Levi-Civita Definition:</p>\r\n    <p>\r\n        \r\n        More generally, in \\(n\\) dimensions, the Levi-Civita symbol is defined by:\r\n       </p> \r\n       <p style=\"overflow: auto;\">\r\n        \\[\r\n        \\varepsilon_{a_1 a_2 a_3 \\ldots a_n} = \r\n        \\begin{cases}\r\n        +1 & \\text{if }(a_1, a_2, a_3, \\ldots, a_n) \\text{ is an even permutation of } (1, 2, 3, \\dots, n) \\\\\r\n        -1 & \\text{if }(a_1, a_2, a_3, \\ldots, a_n) \\text{ is an odd permutation of } (1, 2, 3, \\dots, n) \\\\\r\n        0 & \\text{otherwise}\r\n        \\end{cases}\r\n        \\]\r\n        </p>\r\n        <p>\r\n        Thus, it is the sign of the permutation in the case of a permutation, and zero otherwise.\r\n        </p>\r\n        <p>\r\n        Using the capital pi notation \\(\\Pi\\) for ordinary multiplication of numbers, an explicit expression for the symbol is:\r\n        </p>\r\n        <p style=\"overflow: auto;\">\r\n        \\[\r\n        \\begin{align*}\r\n        \\varepsilon_{a_1 a_2 a_3 \\ldots a_n} & = \\prod_{1 \\leq i < j \\leq n} \\text{sgn} (a_j - a_i) \\\\\r\n        & = \\text{sgn}(a_2 - a_1) \\text{sgn}(a_3 - a_1) \\dotsm \\text{sgn}(a_n - a_1) \\text{sgn}(a_3 - a_2) \\text{sgn}(a_4 - a_2) \\dotsm \\text{sgn}(a_n - a_2) \\dotsm \\text{sgn}(a_n - a_{n-1})\r\n        \\end{align*}\r\n        \\]\r\n        </p>\r\n        <p>\r\n        where the signum function (denoted \\(\\text{sgn}\\)) returns the sign of its argument while discarding the absolute value if nonzero. The formula is valid for all index values, and for any \\(n\\) (when \\(n = 0\\) or \\(n = 1\\), this is the empty product). However, computing the formula above naively has a time complexity of \\(O(n^2)\\), whereas the sign can be computed from the parity of the permutation from its disjoint cycles in only \\(O(n \\log(n))\\) cost.\r\n        </p>\r\n</div></div><br>\r\n\r\n<p>Now, the properties of \\(n\\)-Levi-Civita are harder to shoot off because the \\(n\\)-case is more like the top rung of the Levi-Civita latter. Where low dimensional intuition is merely helpful for the definition and general understanding of Levi-Civita, such low dimensional intuition is almost necessary for familiarity with the \\(n\\)-dimensional properties.</p>\r\n<div class='card'><div class='card-body'>\r\n    <p>TLDR; this first paragraph describes the tensor properties of Levi-Civita, but that might warrant a whole different post, so I paraphrased plus reduced the font size and color it to skip or read as you please.</p>\r\n   <div class=\"text-secondary pl-5\"> \r\n    <ul>\r\n        <li><sub>A permutation tensor has components given by the Levi-Civita symbol in an orthonormal basis. It is a tensor of covariant rank.</sub></li>\r\n        <li><sub>The Levi-Civita symbol remains unchanged under pure rotations and in coordinate systems related by orthogonal transformations. However, it is classified as a pseudotensor because it does not change under certain orthogonal transformations that would result in a sign change if it were a tensor.</sub></li>\r\n        <li><sub>Taking a cross product using the Levi-Civita symbol yields a pseudovector, not a vector.</sub></li>\r\n        <li><sub>Under a general coordinate change, the components of the permutation tensor are scaled by the Jacobian of the transformation matrix. This means that in different coordinate frames, the components can differ from those of the Levi-Civita symbol by an overall factor. In an orthonormal frame, the factor will be ±1 depending on the orientation.</sub></li>\r\n        <li><sub>In index-free tensor notation, the Hodge dual replaces the Levi-Civita symbol.</sub></li>\r\n        <li><sub>Einstein notation allows the elimination of summation symbols, where a repeated index implies summation over that index.</sub></li>\r\n      </ul></div>\r\n<p><span class=\"math display\">\\[\\varepsilon_{ijk} \\varepsilon^{imn} \\equiv \\sum_{i=1,2,3} \\varepsilon_{ijk} \\varepsilon^{imn}\\]</span>\r\nIn the following examples, Einstein notation is used.</p>\r\n<b id=\"two_dimensions\">Two dimensions</b>\r\n<p>In two dimensions, when all \\(i, \\ j, \\ m, \\ n\\)  each take the values 1 and 2:</p>\r\n<p style=\"overflow: auto;\">\\[\\varepsilon_{ij} \\varepsilon^{mn} = {\\delta_i}^m {\\delta_j}^n - {\\delta_i}^n {\\delta_j}^m\\]</p>\r\n<p style=\"overflow: auto;\">\\[\\varepsilon_{ij} \\varepsilon^{in} = {\\delta_j}^n\\]</p>\r\n<p style=\"overflow: auto;\">\\[\\varepsilon_{ij} \\varepsilon^{ij} = 2.\\]</p>\r\n<b id=\"three_dimensions\">Three dimensions</b>\r\n<p>In three dimensions, when all \\(i, \\ j, \\ m, \\ n\\) each take values 1, 2, and 3:</p>\r\n\r\n<p style=\"overflow: auto;\">\\[\\varepsilon_{ijk} \\varepsilon^{pqk}=\\delta_i{}^{p}\\delta_j{}^q - \\delta_i{}^q\\delta_j{}^p\\] </p>\r\n<p style=\"overflow: auto;\">\\[\\varepsilon_{jmn} \\varepsilon^{imn}=2{\\delta_j}^i\\] </p>\r\n<p style=\"overflow: auto;\">\\[\\varepsilon_{ijk} \\varepsilon^{ijk}=6.\\] </p>\r\n\r\n<b id=\"product\">Product</b>\r\n<p>The Levi-Civita symbol is related to the Kronecker delta. In three dimensions, the relationship is given by the following equations (vertical lines denote the determinant):</p>\r\n<p style=\"overflow: auto;\">\r\n    \\[\\begin{align}\r\n  \\varepsilon_{ijk}\\varepsilon_{lmn} &amp;= \\begin{vmatrix}\r\n    \\delta_{il} &amp; \\delta_{im} &amp; \\delta_{in} \\\\\r\n    \\delta_{jl} &amp; \\delta_{jm} &amp; \\delta_{jn} \\\\\r\n    \\delta_{kl} &amp; \\delta_{km} &amp; \\delta_{kn} \\\\\r\n  \\end{vmatrix} \\\\[6pt]\r\n                                     &amp;= \\delta_{il}\\left( \\delta_{jm}\\delta_{kn} - \\delta_{jn}\\delta_{km}\\right) - \\delta_{im}\\left( \\delta_{jl}\\delta_{kn} - \\delta_{jn}\\delta_{kl} \\right) + \\delta_{in} \\left( \\delta_{jl}\\delta_{km} - \\delta_{jm}\\delta_{kl} \\right). \r\n\\end{align}\\]</p>\r\n<p>A special case of this result is</p>\r\n<p><span class=\"math display\">\\[\\sum_{i=1}^3 \\varepsilon_{ijk}\\varepsilon_{imn} = \\delta_{jm}\\delta_{kn} - \\delta_{jn}\\delta_{km}\\]</span></p>\r\n<p>sometimes called the \"contracted epsilon identity\" </p>\r\n<p>In Einstein notation, the duplication of the \\(i\\) index implies the sum on \\(i\\). The previous is then denoted \\( \\varepsilon_{ijk}\\varepsilon_{ijn} = \\delta_{jm}\\delta_{kn} - \\delta_{jn}\\delta_{km} \\).</p>\r\n<p><span class=\"math display\">\\[\\sum_{i=1}^3 \\sum_{j=1}^3 \\varepsilon_{ijk}\\varepsilon_{ijn} = 2\\delta_{kn}\\]</span></p>\r\n</div></div><br>\r\n\r\n\r\n<p>Hopefully, that looked quite similar to the UC Berkeley physics exposition because here are the properties for the \\(n\\)-Levi-Civita case:</p>\r\n<div class='card'><div class='card-body'>\r\n    <p>\\(n\\)-Levi-Civita Properties:</p>\r\n    <p>In \\(n\\) dimensions, when all \\(i_1, \\ldots, i_n, j_1, \\ldots, j_n\\) take values from \\(1, 2, \\ldots, n\\):</p>\r\n<ul style=\"overflow: auto;\">\r\n<li>\\((A) \\quad \\varepsilon_{i_1 \\dots i_n} \\varepsilon^{j_1 \\dots j_n} = \\delta^{j_1 \\dots j_n}_{i_1 \\dots i_n} \\)</li>\r\n<li>\\((B) \\quad \\varepsilon_{i_1 \\dots i_k~i_{k+1} \\dots i_n} \\varepsilon^{i_1 \\dots i_k~j_{k+1} \\dots j_n} = \\delta_{ i_1 \\ldots i_k~i_{k+1} \\ldots i_n}^{i_1 \\dots i_k~j_{k+1}\\ldots j_n} = k!~\\delta^{j_{k+1} \\dots j_n}_{i_{k+1} \\dots i_n}\\) </li>\r\n<li>\\((C) \\quad \\varepsilon_{i_1 \\dots i_n}\\varepsilon^{i_1 \\dots i_n} = n!\\) </li>\r\n</ul>\r\n<p>where the exclamation mark (\\(!\\)) denotes the factorial, and \\(\\delta^{\\text{α} \\ldots}_{\\text{β} \\ldots}\\) is the generalized Kronecker delta. For any \\(n\\), the property:</p>\r\n<p style=\"overflow: auto;\">\\[\\sum_{i, j, k, \\ldots = 1}^n \\varepsilon_{ijk\\ldots}\\varepsilon_{ijk\\ldots} = n!\\]</p>\r\n<p>follows from the facts that:</p>\r\n<ol>\r\n<li>every permutation is either even or odd,</li>\r\n<li>\\(1 = (+1)^2 = (-1)^2 = 1\\), and</li>\r\n<li>the number of permutations of any \\(n\\)-element set is exactly \\(n!\\).</li>\r\n</ol>\r\n<p>The particular case above of </p><p style=\"overflow: auto;\">\\((B) \\quad \\varepsilon_{i_1 \\dots i_k~i_{k+1} \\dots i_n} \\varepsilon^{i_1 \\dots i_k~j_{k+1} \\dots j_n} = \\delta_{ i_1 \\ldots i_k~i_{k+1} \\ldots i_n}^{i_1 \\dots i_k~j_{k+1}\\ldots j_n} = k!~\\delta^{j_{k+1} \\dots j_n}_{i_{k+1} \\dots i_n}\\)</p>\r\n<p> with \\(k = n-2\\) is:</p>\r\n<p style=\"overflow: auto;\">\\[\\varepsilon_{i_1\\ldots i_{n-2}jk}\\varepsilon^{i_1\\ldots i_{n-2}lm} = (n-2)!(\\delta_j^l\\delta_k^m - \\delta_j^m\\delta_l^k)\\]</p>\r\n<b>Product</b>\r\n<p>In general, for \\(n\\) dimensions, the product of two Levi-Civita symbols can be written as:</p>\r\n<p style=\"overflow: auto;\">\\[\\varepsilon_{i_1 i_2 \\ldots i_n} \\varepsilon_{j_1 j_2 \\ldots j_n} = \\begin{vmatrix}\r\n\\delta_{i_1 j_1} & \\delta_{i_1 j_2} & \\ldots  & \\delta_{i_1 j_n} \\\\\r\n\\delta_{i_2 j_1} & \\delta_{i_2 j_2} & \\ldots  & \\delta_{i_2 j_n} \\\\\r\n\\vdots           & \\vdots           & \\ddots & \\vdots \\\\\r\n\\delta_{i_n j_1} & \\delta_{i_n j_2} & \\ldots  & \\delta_{i_n j_n} \\\\\r\n\\end{vmatrix}\\]</p>\r\n<p><strong>Proof:</strong> Both sides change signs upon switching two indices, so without loss of generality, assume \\(i_1 \\leq \\ldots \\leq i_n\\) and \\(j_1 \\leq \\ldots \\leq j_n\\). If some \\(i_c = i_{c+1}\\), then the left side is zero, and the right side is also zero since two of its columns are equal. The same applies if \\(j_c = j_{c+1}\\). Finally, if \\(i_1 < \\ldots < i_n\\) and \\(j_1 < \\ldots < j_n\\), then both sides equal 1.</p>\r\n\r\n</div></div><br>\r\n<p>Now, time to put that index notation to use!</p>\r\n<p>\r\n  <b id=\"def4\">Definition 4:</b>  For a differentiable function \\( F: \\mathbb{R}^m \\rightarrow \\mathbb{R}^n \\), the differential \\( DF(y) \\) of \\( F \\) at a point \\( y \\) is the linear mapping from \\( \\mathbb{R}^m \\) to \\( \\mathbb{R}^n \\) with the matrix:\r\n</p>\r\n\r\n<p style=\"overflow: auto;\">\\[\r\n\\{DF\\}_{ij}(y) = \\frac{\\partial F_i}{\\partial x_j}(y) \\quad (9)\r\n\\]</p>\r\n\r\n<p>\r\n  For a vector \\( x \\) of small norm, \\( DF(y)x \\) is the linear approximation of the change of the function \\( F(y + x) - F(y) \\), i.e., (Taylor formula of first order, or linearization formula):\r\n</p>\r\n\r\n<p style=\"overflow: auto;\">\\[\r\nF(y + x) = F(y) + DF(y)x + o(|x|)\r\n\\] </p>\r\n\r\n\r\n<p><b>Theorem 5:</b>  Existence and Uniqueness Theorem.</p>\r\n\r\n<p>\r\n  \r\n  Let \\( \\Omega \\subset \\mathbb{R}^n \\) be an open domain, \\( (a, b) \\) be an open segment of the line \\( \\mathbb{R} \\), and \\( F(t, x) \\) and \\( D_xF(t, x) \\) be continuous in \\( (a, b) \\times \\Omega \\).\r\n  Then, for any point \\( (t_0, x_0) \\) in \\( (a, b) \\times \\Omega \\), there exists a unique solution \\( x(t) \\) of the IVP <span style=\"overflow: auto;\">\\(  \\frac{dx}{dt} = F(t, x) \\Longrightarrow x(t_0) = x_0 \\quad (8)\\)</span> defined in a neighborhood of \\( t_0 \\).\r\n</p>\r\n<p>\r\n<strong>\r\n  Remark 6:\r\n</strong>\r\n</p>\r\n\r\n<ol>\r\n  <li>No global (i.e., on the whole \\( (a, b) \\)) uniqueness is guaranteed. Why?</li>\r\n  <li>The proof will show that another condition on \\( F \\) instead of differentiability suffices: it is enough that \\( F \\) is Lipschitz, i.e.,\r\n    \\( |F(t, x) - F(t, y)| \\leq K|x - y| \\) for some constant \\( K \\) and all \\( (t, x), (t, y) \\) in our domain.\r\n    Is this condition weaker than the one in the theorem?</li>\r\n</ol>\r\n\r\n\r\n<h3>Now about the proof: contraction mappings and such</h3>\r\n\r\n<h4>Some notions and notations:</h4>\r\n<p>\r\n  For a continuous function \\( x: [a, b] \\rightarrow \\mathbb{R}^n \\) on a finite segment \\([a, b]\\), we denote by\r\n</p>\r\n\r\n<p style=\"overflow: auto;\">\\( \\| x \\| = \\max_{t \\in [a, b]} \\| x(t) \\| \\quad (10) \\)</p>\r\n\r\n<p>\r\n  its norm in the space of such continuous functions, where \\( \\| x \\| \\) is the Euclidean norm of a vector in \\( \\mathbb{R}^n \\).\r\n</p>\r\n\r\n<p style=\"overflow: auto;\">\r\n  \\( C^r \\) - class of \\( r \\) times continuously differentiable functions (applicable to both scalar and vector-valued functions of different numbers of variables, should be understandable from the context).\r\n</p>\r\n\r\n<p><b>Definition 7:</b>  A real-valued function \\( A(x) \\) on \\( \\mathbb{R} \\) is a <b>contraction</b> if it satisfies the inequality</p>\r\n\r\n<p style=\"overflow: auto;\">\\[\r\n| A(x) - A(y) | \\leq K | x - y |\r\n\\]</p>\r\n\r\n<p>\r\n  for some \\( K < 1 \\) and all real \\( x \\) and \\( y \\).\r\n</p>\r\n\r\n<p>\r\n  <strong>Remark 8:</strong>\r\n</p>\r\n<ol>\r\n  <li>Condition of continuous differentiability of \\( A \\) and estimate \\( | A'(x) | \\leq k < 1 \\) guarantee that \\( A \\) is a <b>contraction</b>.</li>\r\n  <li>The definition of a <b>contraction</b> can be naturally extended to any metric space \\( M \\) with a metric (distance function) \\( \\rho \\) instead of the real line, replacing \\( | A(x) - A(y) | \\), \\( | x - y | \\) above with \\( \\rho(A(x), A(y)) \\), \\( \\rho(x, y) \\).</li>\r\n</ol>\r\n\r\n<h4>A simple instance of the contraction mapping principle:</h4>\r\n\r\n<p>\r\n  <strong>Theorem 9:</strong> Let \\( A(x) \\) be a contraction on \\( \\mathbb{R} \\). Then:\r\n</p>\r\n<ol>\r\n  <li>The equation \\( x = A(x) \\) has a unique solution \\( x^* \\) (called the fixed point of \\( A(x) \\)).</li>\r\n  <li>This fixed point can be found as \\( x^* = \\lim_{j \\to \\infty} x_j \\), where \\( x_0 \\) is arbitrary and \\( x_{i+1} = A(x_i) \\).</li>\r\n</ol>\r\n\r\n\r\n<h4>The general contraction mapping principle:</h4>\r\n\r\n<p>\r\n  Let \\( X \\) be a <b>metric space</b> (i.e., a set equipped with a metric or \"distance\" \\( \\rho(x, y) \\) with properties \\( \\rho \\geq 0 \\), \\( \\rho(x, y) = \\rho(y, x) \\), \\( \\rho(x, y) = 0 \\) only if \\( x = y \\), and the triangle inequality is satisfied \\( \\rho(x, y) \\leq \\rho(x, z) + \\rho(y, z) \\)).\r\n</p><p>Also let \\( X \\) be a <b>complete</b> metric space (i.e., if a sequence \\( x_n \\) is such that \\( \\rho(x_n, x_m) \\to 0 \\) as \\( n,m \\to \\infty \\), then there exists its limit \\( x \\) such that \\( \\rho(x_n, x) \\to 0 \\)).\r\n</p>\r\n\r\n<p>\r\n  A mapping \\( A: X \\to X \\) is a <b>contraction</b> if\r\n</p>\r\n\r\n<p style=\"overflow: auto;\">\\[\r\n\\rho(A(x), A(y)) \\leq K \\rho(x, y)\r\n\\]</p>\r\n\r\n<p>\r\n  for some \\( K < 1 \\) and all \\( x, y \\in X \\).\r\n</p>\r\n\r\n<b>Theorem 10:</b>\r\n\r\n<p>\r\n  Let \\( A(x) \\) be a contraction on a complete metric space \\( X \\). Then:\r\n</p>\r\n<ol>\r\n  <li>The equation \\( x = A(x) \\) has a unique solution \\( x^* \\) in \\( X \\) (called the fixed point of \\( A(x) \\)).</li>\r\n  <li>This fixed point can be found as \\( x^* = \\lim_{j \\to \\infty} x_j \\), where \\( x_0 \\) is arbitrary and \\( x_{i+1} = A(x_i) \\).</li>\r\n</ol>\r\n\r\n\r\n<h4>An equivalent integral equation reformulation of the IVP <span style=\"overflow: auto;\">\\(  \\frac{dx}{dt} = F(t, x) \\Longrightarrow x(t_0) = x_0 \\quad (8)\\)</span>:</h4>\r\n\r\n<p style=\"overflow: auto;\">\\( x(t) = x_0 + \\int_{t_0}^t F(\\tau, x(\\tau)) d\\tau  \\quad (11)\\)</p>\r\n\r\n<p>\r\n  <strong>Lemma 11:</strong> Continuous solutions of <span style=\"overflow: auto;\">\\( x(t) = x_0 + \\int_{t_0}^t F(\\tau, x(\\tau)) d\\tau  \\quad (11)\\)</span> are exactly the continuously differentiable solutions of <span style=\"overflow: auto;\">\\(  \\frac{dx}{dt} = F(t, x) \\Longrightarrow x(t_0) = x_0 \\quad (8)\\)</span>.\r\n</p>\r\n\r\n<p>\r\n  Now the proof of <b>Theorem 5</b> would be concluded if we prove the existence and uniqueness of continuous solutions of <span style=\"overflow: auto;\">\\( x(t) = x_0 + \\int_{t_0}^t F(\\tau, x(\\tau)) d\\tau  \\quad (11)\\)</span> on a small segment around \\( t_0 \\).\r\n</p>\r\n\r\n<p>\r\n  <strong>The metric space:</strong> Consider the interval \\([t_0 - d, t_0 + d] \\subset (a, b)\\) with a small \\( d \\) (it will be determined later on how small it should be).\r\n  We also consider a ball \\( B = \\{ x \\in \\mathbb{R}^n \\,|\\, \\| x - x_0 \\| \\leq b \\} \\) that is entirely contained in \\( \\Omega \\).\r\n  Now define on the set \\( X \\) of all continuous functions \\( x(t) \\) from \\([t_0 - d, t_0 + d]\\) to \\( B \\) the max norm \\(\\| x \\| = \\max_{t \\in [a, b]} \\| x(t) \\| \\quad (10)\\) as before and the corresponding metric \\( \\rho(x, y) = \\| x - y \\| \\).\r\n</p>\r\n\r\n<p>\r\n  Define the following integral operator \\( x \\to A(x) \\):\r\n  <p style=\"overflow: auto;\">\\[ [A(x)](t) = x_0 + \\int_{t_0}^t F(\\tau, x(\\tau)) d \\tau \\quad (12) \\]</p>\r\n</p>\r\n\r\n<p>\r\n  Note that this definition works for functions that map \\([t_0 - d, t_0 + d]\\) to \\( B \\).\r\n</p>\r\n\r\n<p>\r\n  <strong>Lemma 12:</strong> For a sufficiently small \\( d \\), the operator \\( A(x) \\) maps the above class of functions into itself and is a contraction, i.e.,\r\n  <span style=\"overflow: auto;\">\\( \\| A(x) - A(y) \\| \\leq \\| x - y \\| \\)</span> for some \\( k < 1 \\).\r\n</p>\r\n\r\n<p>\r\n  <strong>Corollary 13:</strong>\r\n</p>\r\n<ol>\r\n  <li>The integral equation \\( x(t) = x_0 + \\int_{t_0}^t F(\\tau, x(\\tau)) d\\tau  \\quad (11)\\) has a unique continuous solution in a neighborhood of \\( t_0 \\)</li>\r\n</ol>\r\n\r\n\r\n<p>2. This solution can be found as the limit in the norm <span style=\"overflow: auto;\">\\( \\| x \\| = \\max_{t \\in [a, b]} \\| x(t) \\| \\quad (10) \\)</span> of Picard iterations:</p>\r\n\r\n<p style=\"overflow: auto;\">\\[\r\ny_{i+1}(t) = x_0 + \\int_{t_0}^t F(\\tau, y_i(\\tau)) d\\tau \\quad (14)\r\n\\]</p>\r\n\r\n<p>\r\n  where \\( y_0 \\) can be chosen arbitrarily in such a way that \\( y_0(t_0) = x_0 \\), e.g., \\( y_0 \\equiv x_0 \\).\r\n</p>\r\n\r\n<p>3. The Uniqueness and Existence <b>Theorem 5</b>  is proven.</p>\r\n\r\n<h3>An existence theorem:</h3>\r\n\r\n<p>\r\n  <strong><b>Theorem 14:</b>  Peano's existence theorem.</strong> Continuity of \\( F \\) alone guarantees local existence of a solution of the IVP <span style=\"overflow: auto;\">\\(  \\frac{dx}{dt} = F(t, x) \\Longrightarrow x(t_0) = x_0 \\quad (8)\\)</span>.\r\n</p>\r\n\r\n<p>\r\n  <strong>Remark 15:</strong>\r\n</p>\r\n\r\n<blockquote>\r\n<ol>\r\n    <li>In the proof of Peano's theorem, the solution is also found as the limit of some sequence of functions, but rather than Picard's iterations, a sequence of Euler's piecewise linear functions (recall the Euler's method of numerical solution) is constructed.</li>\r\n    <li>Example of the IVP problem: \\(\\frac{dx}{dt} = 3x^{2/3}\\), \\(x(0) = 0\\) shows that the conditions of Peano's theorem cannot guarantee uniqueness.</li>\r\n  </ol>\r\n</blockquote>\r\n\r\n<h3>Geometry of ODEs: vector fields</h3>\r\n\r\n<p>\r\n  Consider the autonomous case:\r\n</p>\r\n\r\n<p style=\"overflow: auto;\">\\[\r\n\\frac{dx}{dt} = F(x), \\quad x(t) \\in \\mathbb{R}^n \\quad (15)\r\n\\]</p>\r\n\r\n<p>\r\n  We can think that \\( F(x) \\) assigns to each point \\( x \\) a vector \\( F(x) \\) (a vector \"grows\" out of any point).\r\n  Then we call \\( F(x) \\) a vector field. We will consider at least continuous (or smoother) functions \\( F(x) \\) and corresponding vector fields.\r\n  If \\( F \\) is of some class \\( C^r \\), we will also say that the field is of this class.\r\n</p>\r\n\r\n\r\n<p><b>Lemma 16:</b> </p>\r\n\r\n<p>\r\n  Trajectories of solutions of (15) are exactly the curves that are tangent at each point to the vector field corresponding to this equation. Such curves are called phase curves of the field.\r\n</p>\r\n\r\n<p>\r\n  Note that vector fields are NOT defined for non-autonomous systems.\r\n  The field \\( F(x) \\) is said to be non-singular at a point \\( x_0 \\), if \\( F(x_0) \\neq 0 \\).\r\n</p>\r\n\r\n<p>Exercise 17:</p>\r\n\r\n<p>\r\n  Show that the fields arising from turning a non-autonomous system into autonomous ones are always non-singular at all points.\r\n  Example of a non-singular vector field: a constant vector field, where \\( F(x) \\) is a constant non-zero vector.\r\n</p>\r\n\r\n<p>\r\n  <strong>Question 18:</strong> Is the existence and uniqueness theorem obvious for a constant vector field?\r\n</p>\r\n\r\n<blockquote>\r\n    <p>\r\n        A diffeomorphism of class \\( C^r \\) is a mapping \\( G \\) from a domain such that it is one-to-one and both \\( G \\) and its inverse \\( G^{-1} \\) are of mappings class \\( C^r \\).\r\n        In other words, a diffeomorphism smoothly deforms the domain.\r\n        At each point \\( x \\), the differential \\( (DG)(x) \\) is an invertible linear mapping of vectors in \\( \\mathbb{R}^n \\).\r\n        One can act by diffeomorphisms on vector fields as well.\r\n        One can come up with a right definition using the following heuristics:\r\n        Let \\( x(t) \\) be a solution of the equation defined by our vector field: \\( x_0 = F(x) \\).\r\n        We can act on this solution by our diffeomorphism to get a new function \\( x_G(t) = G(x(t)) \\).\r\n        Then the chain rule gives </p>\r\n        <p style=\"overflow: auto;\">\\[ x_0^G = (DG)(x) x_0 = (DG)(x) F(x) = (DG)(G^{-1}(x_G)) F(G^{-1}(x_G)) \\]</p>\r\n        In other words, the \\( G \\)-modified function \\( x_G \\) satisfies the ODE \\( y_0 = F_G(y) \\) with a vector field \\( F_G(y) = (DG)(G^{-1}(y)) F(G^{-1}(y)) \\).\r\n      </p>\r\n        \r\n</blockquote>\r\n\r\n<p><b>Definition 19:</b> </p>\r\n\r\n<p>\r\n  Let \\( F(x) \\) be a vector field and \\( G \\) be a diffeomorphism.\r\n  Then one defines a new vector field as follows: \\( F_G(x) = (DG)(G^{-1}(x)) F(G^{-1}(x)) \\).\r\n</p>\r\n\r\n<p><b>Theorem 20:</b>  Vector Field Rectification Theorem.</p>\r\n\r\n<p>\r\n  Any vector field of class \\( C^r \\) in a neighborhood of any of its non-singular point \\( x_0 \\) can be reduced to a constant field (\"rectified\") by applying a diffeomorphism of class \\( C^r \\).\r\n  One of the exercises is to show that the rectification theorem implies the existence and uniqueness one.\r\n</p>\r\n\r\n<p>\r\n  <strong>Question:</strong> Can one do the converse, i.e., get an idea of the local construction of the rectifying diffeomorphism from a known solution?\r\n</p>\r\n\r\n\r\n<h3>Dependence of solutions on parameters and initial data.</h3>\r\n\r\n<p>\r\n  The solution of the IVP <span style=\"overflow: auto;\">\\(  \\frac{dx}{dt} = F(t, x) \\Longrightarrow x(t_0) = x_0 \\quad (8)\\)</span> depends on the values of \\( t_0 \\) and \\( x_0 \\). How smooth is this dependence?\r\n  Another important question: Assume that the right hand side (the vector field) also depends on some parameter(s) \\( \\mu \\):\r\n</p>\r\n\r\n<p style=\"overflow: auto;\">\\[\r\n\\frac{dx}{dt} = F(t, x, \\mu), \\quad x(t_0) = x_0. \\quad (16)\r\n\\]</p>\r\n\r\n<p>\r\n  How smoothly does the solution depend on the parameter?\r\n  In fact, it can be seen that dependence on the initial data reduces to dependence on parameters.\r\n  Indeed, introducing a new time variable \\( \\tau = t - t_0 \\) and a new spatial variable \\( y = x - x_0 \\), one reduces <span style=\"overflow: auto;\">\\(  \\frac{dx}{dt} = F(t, x) \\Longrightarrow x(t_0) = x_0 \\quad (8)\\)</span> to:\r\n</p>\r\n\r\n<p style=\"overflow: auto;\">\\[\r\n\\frac{dy}{d\\tau} = F(\\tau + t_0, y + x_0), \\quad y(0) = 0. \\quad (17)\r\n\\]</p>\r\n\r\n<p>\r\n  Now all variable parameters are in the right hand side rather than in the initial data (which become constant). So, this is the only case to handle.\r\n</p>\r\n\r\n<p>\r\n  <strong>Theorem 21:</strong> Let the vector field \\( F(x, \\mu) \\) (where \\( \\mu \\) belongs to an open domain of a space \\( \\mathbb{R}^m \\)) be of class \\( C^r \\). Let also \\( F(x_0, \\mu_0) \\neq 0 \\). Then the (unique) solution \\( x(t, t_0, x, x_0, \\mu) \\) of the IVP:\r\n</p>\r\n\r\n<p style=\"overflow: auto;\">\\[\r\n\\frac{dx}{dt} = F(x, \\mu), \\quad x(t_0) = x_0. \\quad (18)\r\n\\]</p>\r\n\r\n<p>\r\n  depends differentiably of class \\( C^r \\) on \\( (t, t_0, x, x_0, \\mu) \\) for sufficiently small \\( |t - t_0| \\), \\( |x - x_0| \\), \\( |\\mu - \\mu_0| \\).\r\n</p>\r\n\r\n<h3>Extendability of local solutions.</h3>\r\n\r\n<blockquote>\r\n    <p>\r\n        Our theorems guaranteed the existence of a local solution only with no guarantee of how long it will survive.\r\n        Simple examples show the disappearance of solutions into a singular point.\r\n        Even without singular points, a solution curve can grow fast and disappear in a finite time.\r\n        An example is the IVP \\( \\frac{dx}{dt} = x^2 \\), \\( x(0) = 1 \\) that has the solution \\( x = \\frac{1}{1 - t} \\) that disappears at infinity when \\( t \\) approaches 1.\r\n        Are there any other options? Answer: no.\r\n      </p>\r\n      \r\n      \r\n</blockquote>\r\n\r\n\r\n<p><b>Theorem 22:</b>  Extendability Theorem.</p>\r\n<p>\r\n  Let \\( N \\) be a compact (bounded closed) subset in \\( \\Omega \\) (the domain where the smooth field is defined).\r\n  Let also \\( F \\) have no singular points in \\( N \\).\r\n  Then any local solution of <span style=\"overflow: auto;\">\\(  \\frac{dx}{dt} = F(t, x) \\Longrightarrow x(t_0) = x_0 \\quad (8)\\)</span> in \\( (a, b) \\times \\Omega \\) can be extended forward (for \\( t > t_0 \\)) and backward (for \\( t < t_0 \\)) either indefinitely or until it reaches the boundary of \\( N \\).\r\n</p>\r\n<h3>Boundary value problems (BVPs)</h3>\r\n\r\n<blockquote>\r\n    <ul>\r\n        <li>Here the conditions are imposed at both ends of a time interval, rather than at one end only in the IVP case.</li>\r\n        <li>Important applications.</li>\r\n        <li>The number of conditions should still be correct (depending on the order of the system and the number of unknown functions).</li>\r\n        <li>No such nice existence and uniqueness theorem.</li>\r\n      </ul>\r\n</blockquote>",
      "date_published": "2023-07-03T17:00:00-07:00"
    },{
      "id": "https://ischmidls.github.io/posts/abstract/",
      "url": "https://ischmidls.github.io/posts/abstract/",
      "title": "abstract algebra diagrams",
      "content_html": "<head>\r\n    <style>\r\n        #myImg {\r\n            border-radius: 5px;\r\n            cursor: pointer;\r\n            transition: 0.3s;\r\n            display: block;\r\n            margin-left: auto;\r\n            margin-right: auto\r\n        }\r\n        \r\n        #myImg:hover {opacity: 0.7;}\r\n        \r\n        /* The Modal (background) */\r\n        .modal {\r\n            display: none; /* Hidden by default */\r\n            position: fixed; /* Stay in place */\r\n            z-index: 1; /* Sit on top */\r\n            padding-top: 100px; /* Location of the box */\r\n            left: 0;\r\n            top: 0;\r\n            width: 100%; /* Full width */\r\n            height: 100%; /* Full height */\r\n            overflow: auto; /* Enable scroll if needed */\r\n            background-color: rgb(255,255,255); /* Fallback color */\r\n            background-color: rgba(255,255,255,0.9); /* Black w/ opacity */\r\n        }\r\n        \r\n        /* Modal Content (image) */\r\n        .modal-content {\r\n            margin: auto;\r\n            display: block;\r\n            width: 75%;\r\n        }\r\n        \r\n        /* Caption of Modal Image */\r\n        #caption {\r\n            margin: auto;\r\n            display: block;\r\n            width: 80%;\r\n            max-width: 700px;\r\n            text-align: center;\r\n            color: #ccc;\r\n            padding: 10px 0;\r\n            height: 150px;\r\n        }\r\n        \r\n        /* Add Animation */\r\n        .modal-content, #caption {\r\n            -webkit-animation-name: zoom;\r\n            -webkit-animation-duration: 0.6s;\r\n            animation-name: zoom;\r\n            animation-duration: 0.6s;\r\n        }\r\n        \r\n        .out {\r\n          animation-name: zoom-out;\r\n          animation-duration: 0.6s;\r\n        }\r\n        \r\n        @-webkit-keyframes zoom {\r\n            from {-webkit-transform:scale(1)}\r\n            to {-webkit-transform:scale(2)}\r\n        }\r\n        \r\n        @keyframes zoom {\r\n            from {transform:scale(0.4)}\r\n            to {transform:scale(1)}\r\n        }\r\n        \r\n        @keyframes zoom-out {\r\n            from {transform:scale(1)}\r\n            to {transform:scale(0)}\r\n        }\r\n        \r\n        /* The Close Button */\r\n        .close {\r\n            position: absolute;\r\n            top: 15px;\r\n            right: 35px;\r\n            color: #f1f1f1;\r\n            font-size: 40px;\r\n            font-weight: bold;\r\n            transition: 0.3s;\r\n        }\r\n        \r\n        .close:hover,\r\n        .close:focus {\r\n            color: #bbb;\r\n            text-decoration: none;\r\n            cursor: pointer;\r\n        }\r\n        \r\n        /* 100% Image Width on Smaller Screens */\r\n        @media only screen and (max-width: 700px){\r\n            .modal-content {\r\n                width: 100%;\r\n            }\r\n        }\r\n    </style>\r\n</head>\r\n<div class=\"container\" id=\"bsr-wrapper\">\r\n    <p>In July of 2022, I gathered all of these abstract algebra print outs that I found online.</p>\r\n    <p>Coming back in July of 2023 with more knowledge of web development & bored with these prinouts, I embeded a bunch of images from the <a href=\"https://en.wikipedia.org/wiki/Space_(mathematics)\">Wikipedia: Space (mathematics)</a> page.</p>\r\n    <p>I also generalized the \"zoom\" feature from <b><a href=\"https://codeconvey.com/html-image-zoom-on-click/\">this</a></b> guide to work for <code>img</code> tags of a given <code>class</code> rather than a single <code>img</code> of given <code>id</code>. Oh, yes the zoom background is white too to see the black diagrams. That's all. It's mostly for the math. That means you can click all the images to zoom. Yes....\"Enjoy!\"</p>\r\n    <hr>\r\n    <h2>theory printout pictures</h2>\r\n    <div class=\"card\">\r\n      <div class=\"card-body\">\r\n        <h3>group theory</h3>\r\n        <img class=\"zoom\"src=\"/img/theory/group.png\" alt=\"printout\">\r\n        <a href=\"//imgur.com/a/cja61w3\">source</a>\r\n      </div>\r\n    </div>\r\n    <br>\r\n    <div class=\"card\">\r\n      <div class=\"card-body\">\r\n        <h3>ring and field theory</h3>\r\n        <img class=\"zoom\"src=\"/img/theory/rf1.png\" alt=\"printout\">\r\n        <img class=\"zoom\"src=\"/img/theory/rf2.png\" alt=\"printout\">\r\n        <a href=\"//imgur.com/a/atu8zd9\">source</a>\r\n      </div>\r\n    </div>\r\n    <br>\r\n    <div class=\"card\">\r\n      <div class=\"card-body\">\r\n        <h3>complex analysis</h3>\r\n        <img class=\"zoom\"src=\"/img/theory/ca1.png\" alt=\"printout\">\r\n        <img class=\"zoom\"src=\"/img/theory/ca2.png\" alt=\"printout\">\r\n        <img class=\"zoom\"src=\"/img/theory/ca3.png\" alt=\"printout\">\r\n        <img class=\"zoom\"src=\"/img/theory/ca4.png\" alt=\"printout\">\r\n        <a href=\"//imgur.com/a/igjsbfj\">source</a>\r\n      </div>\r\n    </div>\r\n    <br>\r\n    <div class=\"card\">\r\n      <div class=\"card-body\">\r\n        <h3>galois theory</h3>\r\n        <img class=\"zoom\"src=\"/img/theory/galois1.png\" alt=\"printout\">\r\n        <img class=\"zoom\"src=\"/img/theory/galois2.png\" alt=\"printout\">\r\n        <img class=\"zoom\"src=\"/img/theory/galois3.png\" alt=\"printout\">\r\n        <a href=\"//imgur.com/a/ha96a7r\">source</a>\r\n      </div>\r\n    </div>\r\n    <br>\r\n    <hr>\r\n    <h1>bonus:</h1>\r\n    <h2>wikipedia spaces diagram</h2>\r\n    <div class=\"card\">\r\n      <div class=\"card-body\">\r\n      <p>\r\n        <a title=\"Qef, Public domain, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Mathematical_Spaces.svg\">\r\n          <img class=\"zoom\"width=\"256\" alt=\"Mathematical Spaces\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ed/Mathematical_Spaces.svg/256px-Mathematical_Spaces.svg.png\">\r\n        </a>\r\n      </p>\r\n    </div>\r\n  </div>\r\n  <br>\r\n  <div class=\"card\">\r\n    <div class=\"card-body\">\r\n      <p>\r\n        <b>linear & topology</b>\r\n        \r\n          <img class=\"zoom\"src=\"https://upload.wikimedia.org/wikipedia/commons/d/dc/Mathematical_implication_diagram-alt-large-print.svg\" alt=\"Mathematical implication diagram-alt-large-print.svg\" height=\"495\" width=\"1015\">\r\n\r\n      </p>\r\n    </div>\r\n  </div>\r\n  <br>\r\n  <p>\r\n    <sub>\r\n      <a href=\"http://creativecommons.org/publicdomain/zero/1.0/deed.en\" title=\"Creative Commons Zero, Public Domain Dedication\">CC0</a>\r\n    </sub>\r\n    <sub>\r\n      <a href=\"https://commons.wikimedia.org/w/index.php?curid=69882066\">Link</a>\r\n    </sub>\r\n  </p>\r\n  <div class=\"card\">\r\n    <div class=\"card-body\">\r\n      <p>\r\n        <b>affine & projective</b>\r\n        \r\n          <img class=\"zoom\"src=\"https://upload.wikimedia.org/wikipedia/commons/4/4c/Spaces_linear_etc.svg\" alt=\"Spaces linear etc.svg\" height=\"194\" width=\"535\">\r\n        \r\n      </p>\r\n    </div>\r\n  </div>\r\n  <br>\r\n  <p>\r\n    <sub>\r\n      <a href=\"https://creativecommons.org/licenses/by-sa/4.0\" title=\"Creative Commons Attribution-Share Alike 4.0\">CC BY-SA 4.0</a>\r\n    </sub>\r\n    <sub>\r\n      <a href=\"https://commons.wikimedia.org/w/index.php?curid=68601315\">Link</a>\r\n    </sub>\r\n  </p>\r\n  <div class=\"card\">\r\n    <div class=\"card-body\">\r\n      <p>\r\n        <b>metric</b>\r\n        \r\n          <img class=\"zoom\"src=\"https://upload.wikimedia.org/wikipedia/commons/9/98/Spaces_affine_etc.svg\" alt=\"Spaces affine etc.svg\" height=\"126\" width=\"295\">\r\n\r\n      </p>\r\n    </div>\r\n  </div>\r\n  <br>\r\n  <p>\r\n    <sub>\r\n      <a href=\"https://creativecommons.org/licenses/by-sa/4.0\" title=\"Creative Commons Attribution-Share Alike 4.0\">CC BY-SA 4.0</a>\r\n    </sub>\r\n    <sub>\r\n      <a href=\"https://commons.wikimedia.org/w/index.php?curid=68598716\">Link</a>\r\n    </sub>\r\n  </p>\r\n  <div class=\"card\">\r\n    <div class=\"card-body\">\r\n      <p>\r\n        <b>banach & hilbert</b>\r\n        \r\n          <img class=\"zoom\"src=\"https://upload.wikimedia.org/wikipedia/commons/7/7d/Spaces_metric_etc.svg\" alt=\"Spaces metric etc.svg\" height=\"142\" width=\"543\">\r\n\r\n      </p>\r\n    </div>\r\n  </div>\r\n  <br>\r\n  <p>\r\n    <sub>\r\n      <a href=\"https://creativecommons.org/licenses/by-sa/4.0\" title=\"Creative Commons Attribution-Share Alike 4.0\">CC BY-SA 4.0</a>\r\n    </sub>\r\n    <sub>\r\n      <a href=\"https://commons.wikimedia.org/w/index.php?curid=68598009\">Link</a>\r\n    </sub>\r\n  </p>\r\n  <div class=\"card\">\r\n    <div class=\"card-body\">\r\n      <p>\r\n        <b>riemann</b>\r\n        \r\n          <img class=\"zoom\"src=\"https://upload.wikimedia.org/wikipedia/commons/c/c4/Spaces_smooth_etc.svg\" alt=\"Spaces smooth etc.svg\" height=\"182\" width=\"386\">\r\n      </p>\r\n    </div>\r\n  </div>\r\n  <br>\r\n  <p>\r\n    <sub>\r\n      <a href=\"https://creativecommons.org/licenses/by-sa/4.0\" title=\"Creative Commons Attribution-Share Alike 4.0\">CC BY-SA 4.0</a>\r\n    </sub>\r\n    <sub>\r\n      <a href=\"https://commons.wikimedia.org/w/index.php?curid=68604725\">Link</a>\r\n    </sub>\r\n  </p>\r\n  <div class=\"card\">\r\n    <div class=\"card-body\">\r\n      <p>\r\n        <b>measures</b>\r\n        \r\n          <img class=\"zoom\"src=\"https://upload.wikimedia.org/wikipedia/commons/5/52/Spaces_measurable_etc.svg\" alt=\"Spaces measurable etc.svg\" height=\"132\" width=\"463\">\r\n\r\n      </p>\r\n    </div>\r\n  </div>\r\n  <br>\r\n  <p>\r\n    <sub>\r\n      <a href=\"https://creativecommons.org/licenses/by-sa/4.0\" title=\"Creative Commons Attribution-Share Alike 4.0\">CC BY-SA 4.0</a>\r\n    </sub>\r\n    <sub>\r\n      <a href=\"https://commons.wikimedia.org/w/index.php?curid=68606618\">Link</a>\r\n    </sub>\r\n  </p>\r\n  <div class=\"card\">\r\n    <div class=\"card-body\">\r\n      <p>\r\n        <b>schemes</b>\r\n        \r\n          <img class=\"zoom\"src=\"https://upload.wikimedia.org/wikipedia/commons/4/41/Spaces_schemes_etc.svg\" alt=\"Spaces schemes etc.svg\" height=\"163\" width=\"555\">\r\n\r\n      </p>\r\n    </div>\r\n  </div>\r\n  <br>\r\n  <p>\r\n    <sub>\r\n      <a href=\"https://creativecommons.org/licenses/by-sa/4.0\" title=\"Creative Commons Attribution-Share Alike 4.0\">CC BY-SA 4.0</a>\r\n    </sub>\r\n    <sub>\r\n      <a href=\"https://commons.wikimedia.org/w/index.php?curid=68652354\">Link</a>\r\n    </sub>\r\n  </p>\r\n  <div class=\"card\">\r\n    <div class=\"card-body\">\r\n      <p>\r\n        <b>topoi</b>\r\n        \r\n          <img class=\"zoom\"src=\"https://upload.wikimedia.org/wikipedia/commons/0/01/Spaces_topoi_etc.svg\" alt=\"Spaces topoi etc.svg\" height=\"101\" width=\"580\">\r\n\r\n      </p>\r\n    </div>\r\n  </div>\r\n  <br>\r\n  <p>\r\n    <sub>\r\n      <a href=\"https://creativecommons.org/licenses/by-sa/4.0\" title=\"Creative Commons Attribution-Share Alike 4.0\">CC BY-SA 4.0</a>\r\n    </sub>\r\n    <sub>\r\n      <a href=\"https://commons.wikimedia.org/w/index.php?curid=68637039\">Link</a>\r\n    </sub>\r\n  </p>\r\n  </div>\r\n  <!--CONTAINER BSR WRAPPER-->\r\n  <!-- The Modal -->\r\n<div id=\"myModal\" class=\"modal\">\r\n    <img class=\"zoom\"class=\"modal-content\" id=\"img01\">\r\n  </div>\r\n  \r\n  \r\n       \r\n  <script>\r\n  // THIS CODE WAS MODIFIED FROM:\r\n  // https://codeconvey.com/html-image-zoom-on-click/\r\n  // INSTEAD OF ZOOMING ON AN img OF THE GIVE id ATTRIBUTE, THIS PROGRAM\r\n  // ZOOMS ON THE GIVEN class ATTRIBUTE. ALLOWS USE FOR MULTIPLE img TAGS.\r\n  \r\n  // Get the modal\r\n  var modal = document.getElementById('myModal');\r\n  \r\n  // Get the image and insert it inside the modal - use its \"alt\" text as a caption\r\n  var images = document.querySelectorAll('.zoom'); // Replace 'your-image-class' with the actual class name of your images\r\n  \r\n  var modalImg = document.getElementById(\"img01\");\r\n  var captionText = document.getElementById(\"caption\");\r\n  \r\n  // Iterate over each image and attach click event listeners\r\n  images.forEach(function(img) {\r\n    img.onclick = function() {\r\n      modal.style.display = \"block\";\r\n      modalImg.src = this.src;\r\n      modalImg.alt = this.alt;\r\n      captionText.innerHTML = this.alt;\r\n    };\r\n  });\r\n  \r\n  // When the user clicks on <span> (x), close the modal\r\n  modal.onclick = function() {\r\n    img01.className += \" out\";\r\n    setTimeout(function() {\r\n      modal.style.display = \"none\";\r\n      img01.className = \"modal-content\";\r\n    }, 400);\r\n  };\r\n    \r\n      \r\n  </script>\r\n  ",
      "date_published": "2023-07-02T17:00:00-07:00"
    },{
      "id": "https://ischmidls.github.io/posts/intro%20prob/",
      "url": "https://ischmidls.github.io/posts/intro%20prob/",
      "title": "notes - probability &amp; types",
      "content_html": "<h2>table of contents</h2>\r\n<ul>\r\n  <li>1) <a href=\"#event-spaces\">event spaces</a></li>\r\n  <li>2) <a href=\"#outer-measure\">outer measure</a></li>\r\n  <li>3) <a href=\"#measurable-spaces\">measurable spaces</a></li>\r\n  <li>4) <a href=\"#defining-a-measure\">defining a measure</a></li>\r\n  <li>5) <a href=\"#measure-transformations\">measure transformations</a></li>\r\n  <li>6) <a href=\"#probability-theory\">probability theory</a>\r\n    <ul>\r\n      <li>6.1) <a href=\"#independence\">independence</a></li>\r\n    </ul>\r\n    \r\n  </li>\r\n</ul>\r\n<hr>\r\n<h2>intro</h2>\r\n<p>What follows &mdash; as with many of my posts &mdash; is my explanation of work from someone who is more erudite than me.</p>\r\n<p>This time it is \"Formalizing the Beginnings of Bayesian Probability Theory in the Lean Theorem Prover\" a Master's thesis from the lovely <em>Rishikesh Vaishnav</em>. You might find it <a href=\"https://escholarship.org/uc/item/8hb1w6js\">here</a>, ceteris paribus.</p>\r\n<p>I skipped his intro on types &mdash; the foundation of Lean's formal proofs, so read that if you like. As you may know, I enjoy <a href=\"https://www.cse.chalmers.se/~peterd/papers/DependentTypesAtWork.pdf\">Dependent Types at Work</a> by Ana Bove and Peter Dybjer (but this uses a different language &mdash; Agda). That being said, I will also disclude all of <em>Vaishnav's</em> code. So, this will read more like <a href=\"https://hott.github.io/book/hott-online-1404-g79e6d60.pdf\">HoTT</a> in the line of Marin-Lof's or Hofmann's or Voevodsky's or Awodey's publications.</p>\r\n<p><em>Vaishnav's</em> organization follows: </p>\r\n<p>\r\n  <ul>\r\n    <li>Measure-Theoretic Foundations</li>\r\n    <li>The Event Space</li>\r\n    <li>The Outer Measure\r\n        <ul>\r\n            <li>Outer Measure Transformations</li>\r\n        </ul>\r\n    </li>\r\n    <li>Measurable Spaces\r\n        <ul>\r\n            <li>Transforming Measurable Spaces</li>\r\n            <li>A Galois Connection</li>\r\n            <li>The Trimmed Outer Measure</li>\r\n            <li>Measurable Spaces on Pi Types</li>\r\n        </ul>\r\n    </li>\r\n    <li>Defining a Measure</li>\r\n    <li>Measure Transformations\r\n        <ul>\r\n            <li>From a Partial Function</li>\r\n            <li>The Carathéodory Criterion</li>\r\n            <li>Lifting an Outer Measure Transformation</li>\r\n            <li>Restricting a Measure</li>\r\n            <li>Mapping a Measure</li>\r\n        </ul>\r\n    </li>\r\n    <li>Probability Theory\r\n        <ul>\r\n            <li>Independence\r\n            </li>\r\n            <li>Conditional Probability\r\n            </li>\r\n            <li>Random Variables\r\n            </li>\r\n        </ul>\r\n    </li>\r\n</ul>\r\n</p>\r\n<p>Now, <em>Vaishnav's</em> details mostly fall under \"Probability Theory\". He also numbered everything, but that's a bit much for me (I use my own numbering here). Right now, I am taking a probability class, not a measure theory class, so I should probably focus on the \"Probability Theory\" section myself.</p>\r\n<p>The list above is somewhat decieving, as Vasishnav included sub-sub-headings for the \"Probability\" section splitting \"Independence\" and \"Conditional\" into subsections for definitions and properties. But more importantly, \"Random Variables\" includes:</p>\r\n<p>\r\n  <ul><li>Random Variables\r\n      <ul>\r\n          <li>The Joint Distribution</li>\r\n          <li>Restricting a Pi</li>\r\n          <li>The Marginal Distribution</li>\r\n          <li>Marginalization</li>\r\n          <li>Independence on Random Variables</li>\r\n          <li>Conditional Independence on Random Variables</li>\r\n      </ul>\r\n  </li>\r\n</ul></p>\r\n<p>That brings me to the fact of the matter. I took an \"Engineering Stats\" class and forgot everything. I still do not know what any of this means:</p>\r\n<blockquote>\r\n<p>Random variable: A variable that takes on different values based on the outcome of a random event. \\(X\\)</p><p>Distribution: The function that describes the probabilities of different outcomes or values of a random variable. \\(P(X = x)\\)</p><p>Mean: The average value of a set of numbers or a random variable. \\(\\mu\\)</p><p>Variance: A measure of the spread or dispersion of a set of numbers or a random variable. \\(\\sigma^2\\)</p><p>Standard deviation: The square root of the variance, providing a measure of the average distance between each data point and the mean. \\(\\sigma\\)</p><p>Normal distribution: A symmetric probability distribution that is commonly used to model real-world phenomena. \\(N(\\mu, \\sigma^2)\\)</p><p>Hypothesis testing: A statistical method to make inferences or conclusions about a population based on sample data.</p><p>Confidence interval: A range of values that is likely to contain the true value of a population parameter with a certain level of confidence. \\([a, b]\\)</p>\r\n</blockquote>\r\n<p>(quoting nobody, I just prefer the <code>blockquote</code> markup to using the <code>pl-5</code> bootstrap; you can add that to the izak lore if you want)</p>\r\n<!--EVENT SPACES-->\r\n<h2 id=\"event-spaces\">event spaces</h2>\r\n<p>Let a type \\( \\alpha \\) be the \"fundamental event space\", \r\n  where each trial yields a value. \r\n  To assign a <em>measure</em> to sets in \\( \\alpha \\), \r\n  representing probabilities: use a random variable, denoted \\( X: \\alpha \\rightarrow \\beta \\), \r\n  a transformation on the event space. \r\n  </p>\r\n  <p>With a measure function on \\( \\alpha \\), we can define a measure function on \r\n  \\( \\beta \\) corresponding to \\( X \\). For any set \\( s \\) in \\( \\beta \\), \r\n  the measure of \\( s \\) is the measure of the preimage of \\( s \\) under \\( X \\) \r\n  (denoted as \\( X^{-1}(s) \\)). This relationship between \\(\\alpha\\) and \\(\\beta\\) comes to fruition during the \"Mapping/Co-Mapping Measurable Spaces\" part of <em>Vaishnav's</em> \"Outer Measure\" section.\r\n</p>\r\n<p>Restrictions on a measure function \\(m\\) come from the \"<b>outer measure</b>\", \r\n  which states that, for a function \\(m : \\text{set } \\alpha \\rightarrow \\mathbb{R}^+_\\infty\\) \r\n  to be an <b>outer measure</b> \\((\\)<span class=\"text-secondary\">where \\(\\mathbb{R}^+_\\infty\\) represents the non-negative real numbers including infinity</span>\\()\\), \r\n  we have the empty set property \\((1)\\), monotonicity property \\((2)\\), and countable union property \\((3)\\):\r\n</p>\r\n<p>\r\n  \\[\r\n\\begin{align*}\r\nm(\\varnothing) &= 0, \\quad & (1) \\\\\r\n\\forall A, B : \\text{set } \\alpha, A \\subseteq B & \\Rightarrow m(A) \\leq m(B), \\quad & (2) \\\\\r\n\\forall f : \\mathbb{N} \\rightarrow \\text{set } \\alpha, m\\left(\\bigcup_{i=0}^\\infty f(i)\\right) & \\leq \\sum_{i=0}^\\infty m(f(i)), \\quad & (3)\r\n\\end{align*}\r\n\\]\r\n</p>\r\n<p>The empty set property \\((1)\\) ensures that the empty set has zero mass. The monotonicity property \\((2)\\) guarantees that a set has at least as much mass as any of its subsets. The countable union property \\((3)\\) limits the mass of a set to the maximum mass present in any covering set of subsets.</p>\r\n<p>These propertis of <b>outer measure</b> allow us to talk about measure using <em>inequalities</em>.</p>\r\n<p>See <em>Vaishnav's</em> paper for info on \"why countable\".</p>\r\n<h2 id=\"outer-measure\">outer measure</h2>\r\n<p><em>Vaishnav</em> defines:</p>\r\n<p>\\[\r\n  \\text{fromfunc}(m_0(s)) = \\inf_{ f : \\mathbb{N} \\rightarrow \\text{set }\\alpha} \\left\\{s \\subseteq \\bigcup_{i=0}^{\\infty} f(i) \\Rightarrow \\sum_{i=0}^{\\infty} m_0(f(i)) \\right\\}\r\n  \\]</p>\r\n<p>Satisfying: \\(∀ s : set \\ α, m(s) ≤ m_0(s).\\)</p>\r\n<p>In the type theory style that verges on pseudocode, we have \\(\\text{fromfunc}(m_0(s))\\) &mdash; which <em>Vaishnav</em> denotes \\(\\text{fromfunc}(m_0)(s)\\). </p>\r\n<p>The condition \\(s \\subseteq \\bigcup_{i=0}^{\\infty} f(i)\\) ensures that the sets \\(f(i)\\) cover the set \\(s\\). If this condition is not satisfied for some \\(f\\), the value is effectively considered \\(\\infty\\).</p>\r\n\r\n<p>The function \\(\\text{fromfunc}(m_0)\\) is then defined as a valid outer measure, which satisfies the condition \\(m(s) \\leq m_0(s)\\) for all sets \\(s\\). It is also proven that \\(\\text{fromfunc}(m_0)\\) is the unique maximum outer measure that satisfies this condition. No other outer measure can exceed it on any set.</p>\r\n<p><em>Vaishnav</em> also formalizes this for a more general case of partial functions where \\(m_0 : S → \\mathbb{R}^{+\\infty}\\)  such that:</p>\r\n<p>\\[ \\text{extend} (m_0) (s) = \\begin{cases} \r\n  m_0 (s) \\quad s \\in S \\\\\r\n\r\n  \\infty \\quad s \\notin S \r\n\\end{cases}\r\n\\]</p>\r\n<p>Now is the moment you've been waiting for since <a href=\"\"event-spaces>event spaces</a>.</p>\r\n<p>Given a function \\(f: \\alpha \\to \\beta\\) and an outer measure \\(m_0\\) on \\(\\alpha\\), we define \\(\\text{map}_f(m_0)\\) as:</p>\r\n<p>\r\n\\[\r\n\\text{map}_f(m_0)(s) = m_0(f^{-1}(s))\r\n\\]\r\n</p>\r\n<p>Similarly, for an outer measure \\(m_1\\) on \\(\\beta\\), we define \\(\\text{comap}_f(m_1)\\) as:</p>\r\n<p>\r\n\\[\r\n\\text{comap}_f(m_1)(s) = m_1(f(s))\r\n\\]\r\n</p>\r\n<p>Both \\(\\text{map}_f(m_0)\\) and \\(\\text{comap}_f(m_1)\\) are outer measures.</p>\r\n<p>Given an outer measure \\(m_0\\) on \\(\\alpha\\), we can restrict it to a set \\(t\\) by defining:</p>\r\n\r\n\\[\r\n\\text{restrict}_t(m_0)(s) = m_0(s \\cap t)\r\n\\]\r\n\r\n<p>This measures the intersection of \\(s\\) with \\(t\\). The function \\(\\text{restrict}_t(m_0)\\) is proven to be an outer measure. In Lean, we can use the coercion function \\(c_t: t \\to \\alpha\\) to avoid the explicit proof. We can express it as:</p>\r\n\r\n\\[\r\n\\text{restrict}_t(m_0) = \\text{map}_{c_t}(\\text{comap}_{c_t}(m_0))\r\n\\]\r\n\r\n<p>Alternatively, for any \\(s\\), it can be simplified to:</p>\r\n\r\n\\[\r\n\\text{restrict}_t(m_0)(s) = m_0(c_t(c_t^{-1}(s)))\r\n\\]\r\n\r\n<p>where \\(c_t(c_t^{-1}(s))\\) represents the intersection \\(s \\cap t\\). Since \\(\\text{map}(\\cdot)\\) and \\(\\text{comap}(\\cdot)\\) preserve outer measures, we can conclude that \\(\\text{restrict}_t(m_0)(s)\\) is an outer measure.</p>\r\n\r\n<p>Now, that was a very close paraphase of <em>Vaishnav's</em> text.</p>\r\n<p>Alternatively, the beloved Wikipedia says:</p>\r\n<div class=\"card\"><div class=\"card-body\">\r\n  <p>Given a set \\(X\\), let \\(2^X\\) denote the collection of all subsets of \\(X\\), including the empty set \\(\\varnothing\\).</p>\r\n<p>An outer measure on \\(X\\) is a set function \\(\\mu: 2^X \\to [0, \\infty]\\) such that:</p>\r\n<ol>\r\n  <li>Null empty set: \\(\\mu(\\varnothing) = 0\\)</li>\r\n  <li>Countably subadditive: For arbitrary subsets \\(A, B_1, B_2, \\ldots\\) of \\(X\\), if \\(A \\subseteq \\bigcup_{j=1}^\\infty B_j\\), then \\(\\mu(A) \\leq \\sum_{j=1}^\\infty \\mu(B_j)\\).</li>\r\n</ol>\r\n\r\n</div></div><br>\r\n<h2 id=\"measurable-spaces\">measurable spaces</h2>\r\n<p>Outer measures provide information about inequalities in measure functions. We are also interested in equalities between measures of sets in the event space. For disjoint sets \\(A\\) and \\(B\\) in \\(\\alpha\\), we want to express the equality \\(m(A \\cup B) = m(A) + m(B)\\). In the general case where \\(A\\) and \\(B\\) are not necessarily disjoint, we seek the equality \\(m(A \\cup B) = m(A \\setminus B) + m(A \\cap B) + m(B \\setminus A)\\).\r\n</p>\r\n<p>To control equality restrictions on measures, we introduce measurable spaces (\\(σ\\)-algebras). A measurable space \\(M: \\text{set}(\\text{set} \\ \\alpha)\\) includes \"measurable sets\" and satisfies:</p>\r\n\r\n<ul>\r\n  <li>Empty set measurability: \\(\\varnothing \\in M\\)</li>\r\n  <li>Complement closure: For any set \\(s\\) in \\(\\alpha\\), if \\(s \\in M\\), then its complement \\(s^c \\in M\\)</li>\r\n  <li>Countable union closure: For any sequence \\(f: \\mathbb{N} \\to \\text{set} \\ \\alpha\\), if \\(f(i) \\in M\\) for all \\(i\\) in \\(\\mathbb{N}\\), then \\(\\bigcup\\limits_{i=0}^\\infty f(i) \\in M\\)</li>\r\n</ul>\r\n<p>From these properties, we can derive other intuitive closures, including those of countable intersection:</p>\r\n<p>\\[ \\forall f : \\mathbb{N} \\to \\text{set} \\ \\alpha, \\forall i : \\mathbb{N}, f(i) \\in M \\Rightarrow \\bigcap_{i=0}^\\infty f(i) \\in M, \\]</p>\r\n<p>and difference:</p>\r\n<p>\\[ \\forall a,b : \\text{set} \\ \\alpha, \\ a \\in M \\Rightarrow b \\in M \\Rightarrow a \\setminus b \\in M. \\]</p>\r\n<p>Now, how does <em>Vaishnav</em> define measure transformations?</p>\r\n<p>Given \\(S : \\text{set}(\\text{set} \\ \\alpha)\\), the measurable space \"closure\" of \\(S\\) is the smallest measurable space that includes every set in \\(S\\). The generated measurable space can be visualized as the \"mosaic\" of set intersections and differences formed by \\(S\\) and the universal set (\"univ\") in our event space. The set of measurable sets consists of all possible countable unions of the \"tiles\" in this mosaic.\r\n\r\n </p><p> Now, let \\(C : \\text{set} \\ \\alpha \\rightarrow \\text{Prop}\\) be a predicate. To show that \\(C\\) holds for all sets in \\(\\text{generate}(S)\\), we can prove that it holds for \\(S\\) itself \\((1)\\), the empty set \\((2)\\), under complement \\((3)\\), and under countable union \\((4)\\).</p>\r\n <p>\\((1) \\quad \\forall s \\in S\\), \\(C(s) \\quad (1) \\) </p>\r\n <p>\\((2) \\quad C(\\varnothing) \\quad (2) \\) </p>\r\n <p>\\((3) \\quad \\forall s \\in \\text{generate}(S)\\), \\(C(s) \\Rightarrow C(s^c)\\)</p>\r\n <p>\\((4) \\quad \\forall f : \\mathbb{N} \\to \\text{set} \\ \\alpha, \\quad \\left(\\forall i : \\mathbb{N}, \\ f(i) \\in \\text{generate}(S) \\land C(f(i)\\right) \\Rightarrow C(\\bigcup_{i=0}^{\\infty} f(i))\\)</p>\r\n <p>The <a href=\"https://encyclopediaofmath.org/wiki/Measurable_space\">Encyclopedia of Mathematics</a> describes this more mathematically as:</p>\r\n <div class=\"card\"><div class=\"card-body\">\r\n  <p>Let \\((X,A)\\) and \\((Y,B)\\) be measurable spaces.</p>\r\n<p>A map \\(f:X\\rightarrow Y\\) is called measurable if \\(f^{-1}(B)\\in A\\) for every \\(B\\in B\\).</p>\r\n<p>These two measurable spaces are called isomorphic if there exists a bijection \\(f:X\\rightarrow Y\\) such that \\(f\\) and \\(f^{-1}\\) are measurable (such \\(f\\) is called an isomorphism).</p>\r\n<p>Let \\(X\\) be a set, \\((Y,B)\\) a measurable space, and \\((f_i)_{i\\in I}\\) a family of maps \\(f_i:X\\rightarrow Y\\). The \\(\\sigma\\)-algebra generated by these maps is defined as the smallest \\(\\sigma\\)-algebra \\(A\\) on \\(X\\) such that all \\(f_i\\) are measurable from \\((X,A)\\) to \\((Y,B)\\). More generally, one may take measurable spaces \\((Y_i,B_i)\\) and maps \\(f_i:X\\rightarrow Y_i\\). On the other hand, if \\(Y\\) is \\(\\mathbb{R}\\) (or \\(\\mathbb{C}\\), \\(\\mathbb{R}^n\\), etc.) then \\(B\\) is by default the Borel \\(\\sigma\\)-algebra.</p>\r\n </div></div><br>\r\n <p>But, even this description has a very topological tone. Note: a Borel set is simply formed by open sets \"through the operations of countable union, countable intersection, and relative complement\" (<a href=\"https://en.wikipedia.org/wiki/Borel_sets\">Wikipedia: Borel sets</a>) &mdash; this also follows transfinite induction, etc, but that's outside our scope.</p>\r\n <p>This props us up for a more intuitive (while less rigorous) picture:</p>\r\n <blockquote>\r\n  <b>Definition 4</b>\r\n<p>Let \\(F\\) be a collection of subsets of \\(\\Omega\\) such that:</p>\r\n<ol>\r\n  <li>Closed under complements: If \\(A \\in F\\), then \\(A^C \\in F\\).</li>\r\n  <li>Closed under countable unions: If \\(A_1, A_2, \\ldots\\) are all in \\(F\\), then \\(\\bigcup_{i=1}^\\infty A_i \\in F\\).</li>\r\n</ol>\r\n<p>We call \\(F\\) a \\(\\sigma\\)-algebra, and the elements of \\(F\\) measurable sets.</p>\r\n<p>(<a href=\"https://www.markhuberdatascience.org/probability-textbook\">Huber, Probability: Lectures and Labs</a>)</p>\r\n </blockquote>\r\n <p>No mention of topology's maps, Borel, & generating or type theory's ... predicates ... (I guess the type theory is not too bad yet).</p>\r\n<p>That being said, Huber does clarify:</p>\r\n <blockquote>\r\n  Whenever Ω is a finite set, F will be the set of all subsets of Ω. When Ω = R, F is typically taken to be the Borel sets. In this first course in probability we\r\nwill not define the Borel sets precisely, except to note that any interval (open or closed, finite or\r\ninfinite) is a member of the Borel sets.\r\n</blockquote>\r\n <p><em>Vaishnav</em> also looks at \"Combining Measurable Spaces\":</p>\r\n<blockquote>\r\n<p>Given a set \\(S\\) of measurable spaces, we can now define their \"combination\" into the smallest measurable space that contains both of them (i.e. their supremum) as</p>\r\n\\[ \\text{sup}(S) = \\text{generate}\\left(\\bigcup_{s \\in S} s\\right), \\]\r\n<p>the measurable space generation on the union (set supremum) of all of their sets of measurable sets.</p>\r\n</blockquote>\r\n<p>Again, <em>Vaishnav</em> considers \"Mapping/Co-Mapping Measurable Spaces\":</p>\r\n<blockquote>\r\n<p>As for <b>outer measures</b>, a map/comap on \\((M, \\alpha)\\) with respect to \\(s\\) is defined given a second type \\(\\beta\\), a function \\(f : \\alpha \\to \\beta\\), and a measurable space \\((M, \\alpha)\\) on \\(\\alpha\\). The map is the set of all sets \\(\\beta\\) such that their preimage is a measurable set:</p>\r\n\\[ \\text{map}_f((M, \\alpha)) = \\{ b : \\text{set} \\ \\beta \\mid f^{-1}(b) \\in (M, \\alpha) \\} \\]\r\n\r\n<p>Given a measurable space \\((M, \\beta)\\) on \\(\\beta\\), we define our comap to be the set of all sets \\(\\alpha\\) such that they are the preimage of some measurable set:</p>\r\n\\[ \\text{comap}_f((M, \\beta)) = \\{ a : \\text{set} \\ \\alpha \\mid \\exists b : \\text{set} \\ \\beta, f^{-1}(b) = a \\} \\]\r\n<p>(you can also think of this as simply taking the preimages of every set in \\((M, \\beta)\\)).</p>\r\n</blockquote>\r\n<p>From here, the lovely <em>Vaishnav</em> mentions:</p>\r\n<p>In mathematical notation, the definition can be written as follows:</p>\r\n\r\n<p>A Galois connection is a pair of functions \\(l: \\alpha \\to \\beta\\) and \\(u: \\beta \\to \\alpha\\) between preordered sets \\(\\alpha\\) and \\(\\beta\\) that satisfy the following property:</p>\r\n\r\n\\[\r\n\\forall a \\in \\alpha, b \\in \\beta, \\quad l(a) \\leq b \\iff a \\leq u(b)\r\n\\]\r\n\r\n<p>Here, \\(\\leq\\) represents the order relation in the respective preordered sets \\(\\alpha\\) and \\(\\beta\\). The functions \\(l\\) and \\(u\\) establish a connection between the orders in \\(\\alpha\\) and \\(\\beta\\) such that if \\(l(a)\\) is less than or equal to \\(b\\), then \\(a\\) is less than or equal to \\(u(b)\\), and vice versa.</p>\r\n\r\n\r\n<p><em>Vaishnav</em> explains the effect of this, but gives the lemmas in Lean, so here is the property of the lemmas NOT in Lean!</p>\r\n<p>In mathematical notation, an essential property of a Galois connection is that an upper/lower adjoint of a Galois connection uniquely determines the other:</p>\r\n<p>\r\n\\[\r\n\\begin{align*}\r\nF(a) & \\text{ is the least element } \\tilde{b} \\text{ with } a \\leq G(\\tilde{b}), \\\\\r\nG(b) & \\text{ is the largest element } \\tilde{a} \\text{ with } F(\\tilde{a}) \\leq b.\r\n\\end{align*}\r\n\\]\r\n</p>\r\n<p><a href=\"https://en.wikipedia.org/wiki/Galois_connection\">Source</a></p>\r\n<p>But, as <em>Vaishnav</em> explains:</p>\r\n<blockquote>\r\n  <p>In our case, we can show that, for any \\(f: \\alpha \\to \\beta\\), there is a Galois Connection between \\(\\text{comap}_f(\\cdot)\\) and \\(\\text{map}_f(\\cdot)\\) (where the \\(\\leq\\) relation on measurable spaces is the subset relation). To see this, consider an arbitrary measurable space \\(M_\\beta\\) on \\(\\beta\\) and \\(M_\\alpha\\) on \\(\\alpha\\). Here, \\(\\text{comap}_f(M_\\beta) \\subseteq M_\\alpha\\) means that every set in \\(M_\\beta\\) has a preimage in \\(M_\\alpha\\), which is exactly the same statement as \\(M_\\beta \\subseteq \\text{map}_f(M_\\alpha)\\).</p>\r\n  <p>The Galois Connection gives us the fact that, for any list of measurable spaces \\(M_{\\beta_i}\\) on \\(\\beta\\) indexed by \\(i\\) in \\(\\iota\\), we have that the \\(\\text{comap}_f\\) of the indexed supremum of \\(M_{\\beta_i}\\) is the indexed supremum of the \\(\\text{comap}_f\\) of each measurable space (regardless of whether the index type is countable):</p>\r\n\\[\r\n\\text{comap}_f \\left( \\sup_{i \\in \\iota} M_{\\beta_i} \\right) = \\sup_{i \\in \\iota} \\left( \\text{comap}_f (M_{\\beta_i}) \\right)\r\n\\]</p>\r\n</blockquote>\r\n\r\n<p><em>Vaishnav</em> has even more to say about <b>trimmed outer measure</b>:</p>\r\n\r\n<blockquote>\r\n  <p>Suppose we have a measurable space \\( M \\) on \\( \\alpha \\) and a partial set function \\( m_0 \\) defined on \\( M \\), satisfying the three properties of an <b>outer measure</b> restricted to sets from \\( M \\). We can define the outer measure <i>induce</i>\\( (m_0) \\) as <i>fromfunc</i>(<i>extend</i>\\( (m_0) \\)). Interestingly, with this function, we have the property:</p>\r\n\r\n\\[\r\n\\forall s \\in \\text{set } \\alpha, \\text{induce}(m_0)(s) = \\inf_{t \\in \\text{set} \\alpha} s \\subseteq t \\land t \\in M \\Rightarrow m_0(t)\r\n\\]\r\n\r\n<p>In other words, the measurable sets retain their values from the partial function due to the monotonicity property, and each non-measurable set is assigned the infimum of the values of all its measurable supersets.</p>\r\n  <p>\\([...]\\)</p>\r\n  <p>As a matter of fact, there exists a measurable superset that exactly matches this infimum:</p>\r\n\\[\r\n\\forall s \\in \\text{set } \\alpha, \\exists t \\in \\text{set } \\alpha, s \\subseteq t \\land t \\in \\mathcal{M} \\land \\text{induce}(m_0)(s) = m_0(t)\r\n\\]\r\n<p>The proof of this relies on constructing a list \\(f: \\mathbb{N} \\to \\text{set } \\alpha\\) of measurable sets that \"narrows down\" to \\(\\text{induce}(m_0)(s)\\) in the sense that:</p>\r\n\\[\r\n\\forall i \\in \\mathbb{N}, m_0(f(i)) < \\text{induce}(m_0)(s) + i - 1\r\n\\]\r\n<p>Now, given an arbitrary outer measure \\(m\\) and a measurable set \\(M\\), we can check whether \\(m\\) was constructed as above using the notion of a \"trimmed\" outer measure. Let \\(\\text{sub}_M(m)\\) be \\(m\\) restricted to the domain \\(M\\). Defining:</p>\r\n\\[\r\n\\text{trim}_M(m) = \\text{induce}(\\text{sub}_M(m))\r\n\\]\r\n<p>We can now check whether \\(\\text{trim}_M(m) = m\\), i.e., whether \\(\\text{trim}_M\\) assigns the same values to non-measurable sets as \\(m\\) originally had.</p>\r\n</blockquote>\r\n\r\n<p>Finally, <em>Vaishnav</em> touches on Pi-types, which are a unique perspective on measure theory coming from type theory.</p>\r\n\r\n<p>Here is a definition I like of Pi-types:</p>\r\n\r\n<div class=\"card\"><div class=\"card-body\">\r\n  <p>Given a dependent type \\(x : C \\vdash D(x)\\), the Pi-type \\(\\Pi x : C. D(x)\\) can be thought of as the set of all functions \\(f\\) from \\(C\\) to \\(\\prod_{x : C} D(x)\\) such that \\(f(x) \\in D(x)\\) for all \\(x \\in C\\). These functions are called dependent maps from \\(C\\) to \\(D\\). In set theory, we interpret simple types \\(C\\) as sets \\(C\\) and terms \\(x : C\\) as elements \\(x \\in C\\). Dependent types \\(D\\) over \\(C\\) are interpreted as families \\(D = (D(x))_{x \\in C}\\) of sets \\(D(x)\\).</p>\r\n  <p><a href=\"https://arxiv.org/pdf/1905.00993.pdf\">Source</a></p>\r\n</div></div><br>\r\n\r\n<p>Suppose we have an index of types \\(\\beta : \\iota \\rightarrow \\text{Type}\\) and a mapping \\(M\\) that assigns a measurable space to each type. We want to define a measurable space on the function space \\(\\prod_{i:\\iota} \\beta(i)\\) that corresponds to \\(M\\).</p>\r\n\r\n<p>To accomplish this, we define a \"function evaluation\" function \\(eval(i)\\) that extracts the value at a specific index. Given a function \\(f : \\prod_{i:\\iota} \\beta(i)\\), \\(eval(i)(f)\\) gives us the value of \\(f\\) at index \\(i\\).</p>\r\n}}\r\n<p>Next, we consider the preimage of a set \\(s\\) in type \\(\\beta(i)\\) under the function \\(eval(i)\\). This preimage consists of all functions whose value at index \\(i\\) lies in the set \\(s\\).</p>\r\n\r\n<p>By applying the comap operation to \\(eval(i)\\) with respect to \\(M(i)\\), we obtain the measurable sets in \\(\\beta(i)\\) that correspond to these preimages. We repeat this process for all indices \\(i\\) and combine the resulting measurable spaces using the supremum operation. This gives us the definition of the \"pi measurable space\":</p>\r\n\r\n<p>\\(\\Pi\\text{-measurable-space}(M) = \\sup_{i:\\iota} \\left( \\text{comap}_{\\text{eval}(i)}(M(i)) \\right)\\).</p>\r\n\r\n<p>This is not too important to measure-theoretic probability theory?</p>\r\n\r\n<h2 id=\"defining-a-measure\">defining a measure</h2>\r\n\r\n<blockquote>\r\n  <p>We now have all the definitions we need to define an outer measure that corresponds to the concept of probability, allowing us to derive useful equalities based on a measurable space. For any \\( f : \\mathbb{N} \\to \\text{set } \\alpha \\), let \\( PD(f) \\) indicate that \\( f \\) is pairwise disjoint, meaning the sets that \\( f \\) maps to with any distinct pair of indices are disjoint. Given a measurable space \\( M \\), we define a \"measure\" on \\( M \\) as an outer measure \\( m \\) that additionally satisfies the properties of <b>disjoint measurable set countable union</b> and trim invariance:</p>\r\n<p style=\"overflow: auto;\">\r\n\\[\r\n\\forall f : \\mathbb{N} \\to \\text{set } \\alpha, (\\forall i : \\mathbb{N}, f(i) \\in M) \\land (PD(f)) \\Rightarrow \\] \\[\\quad m\\left(\\bigcup_{i=0}^\\infty f(i)\\right) = \\sum_{i=0}^{\\infty} m(f(i)),\r\n\\]\r\n\\[ \\text{trim}_M(m) = m\\]\r\n</p>\r\n\r\n<p>The disjoint measurable set countable union property ensures that this measure corresponds to the notion of probability, as we know it, where the measure of the union of any list of disjoint measurable sets can be expressed as the sum of their individual measures.</p>\r\n</blockquote>\r\n\r\n<p>As <em>Vaishnav</em> explains, the final criterion for an outer measure to correspond to a notion of probability is that it assigns a measure of 1 to the entire event space. A measure on the event space \\(\\alpha\\) satisfying \\(\\mu(\\text{univ} : \\text{set } \\alpha) = 1\\) is referred to as a \"probability measure\". In general, probability measures are a type of \"finite measure\" that satisfies \\(\\mu(\\text{univ}) < \\infty\\).</p>\r\n\r\n<h2 id=\"measure-transformations\">measure transformations</h2>\r\n\r\n<p>Measures themselves are a type of outer measure that satisfies additional criteria. To achieve this, we \"lift\" outer measure transformations to the context of measures.</p>\r\n<p>Two specific transformations we focus on are \\(restrict(·)\\) and \\(map(·)\\), as defined earlier. However, directly applying these transformations to a measure may not always result in an outer measure that satisfies the criteria to be considered a measure. To address this, we need additional transformations that preserve the behavior of the original transformation on measurable sets and ensure that the resulting outer measure remains a measure.</p>\r\n<p>This, again, comes down to varifying properties of <b>outer measure</b> and properties of <b>transformations on measurable spaces</b> already stated.</p>\r\n<p>Recall, for <b>outer measure</b>, we have the empty set property \\((1)\\), monotonicity property \\((2)\\), and countable union property \\((3)\\):</p>\r\n<p>\r\n  \\[\r\n\\begin{align*}\r\nm(\\varnothing) &= 0, \\quad & (1) \\\\\r\n\\forall A, B : \\text{set } \\alpha, A \\subseteq B & \\Rightarrow m(A) \\leq m(B), \\quad & (2) \\\\\r\n\\forall f : \\mathbb{N} \\rightarrow \\text{set } \\alpha, m\\left(\\bigcup_{i=0}^\\infty f(i)\\right) & \\leq \\sum_{i=0}^\\infty m(f(i)), \\quad & (3)\r\n\\end{align*}\r\n\\]\r\n</p>\r\n<p>Recall, for <b>transforming measurable spaces</b> let \\(C : \\text{set} \\ \\alpha \\rightarrow \\text{Prop}\\) be a predicate. To show that \\(C\\) holds for all sets in \\(\\text{generate}(S)\\), we can prove that it holds for \\(S\\) itself \\((1)\\), the empty set \\((2)\\), under complement \\((3)\\), and under countable union \\((4)\\).</p>\r\n <p>\\((1) \\quad \\forall s \\in S\\), \\(C(s) \\quad (1) \\) </p>\r\n <p>\\((2) \\quad C(\\varnothing) \\quad (2) \\) </p>\r\n <p>\\((3) \\quad \\forall s \\in \\text{generate}(S)\\), \\(C(s) \\Rightarrow C(s^c)\\)</p>\r\n <p>\\((4) \\quad \\forall f : \\mathbb{N} \\to \\text{set} \\ \\alpha, \\quad \\left(\\forall i : \\mathbb{N}, \\ f(i) \\in \\text{generate}(S) \\land C(f(i)\\right) \\Rightarrow C(\\bigcup_{i=0}^{\\infty} f(i))\\)</p>\r\n<p>But, most importantly, we must have <b>disjoint measurable set countable union</b>:</p>\r\n\\[\r\n\\forall f : \\mathbb{N} \\to \\text{set } \\alpha, (\\forall i : \\mathbb{N}, f(i) \\in M) \\land (PD(f)) \\Rightarrow \\] \\[\\quad m\\left(\\bigcup_{i=0}^\\infty f(i)\\right) = \\sum_{i=0}^{\\infty} m(f(i)),\r\n\\]\r\n<p>And, the <b>Caratheodory Criterion</b> is sufficient for to prove <b>disjoint measurable set countable union</b>. <blockquote>Given a set function \\(f : \\text{set} \\alpha \\rightarrow \\mathbb{R}^+\\cup\\{\\infty\\}\\) and a set \\(s\\), \\(s\\) is referred to as \"Carathéodory\" (written as \\(\\text{cara } f(s)\\)) if\r\n  \\(\\forall t : \\text{set} \\alpha, f(t) = f(t \\cap s) + f(t \\setminus s)\\),\r\n  which means that the measure of any set \\(t\\) can be decomposed into the measure of its intersection with \\(s\\) and the measure of its difference with \\(s\\). \r\n  \r\n  Now, consider an outer measure \\(m\\) and measurable space \\(M\\). It can be shown that the set of all Carathéodory sets forms a measurable space.</blockquote></p>\r\n <p>At this point, relies on higher levels of type-theoretic abstractions. The more elementary treatments thus far have been alright, with support from topology every now and then. Still, for those interested in a higher level type-theoretic persepctive, check out <em>Vaishnav's</em> subsections on \"Lifting an Outer Measure Transformation\", or \"Restricting a Measure\", or \"Mapping a Measure\". These topics can also be treated with topology outside the scope of these notes.</p>\r\n<h2 id=\"probability-theory\">probability theory</h2>\r\n  <h3 id=\"independence\">independence</h3>\r\n  <p>Given a measure \\(\\mu\\) on \\(\\alpha\\) and two events \\(a\\) and \\(b\\) from our event space, we define independence by requiring that the measure of their intersection is the product of their measures:</p>\r\n  \\[\r\n  \\mu(a \\cap b) = \\mu(a) \\mu(b)\r\n  \\]\r\n  <p>However, we can extend this notion of independence to sets of events. We define independence between sets of events \\(A\\) and \\(B\\) under \\(\\mu\\) as follows:</p>\r\n  <p>\\(A\\) and \\(B\\) are independent under \\(\\mu\\) if for any pair of events \\(a \\in A\\) and \\(b \\in B\\), the measure of their intersection is the product of their measures:</p>\r\n  \\[\r\n  \\text{indep_sets}_\\mu(A, B) \\Leftrightarrow \\forall a \\in A, b \\in B, \\mu(a \\cap b) = \\mu(a) \\mu(b)\r\n  \\]\r\n  <p>This basically says that: two measurable spaces are independent with respect to their measures if their sets-of-events (which are, again, sets of sets) are independent with respect to the measures. This does not need math markup, sorry <em>Vaishnav</em>.</p>\r\n  <p>Given two sets \\(a,b : \\text{set } \\alpha\\), we define independence between them under \\(\\mu\\) as:</p>\r\n\\[ \\text{indep_set}_\\mu(a,b) \\Leftrightarrow \\text{indep}_\\mu(\\text{generate}(\\{a\\}), \\text{generate}(\\{b\\})), \\]\r\n<p>which is the independence of their generated measurable spaces. Note that for any set \\(s : \\text{set } \\alpha\\), \\(\\text{generate}(\\{s\\}) = \\{0,\\emptyset,s,s^c,\\alpha\\}\\) exactly. It may be immediately obvious that this definition is equivalent to \\(\\mu(a \\cap b) = \\mu(a) \\mu(b)\\), but we will soon show that this fact arises from a more general result regarding independence on the measurable spaces generated by a pair of independent \"\\(π\\)-systems\".</p>\r\n\r\n<p>Now, that was <em>Vaishnav's</em> idea of definitions, so here are the <em>Vaishnav's</em> idea of properties.</p>\r\n<blockquote>\r\n  <p>A \"Dynkin system,\" also known as a \"\\(λ\\)-system,\" is a generalization of measurable spaces that enables an induction principles. We define a Dynkin system as a set of sets \\(M : \\text{set } (\\text{set } \\alpha)\\) with the properties of \\(1\\) empty set measurability, \\(2\\) complement closure, and \\(3\\) <em>disjoint</em> countable union closure:</p>\r\n<ul style=\"overflow:auto;\">\r\n<li>\\(\\emptyset \\in M \\quad (1) \\)</li>\r\n<li>\\(\\forall s : \\text{set } \\alpha, s \\in M \\Rightarrow s^c \\in M \\quad (2) \\)</li>\r\n<li>\\(\\forall f : \\mathbb{N} \\rightarrow \\text{set } \\alpha, \\forall i : \\mathbb{N}, f(i) \\in M \\land \\text{PD}(f) \\Rightarrow \\bigcup_{i=0}^{\\infty} f(i) \\in M \\quad (3) \\)</li>\r\n</ul>\r\n<p>The disjointedness condition in \\(1\\) is the only difference from the definition of measurable spaces (see (2.7)).</p>\r\n<p>Given any \\(S : \\text{set } (\\text{set } \\alpha)\\), we can consider its Dynkin system \"closure,\" which is the smallest Dynkin system including every set in \\(S\\). We define this closure similarly to how we did for measurable spaces with \\(\\text{generate}(S)\\), modifying the last construction rule to union only over disjoint sets. We denote this generator as \\(\\text{generate}_D(S)\\).</p>\r\n<p>Now, suppose we have some predicate \\(C : \\text{set } \\alpha \\rightarrow \\text{Prop}\\). If we want to show that \\(C\\) holds for all sets in \\(\\text{generate}_D(S)\\), we have to prove that \\(1\\) it holds for \\(S\\) itself, \\(2\\) for the empty set, \\(3\\) under complement, and \\(4\\) under <em>disjoint</em> countable union:</p>\r\n<ul style=\"overflow:auto;\">\r\n<li>\\(\\forall s \\in S, C(s)\\quad (1) \\)</li>\r\n<li>\\(C(\\emptyset) \\quad (2) \\)</li>\r\n<li>\\(\\forall s \\in \\text{generate}_D(S), C(s) \\Rightarrow C(s^c) \\quad (3) \\)</li>\r\n<li>\\(\\forall f : \\mathbb{N} \\rightarrow \\text{set } \\alpha, \\left( \\forall i : \\mathbb{N}, f(i) \\in \\text{generate}_D(S) \\land C(f(i)) \\right) \\land \\text{PD}(f) \\Rightarrow C \\left( \\bigcup_{i=0}^{\\infty} f(i) \\right) \\quad (4) \\)</li>\r\n</ul>\r\n</blockquote>\r\n<p>As <em>Vaishnav</em> explains that Dynkin systems have an important property: closure under intersection implies measurability. The proof relies on the ability to derive difference closure from intersection closure, enabling the decomposition of sets into a disjoint list with the same union. Additionally, the set \\(\\{t \\mid t \\cap s \\in D\\}\\), where \\(D\\) is a Dynkin system and \\(s \\in D\\), is also a Dynkin system.</p>\r\n<p>To determine the applicability of the extra disjointedness condition, we introduce \\(\\pi\\)-systems. These systems establish the equivalence between generated measurable spaces and generated Dynkin systems, allowing us to leverage \\(\\pi\\)-system properties in specific situations.</p>\r\n<p><em>Vaishnav</em> then describes Dykin's <b>\\(\\pi\\)-\\(\\lambda\\) theorem</b>.</p>\r\n<blockquote>\r\n  <p>We define the \"\\(π\\)-system\" criterion for a set \\(S : \\text{set } (\\text{set } \\alpha)\\) as:</p>\r\n\\[PI(S) \\Leftrightarrow \\forall s, t \\in S, s \\cap t \\neq \\varnothing \\Rightarrow s \\cap t \\in S,\\]\r\n<p>which simply requires that \\(S\\) is closed under non-empty intersection. The inclusion of the non-empty requirement allows for greater generality, although it is not necessary for our results. It is immediately evident that singleton sets satisfy the \\(π\\)-system criterion:</p>\r\n\\[\\forall s : \\text{set } \\alpha, PI(\\{s\\})\\]\r\n<p>Remarkably, if \\(PI(S)\\), then the generated Dynkin system, \\(\\text{generate}_D(S)\\), is closed under intersection, making it a measurable space. Furthermore, we have \\(\\text{generate}_D(S) = \\text{generate}(S)\\), indicating that the generated measurable space and Dynkin systems are identical. This result is known as Dynkin's \\(π\\)-\\(λ\\) theorem.</p>\r\n<p>We now have the criterion needed for a new induction principle on generated measurable spaces: If they are generated by a \\(π\\)-system, we can replace \"countable union for generating measuable spaces\" (recall:) \r\n  </p><p style=\"overflow: auto;\">\\[\\forall f : \\mathbb{N} \\rightarrow \\text{set } \\alpha, \\left( \\forall i : \\mathbb{N}, f(i) \\in \\text{generate}_D(S) \\land C(f(i)) \\right) \\land \\text{PD}(f) \\Rightarrow C \\left( \\bigcup_{i=0}^{\\infty} f(i) \\right)\\]</p><p> \r\n  with \"countable union for generating \\(π\\)-systems\":</p>\r\n  </p><p style=\"overflow: auto;\">\r\n    \\[(\\forall i : \\mathbb{N}, \\ f(i) \\in generate(S) \\land C(f(i))) \\land PD(f) \\Rightarrow C\\left(\\bigcup_{i=0}^\\infty f(i)\\right),\\]\r\n  </p>\r\n    <p>where the new hypothesis, \\(PD(f)\\), assists in the proof.</p>\r\n    <p>Now, <em>Vaishnav</em> offer's an axiomatic defintiion of independence:</p>\r\n    <p>With the help of \"countable union for generating \\(π\\)-systems\" we can prove that,\r\n      </p><p style=\"overflow: auto;\">\r\n      \\[\r\n      \\forall S, T: \\operatorname{set}(\\operatorname{set} \\alpha), \\operatorname{PI}(S) \\wedge \\operatorname{PI}(T) \\Longrightarrow{\\operatorname{indep} \\_\\operatorname{set}_\\mu}_\\mu(S, T) \\Longleftrightarrow \\operatorname{indep}_\\mu(\\operatorname{generate}(S), \\operatorname{generate}(T))\r\n      \\]\r\n      </p><p>\r\n      in English, the independence of \\(\\pi\\)-systems implies the independence of the measurable spaces they generate. We can now simplify our criterion for independence of sets: from (\"the independence of \\(\\pi\\)-systems implies the independence of the measurable spaces they generate\"), and the fact that singletons are \\(\\pi\\)-systems, we have </p>\r\n      <p style=\"overflow: auto;\">\\[\\forall s, t : \\text{set } \\alpha, \\text{indep_set}_\\mu (s, t) \\Leftrightarrow \\text{indep_sets}_\\mu (\\{s\\}, \\{t\\}) \\Leftrightarrow \\mu(s \\cap t) = \\mu(s) \\mu(t)\\]</p>\r\n      <p>So, we now have license to use the \"bundled\" \\(\\operatorname{indep}_{-} \\operatorname{set}_\\mu(s, t)\\) in place of our classical definition of independence (again, the equivalence can easily be seen by cases, but it's good to know the general result).</p>\r\n</blockquote>\r\n\r\n<h2>The End</h2>\r\n\r\n<p>At this point, I do not find the remaining formalities conducive to my personal study of probabiity theory, although they might interest me more if I was more familiar with Lean.</p>\r\n<p>Check out the rest of <em>Vaishnav's</em> paper for (again, you might find it <a href=\"https://escholarship.org/uc/item/8hb1w6js\">here</a>, ceteris paribus):</p>\r\n\r\n<ul>\r\n  <li>conditional probability</li>\r\n  <li>random variables\r\n    <ul>\r\n      <li>the joint distribution</li>\r\n      <li>restricting a pi</li>\r\n      <li>the marginal distribution</li>\r\n      <li>marginalization</li>\r\n      <li>independence on random variables</li>\r\n      <li>conditional independence on random variables</li>\r\n    </ul>\r\n  </li>\r\n</ul>\r\n\r\n\r\n\r\n",
      "date_published": "2023-07-01T17:00:00-07:00"
    },{
      "id": "https://ischmidls.github.io/posts/rank/",
      "url": "https://ischmidls.github.io/posts/rank/",
      "title": "notes - matrix rank",
      "content_html": "<ul>\r\n    <li>\r\n      <a href=\"#consistency\">consistency</a>\r\n    </li>\r\n    <li>\r\n      <a href=\"#splitting-lemma\">splitting lemma</a>\r\n    </li>\r\n    <li>\r\n      <a href=\"#to-be-continued\">to be continued</a>\r\n    </li>\r\n  </ul>\r\n  <h2 id=\"consistency\">consistency</h2>\r\n  <p>Let Let \\(A\\) denote a matrix and \\(A | B \\) denote its augmented matrix.</p>\r\n  <p>Consistent Systems:</p>\r\n  <ul>\r\n    <li>\r\n      <b>Def 1</b> Full Rank: Unique solution. \\(\\text{rank} (A) = \\text{rank} (A|B) = \\text{col} (A)\\)\r\n    </li>\r\n    <li>\r\n      <b>Def 2</b> Underdetermined: Infinitely many solutions. \\(\\text{rank} (A) = \\text{rank} (A|B) < \\text{col} (A) \\) </li>\r\n  </ul>\r\n  <p>Inconsistent System:</p>\r\n  <ul>\r\n    <li>\r\n      <b>Def 3</b> Overdetermined: No solution. \\(\\text{rank} (A) > \\text{col} (A) \\)\r\n    </li>\r\n  </ul>\r\n  <p>Proofs</p>\r\n  <div class=\"card\">\r\n    <div class=\"card-body\">\r\n      <p>To see this note that rank of the matrix is dimension of the span of columns of the matrix. Now if \\(Ax=b\\) has solution, then it means that some linear combination of columns of A gives us b, which implies that \\(b\\) lies in \\(span(A)\\) and so \\(rank(A|b)=rank(A)\\). You can argue similarly in the reverse direction.</p>\r\n      <p>\r\n        <a href=\"https://math.stackexchange.com/a/2090749/1098426\">Source</a>\r\n      </p>\r\n    </div>\r\n  </div>\r\n  <br>\r\n  <div class=\"card\">\r\n    <div class=\"card-body\">\r\n      <p>A matrix \\(A\\in\\mathcal{M}_{nm}(\\Bbb{R})\\) \\([\\) <span class=\"text-secondary\">of dimensions \\(n \\ \\text{rows} \\times m \\ \\text{columns} \\ \\) respectively</span>\\(]\\) is a representation of a linear transformation \\(f\\colon\\Bbb{R}^m\\rightarrow \\Bbb{R}^n\\), so the image of \\(f\\) is a subspace of \\(\\Bbb{R}^n\\). Hence, \\(\\operatorname{rank}(f)=\\dim(\\operatorname{Im}(f))\\leq n\\), </p>\r\n      <p>But also, by the <b>Rank-Nullity Theorem \\((*)\\)</b>, we have \\(\\operatorname{rank}(f)\\leq m\\). Hence, we find the desired result \\(\\operatorname{rank}(f)\\leq \\min(m,n)\\). </p>\r\n      <p>\r\n        <a href=\"https://math.stackexchange.com/a/759000/1098426\">Source</a>\r\n      </p>\r\n      <div class=\"pl-5\">\r\n        <p>Now, recall the <b>Rank-Nullity Theorem \\((*)\\)</b>\r\n        </p>\r\n        <p>\r\n          <b>Def 4a </b>\r\n          <em>Transforms</em>\r\n        </p>\r\n        <p>Let \\(T: V \\rightarrow W\\) be a linear transformation between two vector spaces where \\(T\\)'s domain \\(V\\) is finite dimensional. Then</p>\r\n        <p>\\(\\operatorname{rank}(T) + \\operatorname{nullity}(T) = \\operatorname{dim}(V)\\),</p>\r\n        <p>where \\(\\operatorname{rank}(T)\\) is the rank of \\(T\\) (the dimension of its image) and \\(\\operatorname{nullity}(T)\\) is the nullity of \\(T\\) (the dimension of its kernel). In other words,</p>\r\n        <p>\\(\\operatorname{dim}(\\operatorname{Im}(T)) + \\operatorname{dim}(\\operatorname{Ker}(T)) = \\operatorname{dim}(\\operatorname{Domain}(T))\\).</p>\r\n        <p>This theorem can be refined via the <b>Splitting Lemma \\((**)\\)</b> \\([\\) <span class=\"text-secondary\">a category-theoretic generalization of this theorem from dimensions to spaces </span>\\(]\\) to be a statement about an isomorphism of spaces, not just dimensions. Explicitly, since \\(T\\) induces an isomorphism from \\(V/\\operatorname{Ker}(T)\\) to \\(\\operatorname{Im}(T)\\), the existence of a basis for \\(V\\) that extends any given basis of \\(\\operatorname{Ker}(T)\\) implies, via the splitting lemma, that \\(\\operatorname{Im}(T) \\oplus \\operatorname{Ker}(T) \\cong V\\). Taking dimensions, the rank–nullity theorem follows. </p>\r\n        <p>\r\n          <b>Def 4b </b>\r\n          <em>Matrices</em>\r\n        </p>\r\n        <p>Linear maps can be represented with matrices. More precisely, an \\(m \\times n\\) matrix \\(M\\) represents a linear map \\(f: \\mathbb{F}^n \\rightarrow \\mathbb{F}^m\\), where \\(\\mathbb{F}\\) is the underlying field. So, the dimension of the domain of \\(f\\) is \\(n\\), the number of columns of \\(M\\), and the rank–nullity theorem for an \\(m \\times n\\) matrix \\(M\\) is</p>\r\n        <p>\\(\\operatorname{rank}(M) + \\operatorname{nullity}(M) = n\\).</p>\r\n        <p>\r\n          <a href=\"https://www.d.umn.edu/~jgreene/Math_5327_Linear/Transformations.pdf\">Source</a>\r\n        </p>\r\n      </div>\r\n    </div>\r\n  </div>\r\n  <br>\r\n  <h2 id=\"splitting-lemma\">splitting lemma</h2>\r\n  <div class=\"card\">\r\n    <div class=\"card-body\">\r\n      \r\n        <p>\r\n          <b>\\((**)\\) Splitting Lemma</b>\r\n        </p>\r\n        <p>Perhaps the treatments of the splitting lemma that are closest to linear algebra and matrices are those involving <em>groups</em> or <em>modules</em>\r\n        </p>\r\n        <p> Splitting Lemma for Groups: Let \\( G \\) be a group and \\( H \\) a subgroup of \\( G \\). If there exists a homomorphism \\( \\varphi : G \\rightarrow H \\) such that \\( \\varphi \\) is the identity on \\( H \\), then \\( G \\) is the internal direct product of \\( H \\) and \\( \\ker(\\varphi) \\). This means that if there exists a homomorphism \\( \\varphi \\) from a group \\( G \\) to a subgroup \\( H \\) such that \\( \\varphi \\) is the identity on \\( H \\), then \\( G \\) can be expressed as the direct product of \\( H \\) and the kernel of \\( \\varphi \\). This gives a decomposition of the group \\( G \\) into two subgroups. </p>\r\n        <p>More precisely from Prof. Conrad:</p>\r\n        <blockquote>\r\n          <p>A sequence of groups and group homomorphisms</p>\r\n          <p>\\(H \\xrightarrow{\\alpha} G \\xrightarrow{\\beta} K\\)</p>\r\n          <p>is called exact at \\(G\\) if \\(\\operatorname{im}(\\alpha) = \\operatorname{ker}(\\beta)\\). This means two things: the image of \\(\\alpha\\) is killed by \\(\\beta\\) (\\(\\beta(\\alpha(h)) = 1\\) for all \\(h \\in H\\)), so \\(\\operatorname{im}(\\alpha) \\subseteq \\operatorname{ker}(\\beta)\\), and also only the image of \\(\\alpha\\) is killed by \\(\\beta\\) (if \\(\\beta(g) = 1\\) then \\(g = \\alpha(h)\\) for some \\(h\\)), so \\(\\operatorname{ker}(\\beta) \\subseteq \\operatorname{im}(\\alpha)\\). For example, to say \\(1 \\rightarrow G \\xrightarrow{f} K\\) is exact at \\(G\\) means \\(f\\) is injective, and to say \\(H \\xrightarrow{f} G \\rightarrow 1\\) is exact at \\(G\\) means \\(f\\) is surjective. There is no need to label the homomorphisms coming out of \\(1\\) or going to \\(1\\) since there is only one possible choice. If the group operations are written additively, we may use \\(0\\) in place of \\(1\\) for the trivial group.</p>\r\n          <p>A short exact sequence of groups is a sequence of groups and group homomorphisms</p>\r\n          <p>\\(1 \\rightarrow H \\xrightarrow{\\alpha} G \\xrightarrow{\\beta} K \\rightarrow 1\\)</p>\r\n          <p>which is exact at \\(H\\), \\(G\\), and \\(K\\). That means \\(\\alpha\\) is injective, \\(\\beta\\) is surjective, and \\(\\operatorname{im}(\\alpha) = \\operatorname{ker}(\\beta)\\).</p>\r\n          <p>A more general exact sequence can have lots of terms:</p>\r\n          <p>\\(G_1 \\xrightarrow{\\alpha_1} G_2 \\xrightarrow{\\alpha_2} \\ldots \\xrightarrow{\\alpha_{n-1}} G_n\\),</p>\r\n          <p>and it must be exact at each \\(G_i\\) for \\(1 < i < n\\). Exact sequences can also be of infinite length in one or both directions. We will only deal with short exact sequences here. </p>\r\n              <p>Exact sequences first arose in algebraic topology, and the later development of homological algebra (the type of algebra underlying algebraic topology) spread exact sequences into the rest of mathematics.</p>\r\n              <p>Theorem 3.2. Let \\(1 \\rightarrow H \\xrightarrow{\\alpha} G \\xrightarrow{\\beta} K \\rightarrow 1\\) be a short exact sequence of groups. The following are equivalent:</p>\r\n              <p>(1) There is a homomorphism \\(\\alpha_0: G \\rightarrow H\\) such that \\(\\alpha_0(\\alpha(h)) = h\\) for all \\(h \\in H\\).</p>\r\n              <p>(2) There is an isomorphism \\(\\theta: G \\rightarrow H \\times K\\) such that the diagram</p>\r\n              <p>\r\n                <img src=\"../../img/rank/commute-groups.png\">\r\n              </p>\r\n              <p>commutes, where the bottom row is the short exact sequence for a direct product.</p>\r\n              <p>The commutative diagram in (2) says that \\(\\theta\\) identifies \\(\\alpha\\) with the embedding \\(H \\rightarrow H \\times K\\) and \\(\\beta\\) with the projection \\(H \\times K \\rightarrow K\\). So the point of (2) is not simply that \\(G\\) is isomorphic to \\(H \\times K\\), but it is in a way that turns \\(\\alpha\\) and \\(\\beta\\) into the standard maps from \\(H\\) to \\(H \\times K\\) and from \\(H \\times K\\) to \\(K\\).</p>\r\n              <p>The key point of (1) is that \\(\\alpha_0\\) is a homomorphism. Merely from \\(\\alpha\\) being injective, there is a function \\(\\alpha_0: G \\rightarrow H\\) such that \\(\\alpha_0(\\alpha(h)) = h\\) for all \\(h\\), for instance the function \\(\\alpha_0(g) = \\{ \\mathbf{1} \\iff g \\notin \\alpha(H) \\land h \\iff g = \\alpha(h) \\} \\). But this \\(\\alpha_0\\) is almost surely not a homomorphism.</p>\r\n        </blockquote>\r\n        <p> Splitting Lemma for Modules: Let \\( R \\) be a ring and \\( M \\) a module over \\( R \\). If \\( M \\) is a direct sum of two submodules, say \\( M = A \\oplus B \\), then any submodule \\( N \\) of \\( M \\) can be expressed as the direct sum of \\( N \\cap A \\) and \\( N \\cap B \\). This means that if a module \\( M \\) can be expressed as the direct sum of two submodules \\( A \\) and \\( B \\), then any submodule \\( N \\) of \\( M \\) can be decomposed as the direct sum of its intersections with \\( A \\) and \\( B \\). This provides a way to decompose submodules of a module that is a direct sum. </p>\r\n        <p>Again, more precisely from Prof. Conrad:</p>\r\n        <blockquote>\r\n          <p> Let \\( R \\) be a commutative ring. A sequence of \\( R \\)-modules and \\( R \\)-linear maps \\( N \\xrightarrow{f} M \\xrightarrow{g} P \\) is called exact at \\( M \\) if \\( \\text{im}(f) = \\text{ker}(g) \\). For example, to say \\( 0 \\xrightarrow{h} M \\) is exact at \\( M \\) means \\( h \\) is injective, and to say \\( N \\xrightarrow{h} M \\xrightarrow{0} \\) is exact at \\( M \\) means \\( h \\) is surjective. The linear maps coming out of \\( 0 \\) or going to \\( 0 \\) are unique, so there is no need to label them. </p>\r\n          <p> A short exact sequence of \\( R \\)-modules is a sequence of \\( R \\)-modules and \\( R \\)-linear maps \\[ 0 \\xrightarrow{f} N \\xrightarrow{g} M \\xrightarrow{h} P \\xrightarrow{} 0 \\] which is exact at \\( N \\), \\( M \\), and \\( P \\). That means \\( f \\) is injective, \\( g \\) is surjective, and \\( \\text{im}(f) = \\text{ker}(g) \\). </p>\r\n          <p>[...]</p>\r\n          <p>\r\n            <strong>Theorem 2.1.</strong> Let \\(0 \\rightarrow N \\xrightarrow{f} M \\xrightarrow{g} P \\rightarrow 0\\) be a short exact sequence of \\(R\\)-modules.\r\n          </p>\r\n          <p>The following are equivalent:</p>\r\n          <ol>\r\n            <li>There is an \\(R\\)-linear map \\(f_0: M \\rightarrow N\\) such that \\(f_0(f(n)) = n\\) for all \\(n \\in N\\).</li>\r\n            <li>There is an \\(R\\)-linear map \\(g_0: P \\rightarrow M\\) such that \\(g(g_0(p)) = p\\) for all \\(p \\in P\\).</li>\r\n            <li>The short exact sequence splits: there is an isomorphism \\(\\theta: M \\rightarrow N \\oplus P\\) such that the diagram below commutes.</li>\r\n          </ol>\r\n          <p>\r\n            <img src=\"../../img/rank/commute-modules.png\">\r\n          </p>\r\n          <p>If we replace \\(R\\)-modules with groups and \\(R\\)-linear maps with group homomorphisms, conditions (1) and (2) are not equivalent: for a short exact sequence \\(1 \\rightarrow H \\xrightarrow{f} G \\xrightarrow{g} K \\rightarrow 1\\), (1) corresponds to \\(G\\) being a direct product of \\(H\\) and \\(K\\) while (2) corresponds to \\(G\\) being a semidirect product of \\(H\\) and \\(K\\). The reason (1) and (2) are no longer equivalent for groups is related to noncommutativity. For an exact sequence of abelian groups, (1) and (2) are equivalent (this is the special case \\(R = \\mathbb{Z}\\), since abelian groups are \\(\\mathbb{Z}\\)-modules).</p>\r\n        </blockquote>\r\n        <p>More on the Splitting Lemma for . . . <a href=\"https://kconrad.math.uconn.edu/blurbs/linmultialg/splittingmodules.pdf\">Modules</a> and <a href=\"https://kconrad.math.uconn.edu/blurbs/grouptheory/splittinggp.pdf\">Groups</a> from the lovely <em>Prof. K. Conrad</em>\r\n        </p>\r\n      <p>Now, as for linear algebra, Wikipedia also uses the language of the <b>Splitting Lemma</b> for vector spaces:</p>\r\n      <blockquote>\r\n        <p>The theorem can also be phrased as saying that each short exact sequence of vector spaces splits. Explicitly, given that</p>\r\n\r\n        <p>\r\n        \\[0 \\rightarrow U \\rightarrow V \\overset{T}{\\rightarrow} R \\rightarrow 0\\]\r\n        </p>\r\n\r\n        <p>is a short exact sequence of vector spaces, then</p>\r\n\r\n        <p>\r\n        \\[U \\oplus R \\cong V\\]\r\n        </p>\r\n\r\n        <p>hence</p>\r\n\r\n        <p>\r\n        \\[\\dim(U) + \\dim(R) = \\dim(V).\\]\r\n        </p>\r\n\r\n        <p>Here \\(R\\) plays the role of \\(\\operatorname{Im} T\\) and \\(U\\) is \\(\\operatorname{Ker} T\\), i.e.</p>\r\n\r\n        <p>\r\n        \\[0 \\rightarrow \\operatorname{ker} T \\hookrightarrow V \\overset{T}{\\rightarrow} \\operatorname{im} T \\rightarrow 0\\]\r\n        </p>\r\n\r\n        <p>In the finite-dimensional case, this formulation is susceptible to a generalization: if</p>\r\n\r\n        <p>\r\n        \\[0 \\rightarrow V_1 \\rightarrow V_2 \\rightarrow \\cdots V_r \\rightarrow 0\\]\r\n        </p>\r\n\r\n        <p>is an exact sequence of finite-dimensional vector spaces, then</p>\r\n\r\n        <p>\r\n        \\[\\sum_{i=1}^{r} (-1)^{i} \\dim(V_i) = 0.\\]\r\n        </p>\r\n      </blockquote>\r\n      <p><a href=\"https://en.wikipedia.org/wiki/Rank%E2%80%93nullity_theorem#Reformulations_and_generalizations\">Source</a></p>\r\n      <p>This follows the reasoning that</p>\r\n      <blockquote>\r\n        <p>Let \\(f_0\\) be the first map in the sequence, \\(f_i\\) be the map from \\(V_i\\) to \\(V_{i+1}\\), etc.</p>\r\n\r\n        <p>By the Rank Nullity theorem, we have \\(\\dim V_i = \\dim\\ker f_i + \\dim \\operatorname{im} f_i.\\) Thus the left-hand side is</p>\r\n\r\n        <p>\\[\\sum_{i=1}^{r} (-1)^i \\dim\\ker f_i+\\sum_{i=1}^{r} (-1)^i \\dim\\operatorname{im} f_i.\\]</p>\r\n\r\n        <p>Now, by the defining property of an exact sequence, \\(\\operatorname{im} f_i = \\ker f_{i+1}.\\) Place that information into one of the sums, and the two sums then cancel out.</p>\r\n\r\n        <p>Note that in order for the series in question to converge, the sequence must be of the form in this answer, perhaps with extra \\(0\\)'s.</p>\r\n      </blockquote>\r\n      <p><a href=\"https://math.stackexchange.com/a/255387/1098426\">Source</a></p>\r\n    </div>\r\n  </div>\r\n  <br>\r\n  <h2 id=\"to-be-continued\">to be continued</h2>\r\n  <p>Now, we have a problem.</p>\r\n  <a title=\"ARAKI Satoru, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Rank-nullity.svg\">\r\n    <img style=\"width:50%;\" alt=\"Rank-nullity\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/8f/Rank-nullity.svg/256px-Rank-nullity.svg.png\">\r\n  </a>\r\n  <br>\r\n  <p>And, what in the world are these dotted lines? It appears to be addition for \\(\\operatorname{dim} V\\) but then \\(\\operatorname{dim} \\operatorname{ker} T\\) and \\(\\operatorname{dim} \\operatorname{im} T\\) don't make sense?</p>\r\n  <p>Well, I admittedly asked twitter, where @linguanumerate and @xl772 clarified.\r\n    <blockquote>\r\n        <p>It's just a kind of intuitive picture. The vertical line on the left is a \"picture\" of \\(V\\), with its dimension represented by the height of the line. The arrows show \\(V\\) getting mapped into \\(W\\), with \\(V/\\operatorname{ker} T\\) getting mapped to \\(\\operatorname{im} T\\) and \\(\\operatorname{ker} T\\) getting squashed to 0.</p>\r\n        <p>And I think the dotted lines are just to show which height corresponds to which quantity, I don’t think it’s adding.</p>\r\n    </blockquote>\r\n  </p>\r\n  <p>But, I think that is enough about rank for now.</p>",
      "date_published": "2023-06-30T17:00:00-07:00"
    },{
      "id": "https://ischmidls.github.io/posts/pages/linearsix/",
      "url": "https://ischmidls.github.io/posts/pages/linearsix/",
      "title": "six linear algebra theorems",
      "content_html": "<!doctype html>\r\n<html lang=\"en\" dir=\"ltr\">\r\n  <head>\r\n    <meta charset=\"utf-8\">\r\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\r\n    <link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css\" integrity=\"sha384-B0vP5xmATw1+K9KRQjQERJvTumQW0nPEzvF6L/Z6nronJ3oUOFUFpCjEUQouq2+l\" crossorigin=\"anonymous\">\r\n    <!--<link rel=\"stylesheet\" href=\"css/main.css\">-->\r\n    <title>Linear Algebra Theorems</title>\r\n    <script type=\"text/javascript\" id=\"MathJax-script\" async src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"></script>\r\n    <style>\r\n    .card-body {overflow-x: auto}\r\n    .MathJax_Display, .MJXc-display, .MathJax_SVG_Display, .mjx-container[display=\"true\"] {\r\n    overflow-x: auto;\r\n    overflow-y: hidden;\r\n    }\r\n    </style>\r\n  </head><body>\r\n    <div class=\"container\" id=\"bsr-wrapper\">\r\n      <!-- <div style=\"background-color: coral;\">Under construction 6/19/23-...</div> -->\r\n      <h1>Linear Algebra Theorems</h1>\r\n      <p>Izak, June 2023</p>\r\n      <!-- <p class=\"text-secondary\">Estimated reading time: ~25 min. <br> Approx. word count: ~4,100 words <br> Approx math terms: ~600 LaTeX blocks</p> -->\r\n      <p>The six central theorems of linear algebra come from Gilbert Strang's <strong>Introduction to Linear Algebra, 5th Ed</strong>, and I have provided an example for each. </p>\r\n      <p class=\"pl-5\"><em>Note: some HTML was started with ChatGPT, but LLM's cannot do math or even consistent <a href=\"https://www.latex-project.org/\">LaTeX</a>, so much of the math was checked via <a href=\"https://www.wolframalpha.com/\">Wolfram Alpha</a>, and proofs were checked via <a href=\"https://math.stackexchange.com/\">Math Stack Exchange</a>.</em> The CSS requires <a href=\"https://getbootstrap.com/\">Bootstrap</a>, and the LaTeX requires <a href=\"https://www.mathjax.org/\">MathJax</a>, so this page is best viewed with internet connection &#9825;. Finally, Prof. Strang uses \"nullspace\", where I use \"kernel\" &mdash; they're synonyms, but \"kernel\" is <a href=\"https://math.stackexchange.com/a/235353/1098426\">often used outside</a> of linear algebra too.</p>\r\n      <!-- <p>GPT's failure to do matrix operations, to properly parse LaTeX, etc. helped inspire me to do something an AI cannot replace yet. While Prof. Strang is more rigorous than (for example) 3b1b, I also wanted to bring my own exprience to the table. This latter part means that this page is a mix of somewhat cryptic categorical syntax, contrived examples, Math Stack Exchange insight, and the like. </p>\r\n      <p>It seems DeepMind researchers can train AlphaTensor to multiply matrices, but ChatGPT remains little more than an HTML generator.</p>\r\n      <p>\"Why the <em>needless</em> commentary?\" you, dear reader, may ask. Well, you might consider Axler's <em>Linear Algebra Done Right</em> or Hemmingway's <em>The Old Man and the Sea</em> if you prefer succinct prose. This page also has grown increasingly unwieldy with the explorations of each theorem, numbering hundreds of lines of HTML alone. A few lines of dialogue never hurt anyone &mdash; other than Shakespearean casts, & so on. Enjoy. </p> -->\r\n      <ul>\r\n        <li>\r\n          <a href=\"#dimension-theorem\">Dimension Theorem</a>\r\n        </li>\r\n        <li>\r\n          <a href=\"#counting-theorem\">Counting Theorem</a>\r\n        </li>\r\n        <li> \r\n          <a href=\"#rank-theorem\">Rank Theorem</a>\r\n        </li>\r\n        <li>\r\n          <a href=\"#fundamental-theorem\">Fundamental Theorem</a>\r\n        </li>\r\n        <li>\r\n          <a href=\"#singular-value-decomposition\">Singular Value Decomposition</a>\r\n        </li>\r\n        <li>\r\n          <a href=\"#spectral-theorem\">Spectral Theorem</a>\r\n        </li>\r\n        <li>\r\n          <a href=\"#nutshell\">Nutshell</a>\r\n        </li>\r\n      </ul>\r\n      <hr>\r\n      <h2 id=\"dimension-theorem\">Dimension Theorem</h2>\r\n      <p>All bases for a vector space have the same number of vectors.</p>\r\n      <p>Mathematically: \\( \\text{dim}(V) = \\text{dim}(W) \\) for any bases \\( V \\) and \\( W \\) of the vector space.</p>\r\n      <div>\r\n        <p>\r\n          <strong>Example:</strong>\r\n        </p>\r\n        <p>This is sort of a boring example, but the available proofs give an idea for how nuanced the mechanisms underlying this theorem are. See some discussion here: <a href=\"https://math.stackexchange.com/q/4595743/1098426\">Dimension Theorem discussion</a>\r\n        </p>\r\n        <p>Let's consider a vector space \\(V = \\mathbb{R}^2\\) over the field \\(F = \\mathbb{R}\\) (the set of real numbers). In this case, vectors in \\(V\\) are ordered pairs \\((x, y)\\) where \\(x\\) and \\(y\\) are real numbers.</p>\r\n        <p>Now, let's find two different bases for \\(V\\) and observe that they have the same number of vectors.</p>\r\n        <p>\r\n          <em>Basis 1:</em>\r\n        </p>\r\n        <p>We can choose the following two vectors as a basis for \\(V\\):</p>\r\n        <p>\\(\\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\)</p>\r\n        <p>\\(\\mathbf{v}_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\)</p>\r\n        <p>These vectors are linearly independent (meaning no non-trivial linear combination of them yields the zero vector) and span the entire vector space \\(V\\) &mdash; that is, \\(\\forall v \\in V \\ \\exists \\ \\lambda_1 , \\ \\lambda_2 \\in F \\ | \\ v= \\lambda_1 v_1 + \\lambda_2 v_2 \\)</p>\r\n        <p>\r\n          <em>Basis 2:</em>\r\n        </p>\r\n        <p>Alternatively, we can choose the following two vectors as another basis for \\(V\\):</p>\r\n        <p>\\(\\mathbf{u}_1 = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}\\)</p>\r\n        <p>\\(\\mathbf{u}_2 = \\begin{bmatrix} -1 \\\\ 3 \\end{bmatrix}\\)</p>\r\n        <p>Again, these vectors are linearly independent and span the entire vector space \\(V\\).</p>\r\n        <p>Both Basis 1 and Basis 2 consist of two vectors each. This example demonstrates that all bases (ok, at least <em>two</em> bases) for \\(V\\) have the same number of vectors, which in this case is 2. This property holds true for any vector space, indicating that the number of vectors in a basis is a fundamental characteristic of the vector space itself. </p>\r\n      </div>\r\n      <hr>\r\n      <h2 id=\"counting-theorem\">Counting Theorem</h2>\r\n      <p>Dimension of column space + dimension of kernel = number of columns.</p>\r\n      <p>Mathematically: \\( \\text{dim}(\\text{col}(A)) + \\text{dim}(\\text{ker}(A)) = \\text{cols}(A) \\)</p>\r\n      <div>\r\n        <p>\r\n          <strong>Example:</strong>\r\n        </p>\r\n        <p>Consider the matrix:</p>\r\n        <p>\\[ A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\\\ \\end{bmatrix} \\]</p>\r\n        <p>Let's calculate the dimension of the column space (step 1) and the dimension of the kernel (step 2) of \\(A\\), and verify the theorem.</p>\r\n        <p>\r\n          <em>Solution:</em>\r\n        </p>\r\n        <p>STEP 1: To find the column space of \\(A\\), we reduce \\(A\\) to echelon form:</p>\r\n        <p>\\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\\\ \\end{bmatrix} \\xrightarrow{\\text{Row operations}} \\begin{bmatrix} 1 & 0 & -1 \\\\ 0 & 1 & 2 \\\\ 0 & 0 & 0 \\\\ \\end{bmatrix} \\]</p>\r\n        <div class=\"card\">\r\n          <div class=\"card-body\">\r\n            <p> Do row reduction: \\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\\\ \\end{bmatrix} \\] </p>\r\n            <p>Subtract a multiple of one row from another. <br> Subtract \\(4 \\times\\) (row 1) from row 2: <br> \\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & -3 & -6 \\\\ 7 & 8 & 9 \\\\ \\end{bmatrix} \\] </p>\r\n            <p>Subtract a multiple of one row from another. <br> Subtract \\(7 \\times\\) (row 1) from row 3: <br> \\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & -3 & -6 \\\\ 0 & -6 & -12 \\\\ \\end{bmatrix} \\] </p>\r\n            <p>Swap two rows. <br> Swap row 2 with row 3: <br> \\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & -6 & -12 \\\\ 0 & -3 & -6 \\\\ \\end{bmatrix} \\] </p>\r\n            <p>Subtract a multiple of one row from another. <br> Subtract \\(\\frac{1}{2} \\times\\) (row 2) from row 3: <br> \\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & -6 & -12 \\\\ 0 & 0 & 0 \\\\ \\end{bmatrix} \\] </p>\r\n            <p>Divide row 2 by a scalar. <br> Divide row 2 by -6: <br> \\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 1 & 2 \\\\ 0 & 0 & 0 \\\\ \\end{bmatrix} \\] </p>\r\n            <p>Subtract a multiple of one row from another. <br> Subtract \\(2 \\times\\) (row 2) from row 1: <br> \\[ \\begin{bmatrix} 1 & 0 & -1 \\\\ 0 & 1 & 2 \\\\ 0 & 0 & 0 \\\\ \\end{bmatrix} \\] </p>\r\n            <p>Verify matrix is reduced. <br> This matrix is now in reduced row echelon form. <br> All nonzero rows are above rows of all zeros: <br> \\[ \\begin{bmatrix} 1 & 0 & -1 \\\\ 0 & 1 & 2 \\\\ 0 & 0 & 0 \\\\ \\end{bmatrix} \\] </p>\r\n            <p>Verify pivots and their positions. <br> Each pivot is 1 and is strictly to the right of every pivot above it: <br> \\[ \\begin{bmatrix} 1 & 0 & -1 \\\\ 0 & 1 & 2 \\\\ 0 & 0 & 0 \\\\ \\end{bmatrix} \\] </p>\r\n            <sub>Checked with Wolfram &#9825;</sub>\r\n          </div>\r\n        </div>\r\n        <br>\r\n        <p>The pivot columns are the first two columns, and they form a basis for the column space of \\(A\\). So, the dimension of the column space is \\(2\\).</p>\r\n        <p>STEP 2: Now, let's find the kernel of \\(A\\) by solving the homogeneous equation \\(A\\mathbf{x} = \\mathbf{0}\\):</p>\r\n        <p>\\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\\\ \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ \\end{bmatrix} \\]</p>\r\n        <div class=\"card\">\r\n          <div class=\"card-body\">\r\n            <p>The kernel of a matrix \\(M\\) is the set of solutions \\(v\\) to the homogeneous equation \\(M \\cdot v = 0\\). <br> The kernel of matrix \\(M = \\begin{bmatrix} 1 & 0 & -1 \\\\ 0 & 1 & 2 \\\\ 0 & 0 & 0 \\end{bmatrix}\\) is the set of all vectors \\(v = (x_1, x_2, x_3)\\) such that \\(M \\cdot v = 0\\): <br> \\(\\begin{bmatrix} 1 & 0 & -1 \\\\ 0 & 1 & 2 \\\\ 0 & 0 & 0 \\end{bmatrix} \\cdot \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\\) </p>\r\n            <p>Identify free variables. <br> Free variables in the kernel \\((x_1, x_2, x_3)\\) correspond to the columns in \\(\\begin{bmatrix} 1 & 0 & -1 \\\\ 0 & 1 & 2 \\\\ 0 & 0 & 0 \\end{bmatrix}\\) which have no pivot. <br> Column 3 is the only column with no pivot, so we may take \\(x_3\\) to be the only free variable. </p>\r\n            <p>Perform matrix multiplication. <br> Multiply out the reduced matrix \\(\\begin{bmatrix} 1 & 0 & -1 \\\\ 0 & 1 & 2 \\\\ 0 & 0 & 0 \\end{bmatrix}\\) with the proposed solution vector \\((x_1, x_2, x_3)\\): <br> \\(\\begin{bmatrix} 1 & 0 & -1 \\\\ 0 & 1 & 2 \\\\ 0 & 0 & 0 \\end{bmatrix} \\cdot \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} = \\begin{bmatrix} x_1 - x_3 \\\\ x_2 + 2x_3 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\\) </p>\r\n            <p>Convert to a system and solve in terms of the free variables. <br> Solve the equations \\(x_1 - x_3 = 0\\), \\(x_2 + 2x_3 = 0\\), and \\(0 = 0\\) for \\(x_1\\) and \\(x_2\\): <br> \\(\\{x_1 = x_3, x_2 = -2x_3, 0 = 0\\) for \\(x_1\\) and \\(x_2 \\}\\) </p>\r\n            <p>Replace the pivot variables with free variable expressions. <br> Rewrite \\(v\\) in terms of the free variable \\(x_3\\), and assign it an arbitrary real value of \\(x\\): <br> \\(v = (x_1, x_2, x_3) = (x_3, -2x_3, x_3) = (x, -2x, x)\\) for \\(x \\in \\mathbb{R}\\) </p>\r\n            <p> Rewrite the solution vector \\(v = (x, -2x, x)\\) in set notation: <br> Answer: \\(\\{(x, -2x, x) : x \\in \\mathbb{R}\\}\\) </p>\r\n            <sub>Checked with Wolfram &#9825;</sub>\r\n          </div>\r\n        </div>\r\n        <br>\r\n        <p>Thus, the <em>nullity</em> &mdash;or dimension of the kernel&mdash; is one, \\( \\text{dim}(\\text{ker}(A)) = 1 \\) </p>\r\n        <p>Now, by the theorem, the dimension of the column space plus the dimension of the kernel should be equal to the number of columns, \\( \\text{dim}(\\text{col}(A)) + \\text{dim}(\\text{ker}(A)) = \\text{cols}(A) \\)</p>\r\n        <p>Here, \\( \\text{dim}(\\text{col}(A)) = 2 \\) and \\( \\text{dim}(\\text{ker}(A)) = 1 \\) so \\( \\text{cols}(A) = 2 + 1 = 3 \\). The number of columns in \\(A\\) is also \\(3\\).</p>\r\n      </div>\r\n      <hr>\r\n      <h2 id=\"rank-theorem\">Rank Theorem</h2>\r\n      <p>Dimension of column space = dimension of row space.</p>\r\n      <p>Mathematically: \\( \\text{dim}(\\text{col}(A)) = \\text{dim}(\\text{row}(A)) \\)</p>\r\n      <div>\r\n        <p>\r\n          <strong>Example:</strong>\r\n        </p>\r\n        <p>Consider the matrix:</p>\r\n        <p>\\[ A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\\\ \\end{bmatrix} \\]</p>\r\n        <p>Let's calculate the dimensions of the column space and the row space of \\(A\\), and verify the theorem.</p>\r\n        <p>\r\n          <em>Solution:</em>\r\n        </p>\r\n        <p>First, consider the column space and then, second, consider the row space.</p>\r\n        <p>For both the column and row spaces, we reduce \\(A\\) to echelon form exactly as in step 1 of the example for the <em>Counting Theorem</em>: </p>\r\n        <p>\\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\\\ \\end{bmatrix} \\xrightarrow{\\text{Row operations}} \\begin{bmatrix} 1 & 0 & -1 \\\\ 0 & 1 & 2 \\\\ 0 & 0 & 0 \\\\ \\end{bmatrix} \\]</p>\r\n        <!-- Rank Theorem steps -->\r\n        <p>For the column space: the pivot columns of this matrix \\(A\\) are the first column and the second column &mdash; where \\(1\\) is the only nonzero element of the pivot columns in row-reduced echelon form \\(\\text{rref}(A)\\).</p>\r\n        <p>Thus, the second and first columns of the orignial matrix \\( \\begin{bmatrix} 1 & 4 & 7 \\\\ \\end{bmatrix} \\) and \\( \\begin{bmatrix} 2 & 5 & 8 \\\\ \\end{bmatrix} \\) form a basis for the column space of \\(A\\). So, the dimension of the column space is \\( \\text{dim}(\\text{col}(A)) = 2 \\).</p>\r\n        <p>The row space comes from the non-zero rows of the row-reduced echelon matrix. The row space of \\(A\\) is spanned by \\( \\begin{bmatrix} 1 & 0 & -1 \\\\ \\end{bmatrix} \\) and \\( \\begin{bmatrix} 0 & 1 & 2 \\\\ \\end{bmatrix} \\), so they form a basis for the row space of \\(A\\), and the dimension of the row space is \\( \\text{dim}(\\text{row}(A)) = 2 \\).</p>\r\n        <p>We see \\( \\text{dim}(\\text{col}(A)) = \\text{dim}(\\text{row}(A)) = 2 \\). <br>\r\n          <b>Q.E.D.</b>\r\n        </p>\r\n        <p>That ends the example, but some rigorous (albiet a tad daunting) proofs are here: <a href=\"https://math.stackexchange.com/q/1900437/1098426\">Rank Theorem Proofs</a>\r\n        </p>\r\n      </div>\r\n      <hr>\r\n      <!-- FUNDAMENTAL THEOREM -->\r\n      <h2 id=\"fundamental-theorem\">Fundamental Theorem</h2>\r\n      <p>The row space and kernel of \\( A \\) are orthogonal complements in \\( \\mathbb{R}^n \\).</p>\r\n      <p>Mathematically: \\( \\text{row}(A) \\perp \\text{ker}(A) \\) in \\( \\mathbb{R}^n \\)</p>\r\n      <p>\r\n        <strong>Example:</strong>\r\n      </p>\r\n      <p>Consider the matrix:</p>\r\n      <p>\\[ A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\\\ \\end{bmatrix} \\]</p>\r\n      <p>Let's calculate the row space (step 1) and the kernel (step 2) of \\(A\\) and verify that they are orthogonal complements in \\(\\mathbb{R}^n\\).</p>\r\n      <p>\r\n        <em>Solution:</em>\r\n      </p>\r\n      <div>\r\n        <p>This one requires a bit more thinking outside of the row reduction algorithm</p>\r\n        <p> Recall from <em>The Counting Theorem</em>, the kernel has dimension equal to the difference between the number of columns and the dimension of the row space, \\(\\text{col}(A) - \\text{dim}(\\text{row}(A))\\). It follows that the orthogonal complement of the kernel has dimension \\(\\text{col}(A) - (\\text{col}(A) - \\text{dim}(\\text{row}(A)))=\\text{dim}(\\text{row}(A))\\). Now, the row space is an \\(r\\) dimensional subspace of the orthogonal complement of the kernel, which in turn has dimension \\(r\\). The only \\(r\\) dimensional subspace of an \\(r\\)-dimensional space is the entirety of the space itself, \\( A \\subseteq B, \\ \\text{dim}(A)=\\text{dim}(B) \\vdash B \\subseteq A, \\ A=B\\). So, the row-space is not only a subspace of the orthogonal complement but comprises the entirety of the orthogonal complement. </p>\r\n      </div>\r\n      <p>Also Recall from <em>The Counting Theorem</em>, the kernel is \\(\\{(x, -2x, x) : x \\in \\mathbb{R}\\}\\). </p>\r\n      <p>Similarly, from the row reduction of the matrix, we know that the basis for the row space is \\( \\{ \\begin{bmatrix} 1 & 0 & -1 \\end{bmatrix}, \\ \\begin{bmatrix} 0 & 1 & 2 \\end{bmatrix} \\} \\) </p>\r\n      <p>Now, orthogonal vectors have an inner product equal to zero \\( x^T y = y^T x = 0\\). Spaces are orthogonal when every vector in one is orthogonal to every vector in the other.\r\n      <p>How to check that \\( \\forall n \\in \\{(x, -2x, x) : x \\in \\mathbb{R}\\} \\) and \\( \\forall r \\in \\) the <em>row space</em> which is in some ways more abstract: </p>\r\n      <p>\r\n        <em>For a vector space \\(V\\), a family in \\(V\\) consists of a set \\(I\\) together with a function \\(e: I \\rightarrow V\\). A basis of \\(V\\) is a family \\((I, e)\\) in \\(V\\) such that for all \\(x \\in V\\) there exists a unique finitely-supported function \\(a: I \\rightarrow \\mathbb{R}\\) satisfying \\(x = \\sum_{i \\in I} a_i e_i \\ \\) as well as conditions for well ordering</em> (See <a href=\"https://math.stackexchange.com/a/1898250/1098426\">this definition's source</a> for further context on well ordering and matrices).\r\n      </p>\r\n      <p>And that is not even considering <em>order</em>, which is an essential part of elimination on matrices. All the same, the definition can be even more intuitive if simply considered \\(x = \\sum a e\\) to mean all the linear combinations of \\( \\begin{bmatrix} 1 & 0 & -1 \\end{bmatrix} \\) and \\( \\begin{bmatrix} 0 & 1 & 2 \\end{bmatrix}\\). </p>\r\n      <p>Even more intuitively, the kernel can be seen as a line or a one dimensional subspace of \\( \\mathbb{R}^3 \\), the row space a plane or two dimensional subspace of \\( \\mathbb{R}^3 \\).</p>\r\n      <p>Thus, the normal vector to \\(\\text{row}(A)\\) is \\( \\begin{bmatrix} 1 & 0 & -1 \\end{bmatrix} \\times \\begin{bmatrix} 0 & 1 & 2 \\end{bmatrix} \\)</p>\r\n      <div class=\"card\">\r\n        <div class=\"card-body\">\r\n          <p>Compute the following cross product:</p>\r\n          <p>\\((1, 0, -1) \\times (0, 1, 2)\\)</p>\r\n          <p>Create a matrix out of the vectors \\((1, 0, -1)\\) and \\((0, 1, 2)\\) along with the unit vectors \\(\\hat{i}\\), \\(\\hat{j}\\), and \\(\\hat{k}\\).</p>\r\n          <p>Construct a matrix where the first row contains unit vectors \\(\\hat{i}\\), \\(\\hat{j}\\), and \\(\\hat{k}\\); and the second and third rows are made of vectors \\((1, 0, -1)\\) and \\((0, 1, 2)\\):</p>\r\n          <p>\\[ \\begin{bmatrix} \\hat{i} & \\hat{j} & \\hat{k} \\\\ 1 & 0 & -1 \\\\ 0 & 1 & 2 \\\\ \\end{bmatrix} \\]</p>\r\n          <p>The cross product of the vectors \\((1, 0, -1)\\) and \\((0, 1, 2)\\) is the determinant of the matrix:</p>\r\n          <p>\\[ \\begin{vmatrix} \\hat{i} & \\hat{j} & \\hat{k} \\\\ 1 & 0 & -1 \\\\ 0 & 1 & 2 \\\\ \\end{vmatrix} \\]</p>\r\n          <p>Take the determinant of this matrix:</p>\r\n          <p>\\(\\begin{vmatrix} \\hat{i} & \\hat{j} & \\hat{k} \\\\ 1 & 0 & -1 \\\\ 0 & 1 & 2 \\end{vmatrix}\\)</p>\r\n          <p>Find an optimal row or column to use for Laplace's expansion.</p>\r\n          <p>Expand with respect to row 1:</p>\r\n          <p>The determinant of the matrix \\(\\begin{bmatrix} a_{1,1} & a_{1,2} & a_{1,3} \\\\ a_{2,1} & a_{2,2} & a_{2,3} \\\\ a_{3,1} & a_{3,2} & a_{3,3} \\end{bmatrix}\\) is given by \\(\\sum_{j=1}^{3}(-1)^{1+j}a_{1,j}M_{1,j}\\) where \\(M_{i,j}\\) is the determinant of the matrix obtained by removing row \\(i\\) and column \\(j\\).</p>\r\n          <p>The determinant of the matrix \\(\\begin{bmatrix} \\hat{i} & \\hat{j} & \\hat{k} \\\\ 1 & 0 & -1 \\\\ 0 & 1 & 2 \\end{bmatrix}\\) is given by \\(\\hat{i}\\begin{vmatrix} 0 & -1 \\\\ 1 & 2 \\end{vmatrix} + (-\\hat{j})\\begin{vmatrix} 1 & -1 \\\\ 0 & 2 \\end{vmatrix} + \\hat{k}\\begin{vmatrix} 1 & 0 \\\\ 0 & 1 \\end{vmatrix}\\):</p>\r\n          <p>\\(=\\hat{i}\\begin{vmatrix} 0 & -1 \\\\ 1 & 2 \\end{vmatrix} + (-\\hat{j})\\begin{vmatrix} 1 & -1 \\\\ 0 & 2 \\end{vmatrix} + \\hat{k}\\begin{vmatrix} 1 & 0 \\\\ 0 & 1 \\end{vmatrix}\\)</p>\r\n          <p>The determinant of the matrix \\(\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\\) is given by \\(ad - bc\\).</p>\r\n          <p>\\(\\hat{i}\\begin{vmatrix} 0 & -1 \\\\ 1 & 2 \\end{vmatrix} = \\hat{i}\\)</p>\r\n          <p>\\(=(-\\hat{j})\\begin{vmatrix} 1 & -1 \\\\ 0 & 2 \\end{vmatrix} + \\hat{k}\\begin{vmatrix} 1 & 0 \\\\ 0 & 1 \\end{vmatrix}\\)</p>\r\n          <p>Compute the determinant of the matrix \\(\\begin{bmatrix} 1 & -1 \\\\ 0 & 2 \\end{bmatrix}\\) and multiply the result by \\(-\\hat{j}\\).</p>\r\n          <p>\\((-\\hat{j})\\begin{vmatrix} 1 & -1 \\\\ 0 & 2 \\end{vmatrix} = -2\\hat{j}\\)</p>\r\n          <p>\\(=\\hat{i} - 2\\hat{j} + \\hat{k}\\begin{vmatrix} 1 & 0 \\\\ 0 & 1 \\end{vmatrix}\\)</p>\r\n          <p>Compute the determinant of the matrix \\(\\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\\) and multiply the result by \\(\\hat{k}\\).</p>\r\n          <p>\\(\\hat{k}\\begin{vmatrix} 1 & 0 \\\\ 0 & 1 \\end{vmatrix} = \\hat{k}\\)</p>\r\n          <p>\\(=\\hat{i} - 2\\hat{j} + \\hat{k}\\)</p>\r\n          <p>Collect the coefficients of \\(\\hat{i}\\), \\(\\hat{j}\\), and \\(\\hat{k}\\) into a vector ordered as \\((\\hat{i}, \\hat{j}, \\hat{k})\\).</p>\r\n          <p>\\(\\hat{i} - 2\\hat{j} + \\hat{k} = (1, -2, 1)\\)</p>\r\n          <sub>Checked with Wolfram &#9825;</sub>\r\n        </div>\r\n      </div>\r\n      <br>\r\n      <p>We see the normal of \\( \\text{row}(A)\\) is \\( \\hat{s} = \\begin{bmatrix} 1 & -2 & 1 \\end{bmatrix} \\). Now, it is simple to see that \\( \\text{ker}(A) \\) has direction vector \\( \\hat{s} = \\begin{bmatrix} 1 & -2 & 1 \\end{bmatrix} \\) (i.e. factor out \\(x\\)).</p>\r\n      <p>Now, proving \\( \\text{row}(A) \\perp \\text{ker}(A) \\) in \\( \\mathbb{R}^n \\) is proving that the normal vector \\( \\hat{n} \\) of \\( \\text{row}(A) \\) is parallel to the direction vector \\( \\hat{s} \\) of \\( \\text{ker}(A) \\).</p>\r\n      <p>This parallelism, again, is true if \\( \\hat{n} \\times \\hat{s} = 0 \\). But, we do not need to go this far because out results have yielded the same vector.</p>\r\n      <p>That is, \\( ( \\hat{n} = \\hat{s} ) \\Rightarrow ( \\hat{n} \\times \\hat{s} = 0 ) \\), and \\( ( \\hat{n} \\times \\hat{s} = 0 ) \\Rightarrow (\\hat{n} \\parallel \\hat{s}) \\), so finally, \\( \\hat{n} \\parallel \\hat{s} \\Rightarrow (\\text{row}(A) \\perp \\text{ker}(A)) \\).</p>\r\n      <hr>\r\n      <!-- SINGULAR VALUE DECOMPOSITION-->\r\n      <h2 id=\"singular-value-decomposition\">Singular Value Decomposition</h2>\r\n      <p>This is where the computations become quite intense.</p>\r\n      <p>There are orthonormal bases (\\( v \\)'s and \\( u \\)'s) for the row and column spaces so that \\( Av_i = \\sigma_iu_i \\).</p>\r\n      <p>Mathematically: \\( A = US V^T \\) where \\( U \\) and \\( V \\) are orthonormal matrices, and \\( S \\) is a diagonal matrix of singular values.</p>\r\n      <p>A concise explanation (adapted from an <a href=\"https://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm\">MIT bio engineering tutorial</a>):</p>\r\n      <div class=\"pl-5\">\r\n      <p>\r\n        Singular value decomposition takes a rectangular matrix(defined as \\( A \\), where \\( A \\)\r\n        is an \\( n \\times p \\) matrix) with \\( n \\) rows and \\( p \\) columns.\r\n      </p>\r\n      <p>\r\n        \\[ A = U \\cdot S\\cdot V^T \\]\r\n      </p>\r\n      <p>\r\n        Where:\r\n      </p>\r\n      <p>\r\n        \\[ U^TU = I_{n \\times n} \\]\r\n        \\[ V^TV = I_{p \\times p} \\Rightarrow U \\perp V\\]\r\n      </p>\r\n      <p>\r\n        Where the columns of \\( U \\) are the left singular vectors; \\(S\\) (the same dimensions\r\n        as \\( A \\)) has singular values and is diagonal; and \\( V^T \\) has rows that are the right singular\r\n        vectors.\r\n      </p>\r\n      <p>\r\n        Calculating the SVD consists of finding the eigenvalues and eigenvectors of \\( A^TA \\) and \\( AA^T \\). The\r\n        eigenvectors of \\( AA^T \\) make up the columns of \\( V \\), the eigenvectors of \\( A^TA \\) make up the columns of\r\n        \\( U \\). Also, the singular values in \\(S\\) are the square roots of eigenvalues from \\( AA^T \\) or \\( A^TA \\). The\r\n        singular values are the diagonal entries of the \\(S\\) matrix and are arranged in descending order. The singular\r\n        values are always real numbers. If the matrix \\( A \\) is a real matrix, then \\( U \\) and \\( V \\) are also real.\r\n      </p>\r\n    </div>\r\n      <div>\r\n        <p>\r\n          <strong>Example:</strong>\r\n        </p>\r\n        <p>While I would love to consider this matrix:</p>\r\n        <p>\\[ A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\\\ \\end{bmatrix} \\]</p>\r\n        <p>This yields a horrifying SVD that requires the <code>\\tiny</code> command from LaTeX and the <code>overflow-x: auto</code> command for CSS:</p>\r\n        <div class=\"card\">\r\n          <div class=\"card-body\">\r\n            <p> Find \\(M = U.Σ.V^†\\) where...\r\n              \\[M = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\\\ \\end{bmatrix}\\]\r\n            </p>\r\n            <p> Apologies, I had to shrink this to fit it on the page: \\( U =\\tiny{ \\begin{bmatrix} \\frac{3 - \\frac{2(-1223 - 13\\sqrt{8881})}{3(477 + 5\\sqrt{8881})} - \\frac{(-1015 - 11\\sqrt{8881})}{3(477 + 5\\sqrt{8881})}}{\\sqrt{(9 - \\frac{8(-1223 - 13\\sqrt{8881})}{3(477 + 5\\sqrt{8881})} - \\frac{7(-1015 - 11\\sqrt{8881})}{3(477 + 5\\sqrt{8881})})^2 + (6 - \\frac{5(-1223 - 13\\sqrt{8881})}{3(477 + 5\\sqrt{8881})} - \\frac{4(-1015 - 11\\sqrt{8881})}{3(477 + 5\\sqrt{8881})})^2 + (3 - \\frac{2(-1223 - 13\\sqrt{8881})}{3(477 + 5\\sqrt{8881})} - \\frac{(-1015 - 11\\sqrt{8881})}{3(477 + 5\\sqrt{8881})})^2}} & \\frac{3 - \\frac{2(1223 - 13\\sqrt{8881})}{3(5\\sqrt{8881} - 477)} - \\frac{(1015 - 11\\sqrt{8881})}{3(5\\sqrt{8881} - 477)}}{\\sqrt{(9 - \\frac{8(1223 - 13\\sqrt{8881})}{3(5\\sqrt{8881} - 477)} - \\frac{7(1015 - 11\\sqrt{8881})}{3(5\\sqrt{8881} - 477)})^2 + (6 - \\frac{5(1223 - 13\\sqrt{8881})}{3(5\\sqrt{8881} - 477)} - \\frac{4(1015 - 11\\sqrt{8881})}{3(5\\sqrt{8881} - 477)})^2 + (3 - \\frac{2(1223 - 13\\sqrt{8881})}{3(5\\sqrt{8881} - 477)} - \\frac{(1015 - 11\\sqrt{8881})}{3(5\\sqrt{8881} - 477)})^2}} & \\frac{1}{\\sqrt{6}} \\\\ \\\\ \\frac{6 - \\frac{5(-1223 - 13\\sqrt{8881})}{3(477 + 5\\sqrt{8881})} - \\frac{4(-1015 - 11\\sqrt{8881})}{3(477 + 5\\sqrt{8881})}}{\\sqrt{(9 - \\frac{8(-1223 - 13\\sqrt{8881})}{3(477 + 5\\sqrt{8881})} - \\frac{7(-1015 - 11\\sqrt{8881})}{3(477 + 5\\sqrt{8881})})^2 + (6 - \\frac{5(-1223 - 13\\sqrt{8881})}{3(477 + 5\\sqrt{8881})} - \\frac{4(-1015 - 11\\sqrt{8881})}{3(477 + 5\\sqrt{8881})})^2 + (3 - \\frac{2(-1223 - 13\\sqrt{8881})}{3(477 + 5\\sqrt{8881})} - \\frac{(-1015 - 11\\sqrt{8881})}{3(477 + 5\\sqrt{8881})})^2}} & \\frac{6 - \\frac{5(1223 - 13\\sqrt{8881})}{3(5\\sqrt{8881} - 477)} - \\frac{4(1015 - 11\\sqrt{8881})}{3(5\\sqrt{8881} - 477)}}{\\sqrt{(9 - \\frac{8(1223 - 13\\sqrt{8881})}{3(5\\sqrt{8881} - 477)} - \\frac{7(1015 - 11\\sqrt{8881})}{3(5\\sqrt{8881} - 477)})^2 + (6 - \\frac{5(1223 - 13\\sqrt{8881})}{3(5\\sqrt{8881} - 477)} - \\frac{4(1015 - 11\\sqrt{8881})}{3(5\\sqrt{8881} - 477)})^2 + (3 - \\frac{2(1223 - 13\\sqrt{8881})}{3(5\\sqrt{8881} - 477)} - \\frac{(1015 - 11\\sqrt{8881})}{3(5\\sqrt{8881} - 477)})^2}} & \\frac{1}{\\sqrt{6}} \\\\ \\\\ \\frac{9 - \\frac{8(-1223 - 13\\sqrt{8881})}{3(477 + 5\\sqrt{8881})} - \\frac{7(-1015 - 11\\sqrt{8881})}{3(477 + 5\\sqrt{8881})}}{\\sqrt{(9 - \\frac{8(-1223 - 13\\sqrt{8881})}{3(477 + 5\\sqrt{8881})} - \\frac{7(-1015 - 11\\sqrt{8881})}{3(477 + 5\\sqrt{8881})})^2 + (6 - \\frac{5(-1223 - 13\\sqrt{8881})}{3(477 + 5\\sqrt{8881})} - \\frac{4(-1015 - 11\\sqrt{8881})}{3(477 + 5\\sqrt{8881})})^2 + (3 - \\frac{2(-1223 - 13\\sqrt{8881})}{3(477 + 5\\sqrt{8881})} - \\frac{(-1015 - 11\\sqrt{8881})}{3(477 + 5\\sqrt{8881})})^2}} & \\frac{9 - \\frac{8(1223 - 13\\sqrt{8881})}{3(5\\sqrt{8881} - 477)} - \\frac{7(1015 - 11\\sqrt{8881})}{3(5\\sqrt{8881} - 477)}}{\\sqrt{(9 - \\frac{8(1223 - 13\\sqrt{8881})}{3(5\\sqrt{8881} - 477)} - \\frac{7(1015 - 11\\sqrt{8881})}{3(5\\sqrt{8881} - 477)})^2 + (6 - \\frac{5(1223 - 13\\sqrt{8881})}{3(5\\sqrt{8881} - 477)} - \\frac{4(1015 - 11\\sqrt{8881})}{3(5\\sqrt{8881} - 477)})^2 + (3 - \\frac{2(1223 - 13\\sqrt{8881})}{3(5\\sqrt{8881} - 477)} - \\frac{(1015 - 11\\sqrt{8881})}{3(5\\sqrt{8881} - 477)})^2}} & \\frac{1}{\\sqrt{6}} \\\\ \\end{bmatrix}} \\) </p>\r\n            <p> \\(Σ = \\begin{bmatrix} \\sqrt{\\frac{3}{2}(95 + \\sqrt{8881})} & 0 & 0 \\\\ 0 & \\sqrt{\\frac{3}{2}(95 - \\sqrt{8881})} & 0 \\\\ 0 & 0 & 0 \\\\ \\end{bmatrix} \\) \\(V = \\begin{bmatrix} -\\frac{-1015 - 11\\sqrt{8881}}{3(477 + 5\\sqrt{8881})\\sqrt{1 + \\frac{(-1223 - 13\\sqrt{8881})^2}{9(477 + 5\\sqrt{8881})^2} + \\frac{(-1015 - 11\\sqrt{8881})^2}{9(477 + 5\\sqrt{8881})^2}}} & -\\frac{1015 - 11\\sqrt{8881}}{3(5\\sqrt{8881} - 477)\\sqrt{1 + \\frac{(1223 - 13\\sqrt{8881})^2}{9(5\\sqrt{8881} - 477)^2} + \\frac{(1015 - 11\\sqrt{8881})^2}{9(5\\sqrt{8881} - 477)^2}}} & \\frac{1}{\\sqrt{6}} \\\\ -\\frac{-1223 - 13\\sqrt{8881}}{3(477 + 5\\sqrt{8881})\\sqrt{1 + \\frac{(-1223 - 13\\sqrt{8881})^2}{9(477 + 5\\sqrt{8881})^2} + \\frac{(-1015 - 11\\sqrt{8881})^2}{9(477 + 5\\sqrt{8881})^2}}} & -\\frac{1223 - 13\\sqrt{8881}}{3(5\\sqrt{8881} - 477)\\sqrt{1 + \\frac{(1223 - 13\\sqrt{8881})^2}{9(5\\sqrt{8881} - 477)^2} + \\frac{(1015 - 11\\sqrt{8881})^2}{9(5\\sqrt{8881} - 477)^2}}} & -\\sqrt{\\frac{2}{3}} \\\\ \\frac{1}{\\sqrt{1 + \\frac{(-1223 - 13\\sqrt{8881})^2}{9(477 + 5\\sqrt{8881})^2} + \\frac{(-1015 - 11\\sqrt{8881})^2}{9(477 + 5\\sqrt{8881})^2}}} & \\frac{1}{\\sqrt{1 + \\frac{(1223 - 13\\sqrt{8881})^2}{9(5\\sqrt{8881} - 477)^2} + \\frac{(1015 - 11\\sqrt{8881})^2}{9(5\\sqrt{8881} - 477)^2}}} & 0 \\\\ \\end{bmatrix}^†\\) </p>\r\n            <p>Note: † denotes the conjugate transpose</p>\r\n            <sub>I also found this one with Wolfram &#9825;</sub>\r\n          </div>\r\n        </div>\r\n        <br>\r\n        <div>\r\n          <p>INSTEAD, below I transcribed and explained <a href=\"https://www.d.umn.edu/~mhampton/m4326svd_example.pdf\">a better example</a> from <b>Prof. Marshall Hampton</b> at University of Minnesota Duluth (I merely trascribed & explained <em>Prof. Marshall Hampton's</em> idea &mdash; no plagiarism intended)! </p>\r\n          <p> Find the SVD of \\(A, U S V^{T}\\), where \\[A=\\begin{bmatrix}3 & 2 & 2 \\\\ 2 & 3 & -2\\end{bmatrix}\\] </p>\r\n          <p> First, we compute the singular values \\(\\sigma_{i}\\) by finding the eigenvalues of \\(A A^{T}\\). </p>\r\n          <p> \\[ A A^{T}=\\begin{bmatrix}17 & 8 \\\\ 8 & 17\\end{bmatrix} \\] </p>\r\n          <div class=\"card\">\r\n            <div class=\"card-body\">\r\n              <p> Multiply the following matrices: </p>\r\n              <p> \\[ \\begin{bmatrix} 3 & 2 & 2 \\\\ 2 & 3 & -2 \\end{bmatrix} \\cdot \\begin{bmatrix} 3 & 2 \\\\ 2 & 3 \\\\ 2 & -2 \\end{bmatrix} \\] </p>\r\n              <p> Determine the dimension of the product. The dimensions of the first matrix are \\(2 \\times 3\\) and the dimensions of the second matrix are \\(3 \\times 2\\). This means the dimensions of the product are \\(2 \\times 2\\): </p>\r\n              <p> \\[ \\begin{bmatrix} 3 & 2 & 2 \\\\ 2 & 3 & -2 \\end{bmatrix} \\cdot \\begin{bmatrix} 3 & 2 \\\\ 2 & 3 \\\\ 2 & -2 \\end{bmatrix} = \\begin{bmatrix} \\_ & \\_ \\\\ \\_ & \\_ \\end{bmatrix} \\] </p>\r\n              <p> Find the entry in the 1\\(^{\\text{st}}\\) row and 1\\(^{\\text{st}}\\) column of the product matrix. First look at the 1\\(^{\\text{st}}\\) row of the first matrix and the 1\\(^{\\text{st}}\\) column of the second matrix. </p>\r\n              <p> Highlight the 1\\(^{\\text{st}}\\) row and the 1\\(^{\\text{st}}\\) column: </p>\r\n              <p> \\[ \\begin{bmatrix} 3 & 2 & 2 \\\\ 2 & 3 & -2 \\end{bmatrix} \\cdot \\begin{bmatrix} 3 & 2 \\\\ 2 & 3 \\\\ 2 & -2 \\end{bmatrix} = \\begin{bmatrix} \\_ & \\_ \\\\ \\_ & \\_ \\end{bmatrix} \\] </p>\r\n              <p> Multiply corresponding components of the highlighted row and highlighted column, then add. </p>\r\n              <p> Multiply corresponding components and add: \\(3 \\cdot 3 + 2 \\cdot 2 + 2 \\cdot 2 = 17\\). </p>\r\n              <p> Place this number into the 1\\(^{\\text{st}}\\) row and 1\\(^{\\text{st}}\\) column of the product: </p>\r\n              <p> \\[ \\begin{bmatrix} 3 & 2 & 2 \\\\ 2 & 3 & -2 \\end{bmatrix} \\cdot \\begin{bmatrix} 3 & 2 \\\\ 2 & 3 \\\\ 2 & -2 \\end{bmatrix} = \\begin{bmatrix} 17 & \\_ \\\\ \\_ & \\_ \\end{bmatrix} \\] </p>\r\n              <p> Find the entry in the 1\\(^{\\text{st}}\\) row and 2\\(^{\\text{nd}}\\) column of the product matrix. First look at the 1\\(^{\\text{st}}\\) row of the first matrix and the 2\\(^{\\text{nd}}\\) column of the second matrix. </p>\r\n              <p> Highlight the 1\\(^{\\text{st}}\\) row and the 2\\(^{\\text{nd}}\\) column: </p>\r\n              <p> \\[ \\begin{bmatrix} 3 & 2 & 2 \\\\ 2 & 3 & -2 \\end{bmatrix} \\cdot \\begin{bmatrix} 3 & 2 \\\\ 2 & 3 \\\\ 2 & -2 \\end{bmatrix} = \\begin{bmatrix} 17 & \\_ \\\\ \\_ & \\_ \\end{bmatrix} \\] </p>\r\n              <p> Multiply corresponding components of the highlighted row and highlighted column, then add. </p>\r\n              <p> Multiply corresponding components and add: \\(3 \\cdot 2 + 2 \\cdot 3 + 2 \\cdot (-2) = 8\\). </p>\r\n              <p> Place this number into the 1\\(^{\\text{st}}\\) row and 2\\(^{\\text{nd}}\\) column of the product: </p>\r\n              <p> \\[ \\begin{bmatrix} 3 & 2 & 2 \\\\ 2 & 3 & -2 \\end{bmatrix} \\cdot \\begin{bmatrix} 3 & 2 \\\\ 2 & 3 \\\\ 2 & -2 \\end{bmatrix} = \\begin{bmatrix} 17 & 8 \\\\ \\_ & \\_ \\end{bmatrix} \\] </p>\r\n              <p> Find the entry in the 2\\(^{\\text{nd}}\\) row and 1\\(^{\\text{st}}\\) column of the product matrix. First look at the 2\\(^{\\text{nd}}\\) row of the first matrix and the 1\\(^{\\text{st}}\\) column of the second matrix. </p>\r\n              <p> Highlight the 2\\(^{\\text{nd}}\\) row and the 1\\(^{\\text{st}}\\) column: </p>\r\n              <p> \\[ \\begin{bmatrix} 3 & 2 & 2 \\\\ 2 & 3 & -2 \\end{bmatrix} \\cdot \\begin{bmatrix} 3 & 2 \\\\ 2 & 3 \\\\ 2 & -2 \\end{bmatrix} = \\begin{bmatrix} 17 & 8 \\\\ 8 & \\_ \\end{bmatrix} \\] </p>\r\n              <p> Multiply corresponding components of the highlighted row and highlighted column, then add. </p>\r\n              <p> Multiply corresponding components and add: \\(2 \\cdot 3 + 3 \\cdot 2 + (-2) \\cdot 2 = 8\\). </p>\r\n              <p> Place this number into the 2\\(^{\\text{nd}}\\) row and 1\\(^{\\text{st}}\\) column of the product: </p>\r\n              <p> \\[ \\begin{bmatrix} 3 & 2 & 2 \\\\ 2 & 3 & -2 \\end{bmatrix} \\cdot \\begin{bmatrix} 3 & 2 \\\\ 2 & 3 \\\\ 2 & -2 \\end{bmatrix} = \\begin{bmatrix} 17 & 8 \\\\ 8 & \\_ \\end{bmatrix} \\] </p>\r\n              <p> Find the entry in the 2\\(^{\\text{nd}}\\) row and 2\\(^{\\text{nd}}\\) column of the product matrix. First look at the 2\\(^{\\text{nd}}\\) row of the first matrix and the 2\\(^{\\text{nd}}\\) column of the second matrix. </p>\r\n              <p> Highlight the 2\\(^{\\text{nd}}\\) row and the 2\\(^{\\text{nd}}\\) column: </p>\r\n              <p> \\[ \\begin{bmatrix} 3 & 2 & 2 \\\\ 2 & 3 & -2 \\end{bmatrix} \\cdot \\begin{bmatrix} 3 & 2 \\\\ 2 & 3 \\\\ 2 & -2 \\end{bmatrix} = \\begin{bmatrix} 17 & 8 \\\\ 8 & \\_ \\end{bmatrix} \\] </p>\r\n              <p> Multiply corresponding components of the highlighted row and highlighted column, then add. </p>\r\n              <p> Multiply corresponding components and add: \\(2 \\cdot 2 + 3 \\cdot 3 + (-2) \\cdot (-2) = 17\\). </p>\r\n              <p> Place this number into the 2\\(^{\\text{nd}}\\) row and 2\\(^{\\text{nd}}\\) column of the product: </p>\r\n              <p> Answer: \\[ \\begin{bmatrix} 3 & 2 & 2 \\\\ 2 & 3 & -2 \\end{bmatrix} \\cdot \\begin{bmatrix} 3 & 2 \\\\ 2 & 3 \\\\ 2 & -2 \\end{bmatrix} = \\begin{bmatrix} 17 & 8 \\\\ 8 & 17 \\end{bmatrix} \\] </p>\r\n              <sub>Again, &#9825; lovingly &#9825; transcribed from Wolfram &#9825;</sub>\r\n            </div>\r\n          </div>\r\n          <br>\r\n          <p> The characteristic polynomial is \\(\\det(A A^{T}-\\lambda I)=\\lambda^{2}-34 \\lambda+225= \\) \\( (\\lambda-25)(\\lambda-9)\\), so the singular values are \\[\\sigma_{1}=\\sqrt{25}=5\\] \\[\\sigma_{2}=\\sqrt{9}=3\\] These singular values are important to finding \\(U\\) and \\(V^T\\), but let's pause to think about that last step. </p>\r\n          <div class=\"pl-5\">\r\n          <p>For this sixe theorems page, I do not want to get too much in to determinates (i.e. \\(\\det(A)\\)). Besides, the cross product we did for the <em>Fundamental Theorem</em> already demonstrated the \\(2\\times2\\) determinate. Recall, <em>the determinant of the matrix \\(\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\\) is given by \\(ad - bc\\)</em>.</p>\r\n          <p>The beauty and horror of determinats aside, quickly recall the <em>identity matrix</em>, which is just like the number \\(1\\) but for matrices.</p>\r\n          <p>\r\n            The general identity matrix, denoted as \\(I_n\\), is a square matrix of size \\(n \\times n\\) with ones on the main diagonal (from the top-left to the bottom-right) and zeros elsewhere.\r\n          </p>\r\n          <p>\r\n            The general identity matrix \\(I_n\\) can be represented as:\r\n          </p>\r\n          <p>\r\n            \\[\r\n            I_n = \\begin{bmatrix}\r\n              1 & 0 & \\cdots & 0 \\\\\r\n              0 & 1 & \\cdots & 0 \\\\\r\n              \\vdots & \\vdots & \\ddots & \\vdots \\\\\r\n              0 & 0 & \\cdots & 1\r\n            \\end{bmatrix}\r\n            \\]\r\n          </p>\r\n          <p>\r\n            In this matrix, the element at the \\(i^\\text{th}\\) row and \\(j^\\text{th}\\) column is denoted as \\((I_n)_{ij}\\). It has the value:\r\n          </p>\r\n          <p>\r\n            \\[\r\n            (I_n)_{ij} = \\begin{cases}\r\n              1 & \\text{if } i = j \\\\\r\n              0 & \\text{if } i \\neq j\r\n            \\end{cases}\r\n            \\]\r\n          </p>\r\n          <p>Now, before getting back to <em>Prof. Marshall Hampton</em> is to recall that matrix multiplication is not commutatitive. Generally, \\(AB \\ne BA \\)</p>\r\n        </div>\r\n        <p>Ok, here's another small explanation, but this isn't a mere note. It is important! The original from <em>Prof. Marshall Hampton</em> also does not mention \\(S = \\text{diag}_{n\\times p} (\\sigma) \\) meaning, the singualar value matrix is just a diagonal where \\(n\\times p\\) are just the row and column lengths for the original matrix.</p>\r\n        <p>Thus, \\( \\begin{bmatrix}\\sigma_1 & 0 & 0 \\\\ 0 & \\sigma_2 & 0\\end{bmatrix} = \\begin{bmatrix}3 & 0 & 0 \\\\ 0 & 5 & 0\\end{bmatrix} \\)</p>\r\n          <p> Now we find the right singular vectors (the columns of \\(V\\)) by finding an orthonormal set of eigenvectors of \\(A^{T} A\\). It is also possible to proceed by finding the left singular vectors (columns of \\(U\\)) instead. The eigenvalues of \\(A^{T} A\\) are 25, 9, and 0, and since \\(A^{T} A\\) is symmetric, we know that the eigenvectors will be orthogonal. </p>\r\n          <div class=\"card\"><div class=\"card-body\">\r\n            <p>\r\n              Multiply the following matrices:\r\n              \\[\r\n              \\begin{bmatrix}\r\n                3 & 2 \\\\\r\n                2 & 3 \\\\\r\n                2 & -2\r\n              \\end{bmatrix} \\cdot \\begin{bmatrix}\r\n                3 & 2 & 2 \\\\\r\n                2 & 3 & -2\r\n              \\end{bmatrix}\r\n              \\]\r\n            </p>\r\n            <p>\r\n              Determine the dimension of the product. The dimensions of the first matrix are 3x2 and the dimensions of the second matrix are 2x3.\r\n              This means the dimensions of the product are 3x3:\r\n              \\[\r\n              \\begin{bmatrix}\r\n                3 & 2 \\\\\r\n                2 & 3 \\\\\r\n                2 & -2\r\n              \\end{bmatrix} \\cdot \\begin{bmatrix}\r\n                3 & 2 & 2 \\\\\r\n                2 & 3 & -2\r\n              \\end{bmatrix} = \\begin{bmatrix}\r\n                \\_ & \\_ & \\_ \\\\\r\n                \\_ & \\_ & \\_ \\\\\r\n                \\_ & \\_ & \\_\r\n              \\end{bmatrix}\r\n              \\]\r\n            </p>\r\n            <p>\r\n              Find the entry in the \\(1^\\text{st}\\) row and \\(1^\\text{st}\\) column of the product matrix. First look at the \\(1^\\text{st}\\) row of the first matrix and the \\(1^\\text{st}\\) column of the second matrix.\r\n            </p>\r\n            <p>\r\n              Highlight the \\(1^\\text{st}\\) row and the \\(1^\\text{st}\\) column:\r\n            </p>\r\n            <p>\r\n              \\[\r\n              \\begin{bmatrix}\r\n                3 & 2 \\\\\r\n                2 & 3 \\\\\r\n                2 & -2\r\n              \\end{bmatrix} \\cdot \\begin{bmatrix}\r\n                3 & 2 & 2 \\\\\r\n                2 & 3 & -2\r\n              \\end{bmatrix} = \\begin{bmatrix}\r\n                \\_ & \\_ & \\_ \\\\\r\n                \\_ & \\_ & \\_ \\\\\r\n                \\_ & \\_ & \\_\r\n              \\end{bmatrix}\r\n              \\]\r\n            </p>\r\n            <p>\r\n              Multiply corresponding components of the highlighted row and highlighted column, then add.\r\n            </p>\r\n            <p>\r\n              Multiply corresponding components and add: \\(3 \\cdot 3 + 2 \\cdot 2 = 13\\).\r\n            </p>\r\n            <p>\r\n              Place this number into the \\(1^\\text{st}\\) row and \\(1^\\text{st}\\) column of the product:\r\n            </p>\r\n            <p>\r\n              \\[\r\n              \\begin{bmatrix}\r\n                3 & 2 \\\\\r\n                2 & 3 \\\\\r\n                2 & -2\r\n              \\end{bmatrix} \\cdot \\begin{bmatrix}\r\n                3 & 2 & 2 \\\\\r\n                2 & 3 & -2\r\n              \\end{bmatrix} = \\begin{bmatrix}\r\n                13 & \\_ & \\_ \\\\\r\n                \\_ & \\_ & \\_ \\\\\r\n                \\_ & \\_ & \\_\r\n              \\end{bmatrix}\r\n              \\]\r\n            </p>\r\n            <p>\r\n              Repeat the same process to find the remaining entries of the product matrix:\r\n            </p>\r\n            <p>\r\n              \\[\r\n              \\begin{bmatrix}\r\n                3 & 2 \\\\\r\n                2 & 3 \\\\\r\n                2 & -2\r\n              \\end{bmatrix} \\cdot \\begin{bmatrix}\r\n                3 & 2 & 2 \\\\\r\n                2 & 3 & -2\r\n              \\end{bmatrix} = \\begin{bmatrix}\r\n                13 & 12 & 2 \\\\\r\n                12 & 13 & -2 \\\\\r\n                2 & -2 & 8\r\n              \\end{bmatrix}\r\n              \\]\r\n            </p>\r\n            <p>\r\n              Therefore, the product of the matrices is:\r\n            </p>\r\n            <p>\r\n              \\[A^{T} A =\r\n              \\begin{bmatrix}\r\n                3 & 2 \\\\\r\n                2 & 3 \\\\\r\n                2 & -2\r\n              \\end{bmatrix} \\cdot \\begin{bmatrix}\r\n                3 & 2 & 2 \\\\\r\n                2 & 3 & -2\r\n              \\end{bmatrix} = \\begin{bmatrix}\r\n                13 & 12 & 2 \\\\\r\n                12 & 13 & -2 \\\\\r\n                2 & -2 & 8\r\n              \\end{bmatrix}\r\n              \\]\r\n            </p>\r\n            <sub>Yes, Wolfram :)</sub>\r\n          </div></div><br>\r\n          <p> For \\(\\lambda=25\\), we have </p>\r\n          <p> \\[ A^{T} A-25 I\\] \\[=\\begin{bmatrix}-12 & 12 & 2 \\\\ 12 & -12 & -2 \\\\ 2 & -2 & -17\\end{bmatrix} \\] </p>\r\n          <p> This row-reduces to the matrix \\(\\begin{bmatrix}1 & -1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0\\end{bmatrix}\\) giving the unit-length vector in the kernel of that matrix of \\(v_{1}=\\begin{bmatrix}\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0\\end{bmatrix}\\) </p>\r\n          <div class=\"card\">\r\n            <div class=\"card-body\">\r\n              <h5 class=\"card-title\">Convert \\(A^{T} A-25 I\\) to reduced row echelon form:</h5>\r\n              <p> \\[ \\begin{bmatrix} -12 & 12 & 2 \\\\ 12 & -12 & -2 \\\\ 2 & -2 & -17 \\end{bmatrix} \\] </p>\r\n              <p> Add one row to another: </p>\r\n              <p> \\[ \\begin{bmatrix} -12 & 12 & 2 \\\\ 0 & 0 & 0 \\\\ 2 & -2 & -17 \\end{bmatrix} \\] </p>\r\n              <p> Add a multiple of one row to another: </p>\r\n              <p> \\[ \\begin{bmatrix} -12 & 12 & 2 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & -\\frac{50}{3} \\end{bmatrix} \\] </p>\r\n              <p> Swap two rows: </p>\r\n              <p> \\[ \\begin{bmatrix} -12 & 12 & 2 \\\\ 0 & 0 & -\\frac{50}{3} \\\\ 0 & 0 & 0 \\end{bmatrix} \\] </p>\r\n              <p> Multiply row 2 by a scalar: </p>\r\n              <p> \\[ \\begin{bmatrix} -12 & 12 & 2 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} \\] </p>\r\n              <p> Subtract a multiple of one row from another: </p>\r\n              <p> \\[ \\begin{bmatrix} -12 & 12 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} \\] </p>\r\n              <p> Divide row 1 by a scalar: </p>\r\n              <p> \\[ \\begin{bmatrix} 1 & -1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} \\] </p>\r\n              <p> Verify matrix is reduced: </p>\r\n              <p> This matrix is now in reduced row echelon form. All nonzero rows are above rows of all zeros: </p>\r\n              <p> \\[ \\begin{bmatrix} 1 & -1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} \\] </p>\r\n              <p> Verify pivots and their positions: </p>\r\n              <p> Each pivot is 1 and is strictly to the right of every pivot above it: </p>\r\n              <p> \\[ \\begin{bmatrix} 1 & -1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} \\] </p>\r\n              <p> Verify all non-pivot elements in pivot columns are zeros: </p>\r\n              <p> Each pivot is the only nonzero entry in its column: </p>\r\n              <p> Answer: </p>\r\n              <p> \\[ \\begin{bmatrix} 1 & -1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} \\] </p>\r\n              <sub>Yes, I transcribed from Wolfram &#9825; &#9825;</sub>\r\n            </div>\r\n          </div>\r\n          <br>\r\n          <p> For \\(\\lambda=9\\), we have \\(A^{T} A-9 I=\\begin{bmatrix}4 & 12 & 2 \\\\ 12 & 4 & -2 \\\\ 2 & -2 & -1\\end{bmatrix}\\) which row-reduces to \\(\\begin{bmatrix}1 & 0 & -\\frac{1}{4} \\\\ 0 & 1 & \\frac{1}{4} \\\\ 0 & 0 & 0\\end{bmatrix}\\). </p>\r\n          <p> A unit-length vector in the kernel is \\(v_{2}=\\begin{bmatrix}\\frac{1}{\\sqrt{18}} \\\\ -\\frac{1}{\\sqrt{18}} \\\\ \\frac{4}{\\sqrt{18}}\\end{bmatrix}\\). </p>\r\n          <div class=\"card\">\r\n            <div class=\"card-body\">\r\n              <h5 class=\"card-title\">Find the kernel of the matrix M:</h5>\r\n              <p>\r\n                \\( M = \\begin{bmatrix} 1 & 0 & -\\frac{1}{4} \\\\ 0 & 1 & \\frac{1}{4} \\\\ 0 & 0 & 0 \\end{bmatrix} \\)\r\n              </p>\r\n              <p>\r\n                The kernel of a matrix \\( M \\) is the set of solutions \\( v \\) to the homogeneous equation \\( M \\cdot v = 0 \\).\r\n              </p>\r\n              <p>\r\n                The kernel of matrix \\( M = \\begin{bmatrix} 1 & 0 & -\\frac{1}{4} \\\\ 0 & 1 & \\frac{1}{4} \\\\ 0 & 0 & 0 \\end{bmatrix} \\) is the set of all vectors \\( v = (x_1, x_2, x_3) \\) such that \\( M \\cdot v = 0 \\):\r\n              </p>\r\n              <p>\r\n                \\( \\begin{bmatrix} 1 & 0 & -\\frac{1}{4} \\\\ 0 & 1 & \\frac{1}{4} \\\\ 0 & 0 & 0 \\end{bmatrix} \\cdot (x_1, x_2, x_3) = (0, 0, 0) \\)\r\n              </p>\r\n              <p>\r\n                Identify free variables. Free variables in the kernel \\((x_1, x_2, x_3)\\) correspond to the columns in \\( \\begin{bmatrix} 1 & 0 & -\\frac{1}{4} \\\\ 0 & 1 & \\frac{1}{4} \\\\ 0 & 0 & 0 \\end{bmatrix} \\) which have no pivot.\r\n              </p>\r\n              <p>\r\n                Column 3 is the only column with no pivot, so we may take \\( x_3 \\) to be the only free variable.\r\n              </p>\r\n              <p>\r\n                Perform matrix multiplication. Multiply out the reduced matrix \\( \\begin{bmatrix} 1 & 0 & -\\frac{1}{4} \\\\ 0 & 1 & \\frac{1}{4} \\\\ 0 & 0 & 0 \\end{bmatrix} \\) with the proposed solution vector \\( (x_1, x_2, x_3) \\):\r\n              </p>\r\n              <p>\r\n                \\( \\begin{bmatrix} 1 & 0 & -\\frac{1}{4} \\\\ 0 & 1 & \\frac{1}{4} \\\\ 0 & 0 & 0 \\end{bmatrix} \\cdot (x_1, x_2, x_3) = (x_1 - \\frac{x_3}{4}, x_2 + \\frac{x_3}{4}, 0) = (0, 0, 0) \\)\r\n              </p>\r\n              <p>\r\n                Convert to a system and solve in terms of the free variables. Solve the equations \\( \\begin{cases} x_1 - \\frac{x_3}{4} = 0 \\\\ x_2 + \\frac{x_3}{4} = 0 \\\\ 0 = 0 \\end{cases} \\) for \\( x_1 \\) and \\( x_2 \\):\r\n              </p>\r\n              <p>\r\n                \\( \\begin{cases} x_1 = \\frac{x_3}{4} \\\\ x_2 = -\\frac{x_3}{4} \\end{cases} \\)\r\n              </p>\r\n              <p>\r\n                Replace the pivot variables with free variable expressions. Rewrite \\( v \\) in terms of the free variable \\( x_3 \\), and assign it an arbitrary real value of \\( x \\):\r\n              </p>\r\n              <p>\r\n                \\( v = \\left(\\frac{x_3}{4}, -\\frac{x_3}{4}, x_3\\right) = \\left(\\frac{x}{4}, -\\frac{x}{4}, x\\right) \\) for \\( x \\in \\mathbb{R} \\)\r\n              </p>\r\n              <p>\r\n                Rewrite the solution vector without using fractions. Since \\( x \\) is taken from \\( \\mathbb{R} \\), we can replace it with \\( 4x \\):\r\n              </p>\r\n              <p>\r\n                \\( \\left(\\frac{x}{4}, -\\frac{x}{4}, x\\right) \\rightarrow \\left(\\frac{4x}{4}, -\\frac{1}{4}(4x), 4x\\right) = (x, -x, 4x) \\) for \\( x \\in \\mathbb{R} \\)\r\n              </p>\r\n              <p>\r\n                Convert to set-builder notation. Rewrite the solution vector \\( v = (x, -x, 4x) \\) in set notation:\r\n              </p>\r\n              <p>\r\n                Answer: \\( \\left\\{ (x, -x, 4x) : x \\in \\mathbb{R} \\right\\} \\)\r\n              </p>\r\n              <sub>Yes, yes... &#9825; &#9825; Wolfram &#9825;</sub>\r\n            </div>\r\n          </div><br>\r\n          <p> For the last eigenvector, we could (option 1) compute the kernel of \\(A^{T} A\\) or (option 2) find a unit vector perpendicular to \\(v_{1}\\) and \\(v_{2}\\). </p>\r\n          <p><em>Prof. Marshall Hampton</em> only uses option 2, which requires more reasoning and abstraction but is far less computational. Still, I like option 1 too.</p>\r\n          <div class=\"card\"><div class=\"card-body\">\r\n            <h5 class=\"card-title\">Option 1: \\( \\quad \\text{ker}(A^{T}A)\\)</h5>\r\n            <p>Let \\(M\\) denote \\(A^{T}A = \\begin{bmatrix}\r\n            13 & 12 & 2 \\\\\r\n            12 & 13 & -2 \\\\\r\n            2 & -2 & 8\r\n          \\end{bmatrix}\\)</p>\r\n          <p>\r\n            \\( M = \\begin{bmatrix} 13 & 12 & 2 \\\\ 12 & 13 & -2 \\\\ 2 & -2 & 8 \\end{bmatrix} \\)\r\n          </p>\r\n          <p>\r\n            The kernel of a matrix \\( M \\) is the set of solutions \\( v \\) to the homogeneous equation \\( M \\cdot v = 0 \\).\r\n          </p>\r\n          <p>\r\n            The kernel of matrix \\( M = \\begin{bmatrix} 13 & 12 & 2 \\\\ 12 & 13 & -2 \\\\ 2 & -2 & 8 \\end{bmatrix} \\) is the set of all vectors \\( v = (x_1, x_2, x_3) \\) such that \\( M \\cdot v = 0 \\):\r\n          </p>\r\n          <p>\r\n            \\( \\begin{bmatrix} 13 & 12 & 2 \\\\ 12 & 13 & -2 \\\\ 2 & -2 & 8 \\end{bmatrix} \\cdot (x_1, x_2, x_3) = (0, 0, 0) \\)\r\n          </p>\r\n          <p>\r\n            The kernel of a matrix is equal to the kernel of the row echelon form of the matrix.\r\n          </p>\r\n          <p>\r\n            Reduce the matrix \\( \\begin{bmatrix} 13 & 12 & 2 \\\\ 12 & 13 & -2 \\\\ 2 & -2 & 8 \\end{bmatrix} \\) to row echelon form:\r\n          </p>\r\n          <p>\r\n            \\( \\begin{bmatrix} 13 & 12 & 2 \\\\ 12 & 13 & -2 \\\\ 2 & -2 & 8 \\end{bmatrix} \\)\r\n          </p>\r\n          <p>\r\n            Subtract a multiple of one row from another.\r\n            Subtract \\( \\frac{12}{13} \\times \\) (row 1) from row 2:\r\n          </p>\r\n          <p>\r\n            \\( \\begin{bmatrix} 13 & 12 & 2 \\\\ 0 & \\frac{25}{13} & -\\frac{50}{13} \\\\ 2 & -2 & 8 \\end{bmatrix} \\)\r\n          </p>\r\n          <p>\r\n            Subtract a multiple of one row from another.\r\n            Subtract \\( \\frac{2}{13} \\times \\) (row 1) from row 3:\r\n          </p>\r\n          <p>\r\n            \\( \\begin{bmatrix} 13 & 12 & 2 \\\\ 0 & \\frac{25}{13} & -\\frac{50}{13} \\\\ 0 & -\\frac{50}{13} & \\frac{100}{13} \\end{bmatrix} \\)\r\n          </p>\r\n          <p>\r\n            Swap two rows.\r\n            Swap row 2 with row 3:\r\n          </p>\r\n          <p>\r\n            \\( \\begin{bmatrix} 13 & 12 & 2 \\\\ 0 & -\\frac{50}{13} & \\frac{100}{13} \\\\ 0 & \\frac{25}{13} & -\\frac{50}{13} \\end{bmatrix} \\)\r\n          </p>\r\n          <p>\r\n            Add a multiple of one row to another.\r\n            Add \\( \\frac{1}{2} \\times \\) (row 2) to row 3:\r\n          </p>\r\n          <p>\r\n            \\( \\begin{bmatrix} 13 & 12 & 2 \\\\ 0 & -\\frac{50}{13} & \\frac{100}{13} \\\\ 0 & 0 & 0 \\end{bmatrix} \\)\r\n          </p>\r\n          <p>\r\n            Multiply row 2 by a scalar.\r\n            Multiply row 2 by \\( -\\frac{13}{50} \\):\r\n          </p>\r\n          <p>\r\n            \\( \\begin{bmatrix} 13 & 12 & 2 \\\\ 0 & 1 & -2 \\\\ 0 & 0 & 0 \\end{bmatrix} \\)\r\n          </p>\r\n          <p>\r\n            Subtract a multiple of one row from another.\r\n            Subtract \\( 12 \\times \\) (row 2) from row 1:\r\n          </p>\r\n          <p>\r\n            \\( \\begin{bmatrix} 13 & 0 & 26 \\\\ 0 & 1 & -2 \\\\ 0 & 0 & 0 \\end{bmatrix} \\)\r\n          </p>\r\n          <p>\r\n            Divide row 1 by a scalar.\r\n            Divide row 1 by 13:\r\n          </p>\r\n          <p>\r\n            \\( \\begin{bmatrix} 1 & 0 & 2 \\\\ 0 & 1 & -2 \\\\ 0 & 0 & 0 \\end{bmatrix} \\)\r\n          </p>\r\n          <p>\r\n            Identify free variables.\r\n            Free variables in the kernel \\( (x_1, x_2, x_3) \\) correspond to the columns in \\( \\begin{bmatrix} 1 & 0 & 2 \\\\ 0 & 1 & -2 \\\\ 0 & 0 & 0 \\end{bmatrix} \\) which have no pivot.\r\n            Column 3 is the only column with no pivot, so we may take \\( x_3 \\) to be the only free variable.\r\n          </p>\r\n          <p>\r\n            Perform matrix multiplication.\r\n            Multiply out the reduced matrix \\( \\begin{bmatrix} 1 & 0 & 2 \\\\ 0 & 1 & -2 \\\\ 0 & 0 & 0 \\end{bmatrix} \\) with the proposed solution vector \\( (x_1, x_2, x_3) \\):\r\n          </p>\r\n          <p>\r\n            \\( \\begin{bmatrix} 1 & 0 & 2 \\\\ 0 & 1 & -2 \\\\ 0 & 0 & 0 \\end{bmatrix} \\cdot (x_1, x_2, x_3) = (x_1 + 2x_3, x_2 - 2x_3, 0) = (0, 0, 0) \\)\r\n          </p>\r\n          <p>\r\n            Convert to a system and solve in terms of the free variables.\r\n            Solve the equations \\( \\{ x_1 + 2x_3 = 0, x_2 - 2x_3 = 0, 0 = 0 \\} \\) for \\( x_1 \\) and \\( x_2 \\):\r\n          </p>\r\n          <p>\r\n            \\( \\{ x_1 = -2x_3, x_2 = 2x_3 \\} \\)\r\n          </p>\r\n          <p>\r\n            Replace the pivot variables with free variable expressions.\r\n            Rewrite \\( v \\) in terms of the free variable \\( x_3 \\), and assign it an arbitrary real value of \\( x \\):\r\n          </p>\r\n          <p>\r\n            \\( v = (x_1, x_2, x_3) = (-2x_3, 2x_3, x_3) = (-2x, 2x, x) \\) for \\( x \\in \\mathbb{R} \\)\r\n          </p>\r\n          <p>\r\n            Convert to set builder notation.\r\n            Rewrite the solution vector \\( v = (-2x, 2x, x) \\) in set notation:\r\n          </p>\r\n          <p>\r\n            Answer: \\( \\{ (-2x, 2x, x) : x \\in \\mathbb{R} \\} \\)\r\n          </p>\r\n          <sub>Wolfram, Wolfram, etc &#9825;</sub>\r\n          </div></div><br>\r\n          <div class=\"pl-5\">\r\n          <p> <b>Option 2:</b> To be perpendicular to \\(v_{1}=\\begin{bmatrix}a & b & c\\end{bmatrix}\\), we need \\(-a=b\\). Then the condition that \\(v_{2}^{T} v_{3}=0\\) becomes \\(\\frac{2a}{\\sqrt{18}}+\\frac{4c}{\\sqrt{18}}=0\\) or \\(-a=2c\\).</p><p>Thus, \\(v_{3}=\\begin{bmatrix}a & -a & -\\frac{a}{2}\\end{bmatrix}\\) &mdash; which satsfies the <em>option 1</em> result factoring out \\(-2\\) from \\( \\ker (A)=\\) \\( \\{ (-2x, 2x, x) : x \\in \\mathbb{R} \\} \\) &mdash; and for it to be unit-length, we need \\(a=\\frac{2}{3}\\) which gives \\(v_{3}=\\begin{bmatrix}\\frac{2}{3} & -\\frac{2}{3} & -\\frac{1}{3}\\end{bmatrix}\\).</p>\r\n          </div>\r\n          <p>At this point, we know that </p>\r\n          <p class=\"overflow-auto\"> \\[ A=U S V^{T}=U\\begin{bmatrix}5 & 0 & 0 \\\\ 0 & 3 & 0\\end{bmatrix}\\begin{bmatrix}\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0 \\\\ \\frac{1}{\\sqrt{18}} & -\\frac{1}{\\sqrt{18}} & \\frac{4}{\\sqrt{18}} \\\\ \\frac{2}{3} & -\\frac{2}{3} & -\\frac{1}{3}\\end{bmatrix} \\] </p>\r\n\r\n          \r\n          <p> Finally, we can compute \\(U\\) (by the product of the eigenvectors \\(v_i\\) and the original matrix \\(A\\) and also the reciprocals of the singular values\\(\\frac{1}{\\sigma_i}\\)) all via the formula \\(\\sigma u_{i}=A v_{i}\\) or \\(u_{i}=\\frac{1}{\\sigma} A v_{i}\\). This gives \\(U=\\begin{bmatrix}\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}}\\end{bmatrix}\\). </p>\r\n          <p>As <em>Prof. Marshall Hampton</em> puts it: \"So, in its full glory, the SVD is:\"</p>\r\n          <p class=\"overflow-auto\"> \\[ A=U S V^{T}=\\begin{bmatrix}\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}}\\end{bmatrix}\\begin{bmatrix}5 & 0 & 0 \\\\ 0 & 3 & 0\\end{bmatrix}\\begin{bmatrix}\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0 \\\\ \\frac{1}{\\sqrt{18}} & -\\frac{1}{\\sqrt{18}} & \\frac{4}{\\sqrt{18}} \\\\ \\frac{2}{3} & -\\frac{2}{3} & -\\frac{1}{3}\\end{bmatrix} \\] </p>\r\n        </div>\r\n      </div>\r\n      <hr>\r\n      <!-- SPECTRAL THEOREM -->\r\n      <h2 id=\"spectral-theorem\">Spectral Theorem</h2>\r\n      <p>If \\( A^T = A \\), there are orthonormal \\( q \\)'s so that \\( Aq_i = \\lambda_iq_i \\) and \\( A = Q\\Lambda Q^T \\).</p>\r\n      <p>Mathematically: If \\( A \\) is symmetric, there exist orthonormal eigenvectors (\\( q \\)'s) and eigenvalues (\\( \\lambda \\)'s) such that \\( Aq_i = \\lambda_iq_i \\).</p>\r\n      <p>Now we're really talking.</p>\r\n      <p>Here is another explanation, this time explaining a lovely example from <em>Prof. Bruce Ikenaga</em> transcribed from <a href=\"https://sites.millersville.edu/bikenaga/linear-algebra/spectral-theorem/spectral-theorem.html\">here</a>.</p>\r\n      <p class=\"pl-5\">Also, <a href=\"https://mast.queensu.ca/~br66/419/spectraltheoremproof.pdf\">here</a> is a shorter proof from <em>Prof. Brad Rodgers</em> at an undergraduate level, and <a href=\"https://math.dartmouth.edu/~m113s19/ln-spec-thm.pdf\">here</a> is a longer proof from <em>Prof. Dana Williams</em> at a graduate level applying this theorem to bounded and unbounded operators on an infinite dimensional complex Hilbert space, all culminating in Stone's Theorem.</p>\r\n      <p>Finally, <em>Prof. Ikenaga's</em> example requires little explanation or calculation, but there are two points to touch on first.</p>\r\n      <p>First, each column has unit length and is perpendicular to every other column, so the inverse is the transpose. \"If \\(A^{-1}=A^T\\), then \\(A^TA=I\\). This means that each column has unit length and is perpendicular to every other column. That means it is an orthonormal matrix. [...] Think of \\(A\\) as an arrangement of \\(n\\) columns (each \\(n\\) elements tall). Then the \\((i, j)\\) element of \\(A^TA\\) is the dot product of the \\(i\\)th and \\(j\\)th columns of \\(A\\) since the \\(i\\)th row of \\(A^T\\) is the \\(i\\)th column of \\(A\\).\" (<a href=\"https://math.stackexchange.com/a/156742/1098426\">Math Stack Exchange</a>)</p>\r\n      <p>Second, eigenvectors from different eigenvalues are linearly independent.</p>\r\n      <p>The best explanation I could find of this is from <a href=\"https://math.stackexchange.com/a/29374/1098426\">Math Stack Exchange</a>:</p>\r\n      <p class=\"pl-5 overflow-auto\">Suppose \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\) correspond to distinct eigenvalues \\(\\lambda_1\\) and \\(\\lambda_2\\), respectively. \r\n        \r\n        Take a linear combination that is equal to \\(0\\), \\(\\alpha_1\\mathbf{v}_1+\\alpha_2\\mathbf{v}_2 = \\mathbf{0}\\). We need to show that \\(\\alpha_1=\\alpha_2=0\\).\r\n        \r\n        Applying \\(T\\) to both sides, we get\r\n        \\[\\mathbf{0} = T(\\mathbf{0}) = T(\\alpha_1\\mathbf{v}_1+\\alpha_2\\mathbf{v}_2) = \\alpha_1\\lambda_1\\mathbf{v}_1 + \\alpha_2\\lambda_2\\mathbf{v}_2.\\]\r\n        Now, instead, multiply the original equation by \\(\\lambda_1\\):\r\n        \\[\\mathbf{0} = \\lambda_1\\alpha_1\\mathbf{v}_1 + \\lambda_1\\alpha_2\\mathbf{v}_2.\\]\r\n        Now take the two equations,\r\n        \\[\\begin{align*}\r\n        \\mathbf{0} &= \\alpha_1\\lambda_1\\mathbf{v}_1 + \\alpha_2\\lambda_2\\mathbf{v}_2\\\\\r\n        \\mathbf{0} &= \\alpha_1\\lambda_1\\mathbf{v}_1 + \\alpha_2\\lambda_1\\mathbf{v}_2\r\n        \\end{align*}\\]\r\n        and taking the difference, we get:\r\n        \\[\\mathbf{0} = 0\\mathbf{v}_1 + \\alpha_2(\\lambda_2-\\lambda_1)\\mathbf{v}_2 = \\alpha_2(\\lambda_2-\\lambda_1)\\mathbf{v}_2.\\]\r\n        \r\n        Since \\(\\lambda_2-\\lambda_1\\neq 0\\), and since \\(\\mathbf{v}_2\\neq\\mathbf{0}\\) (because \\(\\mathbf{v}_2\\) is an eigenvector), then \\(\\alpha_2=0\\). Using this on the original linear combination \\(\\mathbf{0} = \\alpha_1\\mathbf{v}_1 + \\alpha_2\\mathbf{v}_2\\), we conclude that \\(\\alpha_1=0\\) as well (since \\(\\mathbf{v}_1\\neq\\mathbf{0}\\)).\r\n        \r\n        So \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\) are linearly independent.\r\n        \r\n        Now try using induction on \\(n\\) for the general case.</p>\r\n      <div class=\"card\"><div class=\"card-body\">\r\n        <p>Now, take a symmetric matrix \\(\r\n          A=\\left[\\begin{array}{ccc}\r\n          -1 & 2 & 0 \\\\\r\n          2 & 2 & 0 \\\\\r\n          0 & 0 & 3\r\n          \\end{array}\\right]\r\n          \\)</p>\r\n          \r\n          <p>Find an orthogonal matrix \\(\\mathrm{O}\\) which diagonalizes \\(\\mathrm{A}\\). Find \\(\\mathrm{O}^{-1}\\) and the corresponding diagonal matrix.</p>\r\n          \r\n          <p>The characteristic polynomial is</p>\r\n          \r\n          <p>\\[\r\n          \\det{(A - xI)}\r\n          \\]\r\n        \\[ = (x-3)[(x-2)(x+1)-(2)(2)]\\]\r\n        \\[=-(x-3)^2(x+2)\\]\r\n        </p>\r\n          \r\n          <p>The eigenvalues are \\(x=3\\) and \\(x=-2\\).</p>\r\n          \r\n          <p>For \\(x=3\\), the eigenvector matrix is</p>\r\n          \r\n          <p>\\[\r\n          A-3 I=\\left[\\begin{array}{ccc}\r\n          -4 & 2 & 0 \\\\\r\n          2 & -1 & 0 \\\\\r\n          0 & 0 & 0\r\n          \\end{array}\\right] \\rightarrow\\left[\\begin{array}{ccc}\r\n          2 & -1 & 0 \\\\\r\n          0 & 0 & 0 \\\\\r\n          0 & 0 & 0\r\n          \\end{array}\\right]\r\n          \\]</p>\r\n          \r\n          <p>This gives the independent eigenvectors \\((1,2,0)\\) and \\((0,0,1)\\). Dividing them by their lengths, I get \\(\\frac{1}{\\sqrt{5}}(1,2,0)\\) and \\((0,0,1)\\).</p>\r\n          \r\n          <p>For \\(x=-2\\), the eigenvector matrix is</p>\r\n          \r\n          <p>\\[\r\n          A+2 I=\\left[\\begin{array}{lll}\r\n          1 & 2 & 0 \\\\\r\n          2 & 4 & 0 \\\\\r\n          0 & 0 & 5\r\n          \\end{array}\\right] \\rightarrow\\left[\\begin{array}{lll}\r\n          1 & 2 & 0 \\\\\r\n          0 & 0 & 1 \\\\\r\n          0 & 0 & 0\r\n          \\end{array}\\right]\r\n          \\]</p>\r\n          \r\n          <p>This gives the independent eigenvector \\((-2,1,0)\\). Dividing it by its length, I get \\(\\frac{1}{\\sqrt{5}}(-2,1,0)\\).</p>\r\n          \r\n          <p>Thus, the orthogonal diagonalizing matrix is</p>\r\n          \r\n          <p>\\[\r\n          O=\\left[\\begin{array}{ccc}\r\n          \\frac{1}{\\sqrt{5}} & 0 & -\\frac{2}{\\sqrt{5}} \\\\\r\n          \\frac{2}{\\sqrt{5}} & 0 & \\frac{1}{\\sqrt{5}} \\\\\r\n          0 & 1 & 0\r\n          \\end{array}\\right]\r\n          \\]</p>\r\n          \r\n          <p>Then (note again: each column has unit length and is perpendicular to every other column, so the inverse is the transpose)</p>\r\n          \r\n          <p>\\[\r\n          O^{-1}=O^T=\\left[\\begin{array}{ccc}\r\n          \\frac{1}{\\sqrt{5}} & \\frac{2}{\\sqrt{5}} & 0 \\\\\r\n          0 & 0 & 1 \\\\\r\n          -\\frac{2}{\\sqrt{5}} & \\frac{1}{\\sqrt{5}} & 0\r\n          \\end{array}\\right]\r\n          \\]</p>\r\n          \r\n          <p>The diagonal matrix is</p>\r\n          \r\n          <p>\\[\r\n          O^T A O=\\left[\\begin{array}{ccc}\r\n          \\frac{1}{\\sqrt{5}} & \\frac{2}{\\sqrt{5}} & 0 \\\\\r\n          0 & 0 & 1 \\\\\r\n          -\\frac{2}{\\sqrt{5}} & \\frac{1}{\\sqrt{5}} & 0\r\n          \\end{array}\\right]\r\n          \r\n          \\left[\\begin{array}{ccc}\r\n          -1 & 2 & 0 \\\\\r\n          2 & 2 & 0 \\\\\r\n          0 & 0 & 3\r\n          \\end{array}\\right]\r\n\r\n          \\left[\\begin{array}{ccc}\r\n          \\frac{1}{\\sqrt{5}} & 0 & -\\frac{2}{\\sqrt{5}} \\\\\r\n          \\frac{2}{\\sqrt{5}} & 0 & \\frac{1}{\\sqrt{5}} \\\\\r\n          0 & 1 & 0\r\n          \\end{array}\\right]\r\n          \\] \\[ =\\left[\\begin{array}{ccc}\r\n          3 & 0 & 0 \\\\\r\n          0 & 3 & 0 \\\\\r\n          0 & 0 & -2\r\n          \\end{array}\\right] \\]</p>\r\n          \r\n        <sup>We all &#9825; Wolfram</sup></div></div><br>\r\n      <hr>\r\n      <h2 id=\"nutshell\">Linear Algebra in a Nutshell</h2>\r\n      <p>As a bonus, here is the other part of the sections from from Gilbert Strang's <em>Introduction to Linear Algebra, 5th Ed</em>. I was tempted not to include this because I cannot think of a sufficient example. But, I was struck by the similarity with \"The Key Theorem of Linear Algebra\" from Prof. Thomas Garrity's <em>All the Math you Missed, 2nd Ed</em>. Prof. Garrity lays out his version after giving three definitions for the determinant (basically from induction, from linear rules, & from signed volume), but Prof. Garrity does so before defining eigenvectors, as his intro to matrices as linear transformations.</p>\r\n      <p><b>Let \\(A\\) be a \\(n\\times n\\) matrix...</b></p>\r\n      <div class=\"table-responsive\">\r\n      <table class=\"table\">\r\n        <thead class=\"thead-light\">\r\n        <tr>\r\n          <th scope=\"col\" class=\"col-3\">Singular</th>\r\n          <th scope=\"col\" class=\"col-3\">Nonsingular</th>\r\n        </tr>\r\n      </thead>\r\n      <tbody>\r\n        <tr>\r\n          <td>\\(A\\) is not invertible</td>\r\n          <td>\\(A\\) is invertible</td>\r\n        </tr>\r\n        <tr>\r\n          <td>The columns are dependent</td>\r\n          <td>The columns are independent</td>\r\n        </tr>\r\n        <tr>\r\n          <td>The rows are dependent</td>\r\n          <td>The rows are independent</td>\r\n        </tr>\r\n        <tr>\r\n          <td>The determinant is zero</td>\r\n          <td>The determinant is not zero</td>\r\n        </tr>\r\n        <tr>\r\n          <td>\\(Ax=0\\) has infinitely many solutions</td>\r\n          <td>\\(Ax=0\\) has one solution \\(x=0\\)</td>\r\n        </tr>\r\n        <tr>\r\n          <td>\\(Ax=b\\) has no solution or \\(\\infty\\) many</td>\r\n          <td>\\(Ax=b\\) has one solution \\(x=A^{-1}b\\)</td>\r\n        </tr>\r\n        <tr>\r\n          <td>\\(A\\) has \\(r < n\\) pivots</td>\r\n          <td>\\(A\\) has \\(n\\) (nonzero) pivots</td>\r\n        </tr>\r\n        <tr>\r\n          <td>\\(A\\) has rank \\(r < n\\)</td>\r\n          <td>\\(A\\) has full rank \\(r=n\\)</td>\r\n        </tr>\r\n        <tr>\r\n          <td>Reduced row echelon form isn't \\(R=I\\)</td>\r\n          <td>Reduced row echelon form is \\(R=I\\)</td>\r\n        </tr>\r\n        <tr>\r\n          <td>The column space has dimension \\(r < n\\)</td>\r\n          <td>The column space is all of \\(R^n\\)</td>\r\n        </tr>\r\n        <tr>\r\n          <td>The row space has dimension \\(r < n\\)</td>\r\n          <td>The row space is all of \\(R^n\\)</td>\r\n        </tr>\r\n        <tr>\r\n          <td>Zero is an eigenvalue of \\(A\\)</td>\r\n          <td>All eigenvalues are nonzero</td>\r\n        </tr>\r\n        <tr>\r\n          <td>\\(A^TA\\) is only semidefinite</td>\r\n          <td>\\(A^TA\\) is symmetric positive definite</td>\r\n        </tr>\r\n        <tr>\r\n          <td>\\(A\\) has \\(r < n\\) singular values</td>\r\n          <td>\\(A\\) has \\(n\\) (positive) singular values</td>\r\n        </tr>\r\n      </tbody>\r\n      </table>\r\n    </div>\r\n      <br>\r\n      <div class=\"card\"><div class=\"card-body\">\r\n        <p><b>Now &#9825; transcribed &#9825; from <em>All the Math you Missed</em> by Prof. Garrity &#9825; &#9825;</b></p>\r\n        <p><em>Theorem 1.6.1 (Key Theorem)</em></p>\r\n        <p>Let \\(A\\) be an \\(n \\times n\\) matrix. Then the following are equivalent:</p>\r\n        <ol>\r\n          <li>\\(A\\) is invertible.</li>\r\n          <li>\\(\\det(A) \\neq 0\\).</li>\r\n          <li>\\(\\text{ker}(A) = \\{0\\}\\).</li>\r\n          <li>If \\(b\\) is a column vector in \\(\\mathbb{R}^n\\), there is a unique column vector \\(x\\) in \\(\\mathbb{R}^n\\) satisfying \\(Ax = b\\).</li>\r\n          <li>The columns of \\(A\\) are linearly independent \\(n \\times 1\\) column vectors.</li>\r\n          <li>The rows of \\(A\\) are linearly independent \\(1 \\times n\\) row vectors.</li>\r\n          <li>The transpose \\(A^T\\) of \\(A\\) is invertible. (Here, if \\(A = (a_{ij})\\), then \\(A^T = (a_{ji})\\)).</li>\r\n          <li>All of the eigenvalues of \\(A\\) are non-zero.</li>\r\n        </ol>\r\n        \r\n        <p><em>Theorem 1.6.2 (Key Theorem)</em></p>\r\n        <p>Let \\(T : V \\rightarrow V\\) be a linear transformation. Then the following are equivalent:</p>\r\n        <ol>\r\n          <li>\\(T\\) is invertible.</li>\r\n          <li>\\(\\det(T) \\neq 0\\), where the determinant is defined by a choice of basis on \\(V\\).</li>\r\n          <li>\\(\\text{ker}(T) = \\{0\\}\\).</li>\r\n          <li>If \\(b\\) is a vector in \\(V\\), there is a unique vector \\(v\\) in \\(V\\) satisfying \\(T(v) = b\\).</li>\r\n          <li>For any basis \\(v_1, \\ldots, v_n\\) of \\(V\\), the image vectors \\(T(v_1), \\ldots, T(v_n)\\) are linearly independent.</li>\r\n          <li>For any basis \\(v_1, \\ldots, v_n\\) of \\(V\\), if \\(S\\) denotes the transpose linear transformation of \\(T\\), then the image vectors \\(S(v_1), \\ldots, S(v_n)\\) are linearly independent.</li>\r\n          <li>The transpose of \\(T\\) is invertible. (Here the transpose is defined by a choice of basis on \\(V\\).)</li>\r\n          <li>All of the eigenvalues of \\(T\\) are non-zero.</li>\r\n        </ol>\r\n      </div></div><br>\r\n      <hr>\r\n      <p>I hope you <em>enjoyed</em> because I sure did.</p>\r\n\r\n    </div><!-- bootstrap wrapper container closing div tag-->\r\n    <!-- <footer class=\"bg-light pt-5 pb-2 px-5\">\r\n      <hr> \r\n      <a href=\"/\">Home</a>\r\n    </footer> -->\r\n    <script src=\"https://code.jquery.com/jquery-3.4.1.min.js\"></script>\r\n    <script src=\"https://code.jquery.com/ui/1.12.1/jquery-ui.js\"></script>\r\n    <script src=\"https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js\" integrity=\"sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo\" crossorigin=\"anonymous\"></script>\r\n    <script src=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js\" integrity=\"sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6\" crossorigin=\"anonymous\"></script>\r\n    <!--<script src=\"js/main.js\"></script>-->\r\n  </body>\r\n</html>",
      "date_published": "2023-05-31T17:00:00-07:00"
    },{
      "id": "https://ischmidls.github.io/posts/pages/heyting/",
      "url": "https://ischmidls.github.io/posts/pages/heyting/",
      "title": "heyting",
      "content_html": "<!DOCTYPE html>\r\n<html>\r\n<head>\r\n    <meta charset=\"UTF-8\" />\r\n    <title>Heyting</title>\r\n    <!--MATH SUPPORT-->\r\n    <script type=\"text/javascript\" id=\"MathJax-script\" async src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"></script>\r\n    <!--STYLE-->\r\n    <link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css\" integrity=\"sha384-B0vP5xmATw1+K9KRQjQERJvTumQW0nPEzvF6L/Z6nronJ3oUOFUFpCjEUQouq2+l\" crossorigin=\"anonymous\">\r\n    <!--BACKWARDS COMPATIBILITY-->\r\n    <script src=\"https://polyfill.io/v3/polyfill.min.js?features=es6\"></script>\r\n</head>\r\n<body>\r\n    <header><br></header>\r\n    <div class=\"container\" id=\"bsr-wrapper\">\r\n        <h1>Heyting Algebras</h1>\r\n        <p>Izak, May 2023</p>\r\n      <!-- <p class=\"text-secondary\">Estimated reading time: ~20 min. <br> \r\n        Approx. word count: ~3,700 words <br> \r\n        Approx math terms: ~300 LaTeX blocks</p> -->\r\n\r\n        <h2>Table of Contents</h2>\r\n        <ul>\r\n            <li><a href=\"#heyting-algebra\">Heyting Algebra</a></li>\r\n            <li><a href=\"#topologize-this\">Topologize This</a></li>\r\n            <li><a href=\"#adjunction-boolean\">Adjunction Boolean</a></li>\r\n            <li><a href=\"#quantifiers-as-adjoints\">Quantifiers as Adjoints</a></li>\r\n            <li><a href=\"#topoi-of-heyting-algebra-models\">Topoi of ʜᴇʏᴛɪɴɢ ᴀʟɢᴇʙʀᴀ Models</a></li>\r\n          </ul>\r\n          \r\n          <h2 id=\"heyting-algebra\">Heyting Algebra</h2>\r\n          \r\n          \r\n<p>Now, Heyting algebras often take the unhelpful label—for those who are not yet students of topology—that: \"Every topology provides a complete Heyting algebra in the form of its open set lattice.\"</p>\r\n<p>NOTE: An \"open\" set is defined as a set where each point is surrounded by other members of the set. To provide a precise definition, one begins with a \"basic open set\" and forms other open sets from it. In the presence of a metric, basic open sets can be represented by \\(ϵ\\)-neighborhoods (points are less than \\(ϵ\\) away from some point for \\(ϵ > 0\\)). \\(A\\) set \\(S\\) is considered open if, for every point \\(P ∈ S\\), there exists a basic open set \\(U\\) containing \\(P\\) such that \\(U ⊆ S\\). \\(A\\) set is classified as closed if it either has no complement or if its complement is open.</p>\r\n<p>A subset lattice (also called powerset lattice) is prototypical. Where join is disjunction, union, and inclusion, and meet is conjunction, intersection, and (something like) includes. In the case of Heyting algebras, these might also be maximum and minimum respectively.</p>\r\n<img alt=\"\" src=\"images/image2.png\" style=\"width: 100%; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);\" title=\"\">\r\n<p>Source: <a href=\"https://en.wikipedia.org/wiki/Power_set\">Wikipedia Power set</a></p>\r\n<p>For a more concrete example of a Heyting algebra, let us look at the next Figure where is is clear conjunction \\(A \\land B\\) is \\(min(A, B)\\) and the disjunction \\(A \\lor B\\) is \\(max(A, B)\\) but less clear is what Wikipedia kindly spells out: \"Every totally ordered set that has a least element \\(0\\)and a greatest element \\(1\\) is a Heyting algebra (if viewed as a lattice). In this case \\(A \\rightarrow B\\) equals to \\(1\\)(or \\(T\\) for those generalized fans out there) when \\(A \\leq B\\), and \\(B\\) otherwise.\"</p>\r\n<img alt=\"\" src=\"images/image6.png\" style=\"width: 100%; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);\" title=\"\">\r\n<p>Fig: Source: <a href=\"https://en.wikipedia.org/wiki/Heyting_algebra\">Wikipedia Heyting algebra</a></p>\r\n<p>With all that aside, it becomes somewhat clearer how anybody would evaluate truth via a Heyting algebra. But what of intuitionistic logic? Well, Wikipedia touches on that too!</p>\r\n<p>Picture this: We've got this formula \\((P \\lor Q) \\rightarrow P\\), and by the definition of the pseudo-complement (fancy term alert!), it's all about finding the largest element \\(x\\) such that \\(P \\land Q \\land x \\leq P\\). And guess what? We hit the jackpot! We find an \\(x\\) that satisfies the equation, and you know what that means? It's \\(1\\),baby! That's the biggest \\(x\\) we can get.</p>\r\n<p>But wait, there's more! The rule of modus ponens swoops in like a superhero, allowing us to derive this formula from the formulas \\(P\\) and \\(P \\rightarrow Q\\). Now, here's where things get interesting. In any Heyting algebra, if \\(P\\) has that value of \\(1\\),and \\(P \\rightarrow Q\\) is strutting with a value of \\(1\\)too, it can only mean one thing: \\(P \\land 1 \\leq Q\\). And when we crunch the numbers, \\(1 \\land 1 \\leq Q\\), it's crystal clear—\\(Q\\) has got to be \\(1\\)too!</p>\r\n<p>So here's the deal: If a formula can be deduced from the laws of intuitionistic logic using our trusty modus ponens rule, it's a winner in all Heyting algebras. No matter how you assign those variable values, it'll always score a perfect \\(1\\).Talk about consistency!</p>\r\n<p>We can whip up a Heyting algebra where Peirce's law doesn't always hit that \\(1\\)mark. Let's take a peek at a 3-element algebra \\(\\{0, \\frac{1}{2}, 1\\}\\). If we play around and give \\(P\\) a value of \\(\\frac{1}{2}\\) and \\(Q\\) a sad \\(0\\),the value of Peirce's law \\(((P \\rightarrow Q) \\rightarrow P) \\rightarrow P\\) turns out to be \\(\\frac{1}{2}\\). Peirce's law (excluded middle via conditionals) just can't quite make the intuitionistic cut.</p>\r\n<p>But fear not. There's a flip side to this story. If a formula always takes a value of \\(1\\),it can be deduced from the laws of intuitionistic logic. The intuitionistically valid formulas always have that \\(1\\)value. It's just like those classically valid formulas that score a perfect \\(1\\)in the two-element Boolean algebra, no matter what true and false you throw their way.</p>\r\n<p>So, my friends, a Heyting algebra is like a wild and wonderful playground for logic. It's a grand generalization of the usual truth values system, and the biggest element in the algebra is like the ultimate \"true.\" And guess what? Our usual two-valued logic system is just a special case of a Heyting algebra—the baby version, where we only have \\(1\\) (true) and \\(0\\) (false) hanging out.</p>\r\n<p>Let’s take a turn from the big boys and churn out another spindle on the so-called Computational Trilogy.</p>\r\n\r\n\r\n<h2 id=\"topologize-this\">Topologize This</h2>\r\n\r\n<p>The question now: how do elementary topoi relate to Heyting algebras? Here is my transposition of knowledge dependencies—mostly from Wikipedia.\r\n    The following sparse, definitional style is one of many manners for illustrating how these concepts hang together.\r\n    As a preliminary, some chapter zero jargon, (monics are injective homomorphisms—injective meaning all inputs used \\( \\forall x \\exists f(x) \\)—homomorphisms satisfying \\( f(ab) = f(a)f(b) \\)) and isomorphisms. That being said, there are several layers of definitions to peel away, at least to recall: global element, and subobject classifier, and elementary topos. With that out of the way: recall, the global element of an object \\(A\\) from a category is a morphism \\(h:1 \\rightarrow A\\), where \\(1\\) is a terminal object of the category. (recall: a terminal object is the object of the category that is the target of morphisms from all objects in the category) Also recall, the subobject classifier relies on two other definitions, at least: subobject & pullback. \\(A\\) subobject is analogous to a subset, but formally: Some object \\(A\\) in some category has two monics denoted \\(u:S \\rightarrow A\\) and \\(v:T \\rightarrow A\\) (satisfying an equivalence relation \\(uRv\\) with isomorphism \\(f:S \\rightarrow T\\) where \\(u = v \\circ f\\)), and the subobjects of \\(A\\) are the equivalence classes of those monics that can be expressed as the equalizer of two morphisms. An equalizer is analogous to the set-theoretic notion: let \\(X\\) and \\(Y\\) be sets. Let \\(f\\) and \\(g\\) be functions, both from \\(X\\) to \\(Y\\). Then the equalizer of \\(f\\) and \\(g\\) is the set of elements \\(x\\) of \\(X\\) such that \\(f(x)\\) equals \\(g(x)\\) in \\(Y\\). This is also denoted \\(Eq(f,g) := \\{x \\in X \\mid f(x) = g(x)\\}\\). This generalizes to any set of functions \\(f,g \\in F\\), but is degenerate (e.g., singletons and empties). Categorically, the equalizer is the limit of the diagram (note: a diagram is analogous to an indexed family of sets, except morphisms are indexed too, i.e., a diagram with shape \\(J\\) in category \\(C\\) is a functor \\(F:J \\rightarrow C\\) for morphisms \\(f,g:X \\rightarrow Y\\)). The limit is a cone (note: a cone to \\(F\\) is an object \\(N\\) of \\(C\\) together with a family \\(p(X):N \\rightarrow F(X)\\) of morphisms indexed by objects \\(X\\) of \\(J\\) such that every morphism \\(f:X \\rightarrow Y\\) in \\(J\\) requires \\(F(f) \\circ p(X) = p(Y)\\)) where this specific cone is denoted \\((L,t)\\) to \\(F:J \\rightarrow C\\) such that for every other cone denoted \\((N,p)\\) to \\(F:J \\rightarrow C\\) there exists a unique morphism \\(u:N \\rightarrow L\\) such that \\(t(X) \\circ u = p(X)\\) for all \\(X\\) in \\(J\\).</p>\r\n    <img alt=\"\" src=\"images/image1.png\" style=\"width: 40%; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);\" title=\"\">\r\n    <p>Source: <a href=\"https://en.wikipedia.org/wiki/Limit_(category_theory)\">Wikipedia Limit (category theory)</a></p>\r\n\r\n<p>Now, just as \"the equalizer is the limit of the diagram,\" and every \"subobjects of some object are the equivalence classes of those monics that can be expressed as the equalizer of two morphisms\": for each monic \\(j:U \\rightarrow\r\n\r\n X\\) there is a unique morphism \\(h(j):X \\rightarrow \\Omega\\) where \\(\\Omega\\) is the subobject classifier for some category \\(C\\) with a terminal object satisfying \\(1 \\rightarrow \\Omega\\) in the commutative diagram of a pullback such that U is the limit.</p>\r\n <img alt=\"\" src=\"images/image3.png\" style=\"width: 15%; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);\" title=\"\">\r\n<p>Source: <a href=\"https://en.wikipedia.org/wiki/Subobject_classifier\">Wikipedia Subobject classifier</a></p>\r\n\r\n<p>At risk of redundancy, a pullback is the limit of a diagram with two morphisms \\(f:X \\rightarrow Z\\) and \\(g:Y \\rightarrow Z\\) where an object P and two morphisms \\(p_1:P \\rightarrow X\\) and \\(p_2:P \\rightarrow Y\\) where \\(fp_1 = gp_2\\), all denoted \\((P, p_1, p_2)\\), which is a universal property, meaning for any other triple denoted \\((Q, q_1, q_2)\\) where \\(q_1:Q \\rightarrow X\\) and \\(q_2:Q \\rightarrow Y\\) and \\(fq_1 = gq_2\\) there exists a unique \\(u:Q \\rightarrow P\\) such that \\(p_1 \\circ u = q_1\\) and \\(p_2 \\circ u = q_2\\). This is unique up to isomorphism.</p>\r\n<img alt=\"\" src=\"images/image4.png\" style=\"width: 40%; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);\" title=\"\">\r\n<p>Source: <a href=\"https://en.wikipedia.org/wiki/Pullback_(category_theory)\">Wikipedia Pullback (category theory)</a></p>\r\n\r\n<p>Finally, elementary topoi have many definitions. (To avoid beginning the question, this definition allows one to derive the existence of the subobject classifier rather than positing the subobject classifier to derive other properties.) \\(A\\) topos is a category with two properties: (1) every limit over a finite index category exists, (2) every object has a power object. \\(A\\) power object of an object \\(X\\) is a pair \\((P_X, \\ni_x)\\) with \\(\\ni_x \\subseteq P_X \\times X\\). Now, for every object I, a morphism \\(r:I \\rightarrow P_X\\) induces a subobject \\(\\{(i, x) \\mid x \\in r(i)\\} \\subseteq I \\times X\\). \\(A\\) pullback denoted \\(\\ni_x\\) defines this along \\(r \\times X : I \\times X \\rightarrow P_X \\times X\\).</p>\r\n\r\n<p>So, an isomorphic correspondence between relations \\(R \\subseteq I \\times X\\) and morphisms \\(r: I \\rightarrow P_X\\) is the universal property of a power object. From finite limits and power objects one can derive that:</p>\r\n<ol>\r\n<li>All colimits taken over finite index categories exist.</li>\r\n<li>The category has a subobject classifier.</li>\r\n<li>The category is Cartesian closed.</li>\r\n</ol>\r\n<p>A category \\(C\\) is Cartesian closed if it:</p>\r\n<ol>\r\n<li>Has a terminal object.</li>\r\n<li>Any two objects \\(X\\) and \\(Y\\) in \\(C\\) have a product \\(X \\times Y\\) in \\(C\\).</li>\r\n<li>Any two objects \\(Z\\) and \\(Y\\) in \\(C\\) have an exponential denoted \\(Z(Y)\\) in \\(C\\).</li>\r\n</ol>\r\n<p>Recall: an exponential is an object \\(Z(Y)\\) with a morphism \\((Z(Y) \\times Y) \\rightarrow Z\\) if for any object \\(X\\) and morphism \\(g: X \\times Y \\rightarrow Z\\) there is a unique morphism \\(\\lambda g: X \\rightarrow Z(Y)\\), and thus an isomorphism \\(g \\cong \\lambda g\\) often denoted \\(\\text{Hom}(X \\times Y, Z) \\cong \\text{Hom}(X, Z(Y))\\).</p>\r\n<img alt=\"\" src=\"images/image5.png\" style=\"width: 40%; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);\" title=\"\">\r\n<p>Source: <a href=\"https://en.wikipedia.org/wiki/Exponential_object\">Wikipedia Exponential object</a></p>\r\n<p>With all of this, we can see that the global elements of the subobject classifier denoted \\(\\Omega\\) of an elementary topos form a Heyting algebra (of truth values of the intuitionistic higher-order logic induced by the topos). More generally, the set of subobjects of any object \\(X\\) in a topos forms a Heyting algebra.</p>\r\n<p>The question: how to relate \"(1) every limit over a finite index category exists, (2) every object has a power object\" of my definition for elementary topos with the lattices and logical connectives described for Heyting algebras. Well, it is just a list of verifiable algebraic properties according to my GPT query for Lean code, for explications of tactics, and for a natural language explanation. Here's a list:</p>\r\n<ol>\r\n<li>Limits of finite diagrams exist in the category  \\(C\\) .</li>\r\n<li>Pullbacks exist in the category  \\(C\\) .</li>\r\n<li>Products exist in the category  \\(C\\) .</li>\r\n<li>Equalizers exist in the category  \\(C\\) .</li>\r\n<li>The infimum of subobjects  \\(P\\)  and  \\(Q\\)  is lower or equal to  \\(P\\) .</li>\r\n<li>If a subobject  \\(R\\)  is lower or equal to both  \\(P\\)  and  \\(Q\\) , it is also lower or equal to their infimum.</li>\r\n<li>There is an equivalence between \\(a ≤ b\\) and the implication \\(a ⊓ c ≤ b\\) for any subobject  \\(C\\) .</li>\r\n<li>The infimum of subobjects  \\(P\\)  and  \\(Q\\)  is lower or equal to  \\(Q\\) .</li>\r\n<li>Subobject  \\(P\\)  is lower or equal to the supremum of  \\(P\\)  and  \\(Q\\) .</li>\r\n<li>Subobject  \\(Q\\)  is lower or equal to the supremum of  \\(P\\)  and  \\(Q\\) .</li>\r\n<li>If a subobject  \\(R\\)  is greater or equal to both  \\(P\\)  and  \\(Q\\) , it is also greater or equal to their supremum.</li>\r\n</ol>\r\n\r\n\r\n<h2 id=\"adjunction-boolean\">Adjunction Boolean</h2>\r\n\r\n<p>(See nLab <a href=\"https://ncatlab.org/nlab/show/Heyting+algebra\">ʜᴇʏᴛɪɴɢ ᴀʟɢᴇʙʀᴀ</a> for source)</p>\r\n<div class=\"card\"><div class=\"card-body\">\r\n<p>There are several ways of passing back and forth between Boolean algebras and ʜᴇʏᴛɪɴɢ ᴀʟɢᴇʙʀʌs, having to do with the double negation operator. Recall, double negation \\(¬¬: L → L\\) is a monad. Further, it preserves finite meets.</p>\r\n<p>Now let \\(L(\\neg\\neg)\\) denote the poset of regular elements of \\(L\\), that is, those elements \\(x\\) such that \\(\\neg\\neg x = x\\). (When \\(L\\) is the topology of a space, an open set \\(U\\) is regular if and only if it is the interior of its closure, that is if it a regular element of the ʜᴇʏᴛɪɴɢ ᴀʟɢᴇʙʀʌ of open sets described above.)</p>\r\n<p>A Theorem: The poset \\(L(\\neg\\neg)\\) is a Boolean algebra. Moreover, the assignment \\(L \\mapsto L(\\neg\\neg)\\) is the object part of a functor \\(F: \\text{Heyt} \\to \\text{Bool}\\) called Booleanization, which is left adjoint to the full and faithful inclusion \\(i: \\text{Bool} \\rightarrow \\text{Heyt}\\)</p>\r\n<p>The unit of the adjunction, applied to a ʜᴇʏᴛɪɴɢ ᴀʟɢᴇʙʀʌ \\(L\\), is the map \\(L \\to L(\\neg\\neg)\\) which maps each element \\(x\\) to its regularization \\(\\neg\\neg x\\).</p>\r\n<p><em>Proof.</em> To avoid confusion from two different maps called \\(\\neg\\neg\\) that differ only in their codomain, write \\(M: L \\to L\\) for the monad and \\(U: L \\to L(\\neg\\neg)\\) for the left adjoint. Write \\(\\iota: L(\\neg\\neg) \\to L\\) for the right adjoint, so that \\(M = \\iota \\circ U\\).</p>\r\n<p>We first show that \\(L(\\neg\\neg)\\) is a ʜᴇʏᴛɪɴɢ ᴀʟɢᴇʙʌ and \\(U\\) is a Heyting-algebra map. Because \\(U\\) is surjective (and monotone), it suffices to show that it preserves the Heyting-algebra operators: finite meets, finite joins, and implication.</p>\r\n<p>Because \\(\\iota\\) is full, it reflects meets. Therefore, because \\(\\neg\\neg: L \\to L\\) preserves finite meets, \\(M = \\iota \\circ U\\) preserves finite meets, so too does \\(U\\), and as a left adjoint, it preserves joins.</p>\r\n<p>Finally, for \\(a, b \\in L\\), we show that \\(\\neg\\neg(a \\to b)\\), which (because \\(\\neg\\neg: L \\to L\\) preserves finite meets) is the \\(L\\)-implication \\(\\neg\\neg a \\to \\neg\\neg b\\), satisfies the universal property (\\(1\\)), also in \\(L(\\neg\\neg)\\). For any \\(x \\in L(\\neg\\neg)\\), \\(\\iota(x) \\leq \\neg\\neg a \\to_\\mathrm{L} \\neg\\neg b\\) just if \\(\\iota(x) \\land_\\mathrm{L} \\neg\\neg a \\leq \\neg\\neg b\\) by the universal property in \\(L\\); but because \\(\\iota\\) reflects meets, this is equivalent to \\(x \\land_\\mathrm{L(\\neg\\neg)} (\\neg\\neg a) \\leq \\neg\\neg b\\), completing the universal property.</p>\r\n<p>Now because \\(U\\) preserves implication and (the empty join) \\(0\\), it preserves negation. Therefore, \\(\\neg\\neg\\) in \\(L(\\neg\\neg)\\) is the identity, so the latter is a Boolean algebra.</p>\r\n<p>Therefore \\(U = \\neg\\neg: L \\to L(\\neg\\neg)\\) is a ʜᴇʏᴛɪɴɢ ᴀʟɢᴇʙʌ quotient which is the coequalizer of \\(1\\), \\(\\neg\\neg: L \\to L\\). It follows that a ʜᴇʏᴛɪɴɢ ᴀʟɢᴇʙʌ map \\(L \\to B\\) to any Boolean algebra \\(B\\), i.e., any ʜᴇʏᴛɪɴɢ ᴀʟɢᴇʙʌ where \\(1\\) and \\(\\neg\\neg\\) coincide, factors uniquely through this coequalizer, and the induced map \\(L(\\neg\\neg) \\to B\\) is a Boolean algebra map. In other words, \\(\\neg\\neg: L \\to L(\\neg\\neg)\\) is the universal ʜᴇʏᴛɪɴɢ ᴀʟɢᴇʙʌ map to a Boolean algebra, which establishes the adjunction.</p>\r\n</div></div><br>\r\n\r\n<h2 id=\"quantifiers-as-adjoints\">Quantifiers as Adjoints</h2>\r\n<p>\r\n  (See\r\n  <a href=\"https://www.google.com/url?q=https://math.stackexchange.com/q/452795/1098426&amp;sa=D&amp;source=editors&amp;ust=1688321707808674&amp;usg=AOvVaw2DDZ2-k228Ur5-IiVPwFxy\">\"Quantifiers as Adjoints...\". Math Stack Exchange</a>\r\n  for source)\r\n</p>\r\n<div class=\"card\"><div class=\"card-body\">\r\n<p>\r\n  What is a predicate \\(P(x)\\), with the variable \\(x\\) coming from the sort \\(X\\)? Viewing \\(X\\) as a set, we can interpret \\(P(x)\\) as a subset of \\(X\\). The definable subsets of \\(X\\) naturally form a partially ordered category, where an arrow \\(P(x) \\rightarrow Q(x)\\) exists if and only if \\(P \\subseteq Q\\). This category structure can also be seen as the partial order of implications between predicates. Let's denote this category as Pred(\\(X\\)).\r\n</p>\r\n<p>\r\n  Now, there is a functor \\(Dy: \\text{Pred}(X) \\rightarrow \\text{Pred}(X \\times Y)\\) that corresponds to adding a dummy variable \\(y\\) of type \\(Y\\). So, \\(Dy(P(x)) = \\{(x, y) \\in X \\times Y \\mid x \\in P(x)\\}\\). We can write this as \\(P(x, y)\\), where the dot denotes that \\(y\\) is a dummy variable. This is indeed a functor, as if \\(P(x) \\rightarrow Q(x)\\), then \\(P(x, y) \\rightarrow Q(x, y)\\) (again, you can interpret this as containment or implication).\r\n</p>\r\n<p>\r\n  There are also functors \\(\\exists y: \\text{Pred}(X \\times Y) \\rightarrow \\text{Pred}(X)\\) and \\(\\forall y: \\text{Pred}(X \\times Y) \\rightarrow \\text{Pred}(X)\\) that behave as follows:\r\n</p>\r\n<p>\r\n  - \\(\\exists yP(x, y) = \\{x \\in X \\mid \\exists y(x, y) \\in P(x, y)\\}\\).\r\n</p>\r\n<p>\r\n  - \\(\\forall yP(x, y) = \\{x \\in X \\mid \\forall y(x, y) \\in P(x, y)\\}\\).\r\n</p>\r\n<p>\r\n  Now, we have the adjunctions \\(\\exists y \\dashv Dy \\dashv \\forall y\\). These adjunctions correspond to the following statements:\r\n</p>\r\n<p>\r\n  - \\(\\exists yP(x, y) \\rightarrow Q(x) \\iff P(x, y) \\rightarrow Q(x, y)\\).\r\n</p>\r\n<p>\r\n  - \\(P(x) \\rightarrow \\forall yQ(x, y) \\iff P(x, y) \\rightarrow Q(x, y)\\).\r\n</p>\r\n<p>\r\n  Moving on to viewing predicates as maps \\(X \\rightarrow \\{0,1\\}\\), we can define a corresponding partial order structure on maps \\(X \\rightarrow \\{0,1\\}\\). Specifically, \\(P(x) \\rightarrow Q(x)\\) if and only if for all \\(x \\in X\\), \\(P(x) \\leq Q(x)\\). In other words, the partial order is defined by pointwise comparison of functions.\r\n</p>\r\n<p>\r\n  The functor \\(Dy\\) now takes a map \\(X \\rightarrow \\{0,1\\}\\) and composes it with the projection \\(X \\times Y \\rightarrow X\\), and its adjoints are given by:\r\n</p>\r\n<p>\r\n  - \\((\\exists yP(x, y))(x) = \\{1 \\text{ if } \\exists yP(x, y) = 1, 0 \\text{ otherwise}\\} = \\sup_{y \\in Y} P(x, y)\\).\r\n</p>\r\n<p>\r\n  - \\((\\forall yP(x, y))(x) = \\{1 \\text{ if } \\forall yP(x, y) = 1, 0 \\text{ otherwise}\\} = \\inf_{y \\in Y} P(x, y)\\).\r\n</p>\r\n<p>\r\n  By replacing \\(\\{0,1\\}\\) with \\([0,1]\\), we can generalize this concept further. We take the same category structure on predicates (pointwise \\(\\leq\\) in \\([0,1]\\)) and obtain the quantifiers in continuous model theory as adjoints to the \"dummy variable functor,\" replacing max and min with sup and inf.\r\n</p>\r\n<p>\r\n  Regarding your second question, if the compact Hausdorff space is not ordered, it is not clear what the appropriate category structure should be since the one where the quantifiers naturally appear stems directly from the order. However, it is easier to generalize this situation by replacing \\(\\{0,1\\}\\) and \\([0,1]\\) with any complete lattice, which is what happens in topos theory.\r\n</p>\r\n</div></div><br>\r\n\r\n<h2 id=\"topoi-of-heyting-algebra-models\">Topoi of ʜᴇʏᴛɪɴɢ ᴀʟɢᴇʙʀᴀ Models</h2>\r\n<p>(See <a href=\"https://www.google.com/url?q=https://math.stackexchange.com/q/4506152/1098426&amp;sa=D&amp;source=editors&amp;ust=1688321707815342&amp;usg=AOvVaw31M2RPlEaOas6XPwEkKAzC\">&quot;Is there a topos of Heyting...&quot;,Math Stack Exchange</a> for source)</p>\r\n<div class=\"card\"><div class=\"card-body\">\r\n    \r\n    <p>Let \\(Heyt\\) be the category of Heyting algebras. We first describe a functor \\(F: Set^{op} \\rightarrow Heyt\\). This functor is simply the functor sending \\(X\\) to \\(H^X\\). In fact, \\(F\\) always outputs a complete Heyting algebra since \\(H\\) is complete.</p>\r\n    <p>Let us note that for all \\(f: A \\rightarrow B\\), the map \\(Ff: FB \\rightarrow FA\\) preserves all meets and all joins. Therefore, \\(Ff\\) has a left adjoint \\(\\exists f\\) and a right adjoint \\(\\forall f: FA \\rightarrow FB\\). Moreover, these adjoints satisfy the Beck-Chevalley condition. This gives us a \"first-order hyperdoctrine with equality\".</p>\r\n    <p>From here, we can construct the category of partial equivalence relations.</p>\r\n    <p>In a bit more generality, consider a category \\(C\\) with finite limits and a functor \\(F: C^{op} \\rightarrow Heyt\\), where for all \\(f: A \\rightarrow B\\), \\(Ff\\) has a left and a right adjoint which satisfy the Beck-Chevalley condition. Such a functor is known as a first-order hyperdoctrine with equality. We write \\(f^{-1}\\) instead of \\(Ff\\).</p>\r\n    <p>A partial equivalence relation consists of an object \\(A \\in C\\), together with some predicate \\(P \\in F(A \\times A)\\), which satisfies the following conditions:</p>\r\n    <p>1. Consider the \"swap map\" \\(swap = (p2, p1): A^2 \\rightarrow A^2\\). Then \\(swap^{-1}(P) = P\\). This is \"symmetry\". It corresponds to saying \\(\\forall x, y \\in A (P(x, y) \\iff P(y, x))\\).</p>\r\n    <p>2. Consider the three maps \\((p1, p2), (p1, p3), (p2, p3): A^3 \\rightarrow A^2\\). We have \\((p1, p2)^{-1}(P) \\land (p2, p3)^{-1}(P) \\leq (p1, p3)^{-1}(P)\\). This is \"transitivity\". It corresponds to saying \\(\\forall x, y, z \\in A (P(x, y) \\land P(y, z) \\rightarrow P(x, z))\\).</p>\r\n    <p>The arrows from \\((A, P)\\) to \\((B, Q)\\) will consist of \\(f \\in F(A \\times B)\\) which satisfies the following conditions:</p>\r\n    <p>1. Consider \\(p1: A \\times B \\rightarrow A\\). Then \\(\\exists p1f = P\\). This is \"having the correct domain\" - it corresponds to saying \\(\\forall x \\in A (P(x) \\rightarrow \\exists y \\in B (f(x, y)))\\).</p>\r\n    <p>\r\n        2. Consider \\(p_2: A \\times B \\rightarrow B\\). Then \\(\\exists p_2f \\leq Q\\). This is \"having the correct codomain\". It corresponds to saying \\(\\forall y \\in B (\\exists x \\in A (f(x, y)) \\rightarrow Q(y, y))\\).\r\n        </p>\r\n        <p>\r\n        3. Consider \\((p_1, p_2), (p_1, p_3): A \\times B^2 \\rightarrow A \\times B\\) and \\((p_2, p_3): A \\times B^2 \\rightarrow B^2\\). Then \\((p_1, p_2)^{-1}f \\land (p_1, p_3)^{-1}f \\leq (p_2, p_3)^{-1}(Q)\\). This is known as \"being single-valued\" and corresponds to saying \\(\\forall x \\in A \\forall y, z \\in B (f(x, y) \\land f(x, z) \\rightarrow Q(y, z))\\).\r\n        </p>\r\n        <p>\r\n        Using these tools, we can fairly easily define composition. Given \\(f: (A, P) \\rightarrow (B, Q)\\) and \\(g: (B, Q) \\rightarrow (C, R)\\), we can define \\(g \\circ f: (A, P) \\rightarrow (C, R)\\) as follows. We have\r\n        </p>\r\n        <p>\r\n        &nbsp;&nbsp;maps \\((p_1, p_2): A \\times B \\times C \\rightarrow A \\times B\\), \\((p_1, p_3): A \\times B \\times C \\rightarrow A \\times C\\), and \\((p_2, p_3): A \\times B \\times C \\rightarrow B \\times C\\). Then \\(g \\circ f = \\exists (p_1, p_2)((p_1, p_2)^{-1}(f) \\land (p_2, p_3)^{-1}(g))\\). This corresponds to defining \\(g \\circ f = \\{(x, z) \\in A \\times C | \\exists y \\in B (f(x, y) \\land g(y, z))\\}\\).\r\n        </p>\r\n        <p>\r\n        One can easily show that this gives us a category. In fact, it gives us a Heyting category - a category in which we can interpret first-order intuitionist logic. This type of category has finite limits, an initial object, Heyting algebra structures for subobjects, and left and right adjoints to pullbacks for monos.\r\n        </p>\r\n        <p>\r\n        We now add one final condition on \\(F\\). The condition is:\r\n        </p>\r\n        <p>\r\n        For every object \\(X\\) of \\(C\\), there exists an object \\(PX\\) and an element \\(in_X \\in F(X \\times PX)\\) such that for all \\(Y \\in C\\) and \\(Q \\in F(X \\times Y)\\), there exists some (not necessarily unique) \\(f: Y \\rightarrow PX\\) with \\((1_X \\times f)^{-1}(in_X) = Q\\).\r\n        </p>\r\n        <p>\r\n        This makes \\(F\\) into a \"tripos\". In the particular case \\(FX = HX\\), we see that we can construct \\(P(X) = H^X\\) with \\(in_X: X \\times HX \\rightarrow H\\) being the obvious map.\r\n        </p>\r\n        <p>\r\n        Using the tripos condition, we can prove that the resulting category of partial equivalence relations is\r\n        \r\n        , in fact, a topos. Finally, we have the following theorem:\r\n        </p>\r\n        <p>\r\n        Thm: The tripos-to-topos construction applied to the functor \\(X \\mapsto HX\\) produces a topos which is equivalent to \\(Sh(H)\\).\r\n        </p>\r\n        <p>\r\n        So the \"correct\" way of constructing \\(H\\)-valued types just gives us the topos of sheaves on the locale \\(H\\).\r\n        </p>\r\n        <p>\r\n        There are a few other interesting ways of getting triposes. The first is the \"syntactic tripos\". Let's say we're working with intuitionist higher-order logic. Then let \\(C\\) be the category of types, where the objects are (products of) types, the arrows are function terms, and \\(F(T)\\) is the Heyting algebra of predicates on \\(T\\). The tripos-to-topos construction then produces a topos where the true statements in the internal logic exactly correspond to provable statements.\r\n        </p>\r\n        <p>\r\n        The second is, of course, starting with \\(C\\) being the topos and taking the sub-object functor. The tripos-to-topos construction will simply produce a topos which is equivalent to \\(C\\) - this shouldn't be terribly surprising.\r\n        </p>\r\n        <p>\r\n        The third is using a partial combinatory algebra. This is related to Kleene's realizability models for arithmetic and can be used to construct a topos where, for example, we can show in the internal logic that all functions \\(N \\rightarrow N\\) are computable. It can also be used to model Brouwer's intuitionism.\r\n        </p>\r\n        \r\n\r\n</div></div><br>\r\n</div>\r\n<!-- <footer  class=\"bg-light pt-5 pb-2 px-5\"><a href=\"/\">Home</a><hr><p>Copyright © 2023 Izak</p></footer> -->\r\n\r\n<!--HONESTLY, I DO NOT KNOW WHICH OF THESE SCRIPTS ARE NECESSARY DEPENDENCIES-->\r\n<script src=\"https://code.jquery.com/jquery-3.4.1.min.js\"></script>\r\n<script src=\"https://code.jquery.com/ui/1.12.1/jquery-ui.js\"></script>\r\n<script src=\"https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js\" integrity=\"sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo\" crossorigin=\"anonymous\"></script>\r\n<script src=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js\" integrity=\"sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6\" crossorigin=\"anonymous\"></script>\r\n</body>\r\n</html>",
      "date_published": "2023-04-30T17:00:00-07:00"
    },{
      "id": "https://ischmidls.github.io/posts/pages/translate/",
      "url": "https://ischmidls.github.io/posts/pages/translate/",
      "title": "translate",
      "content_html": "<!DOCTYPE html>\r\n<html>\r\n<head>\r\n  <title>Language Translation</title>\r\n</head>\r\n<body>\r\n  <label for=\"input-text\">Enter text to translate:</label>\r\n  <textarea id=\"input-text\" rows=\"4\" cols=\"50\"></textarea><br><br>\r\n  <button onclick=\"strTranslate()\">Translate</button>\r\n  <h2>Translations:</h2>\r\n<table>\r\n    <tr>\r\n      <th><span>German:</span></th>\r\n      <th><span>French:</span></th>\r\n      <th><span>Japanese:</span></th>\r\n      <th><span>Russian:</span></th>\r\n      <th><span>Arabic:</span></th>\r\n      <th><span>Mandarin:</span></th>\r\n      <th><span>Spanish:</span></th>\r\n      <th><span>Italian:</span></th>\r\n    </tr>\r\n    <tr>\r\n      <td><span id=\"output-de\"></span></td>\r\n      <td><span id=\"output-fr\"></span></td>\r\n      <td><span id=\"output-ja\"></span></td>\r\n      <td><span id=\"output-ru\"></span></td>\r\n      <td><span id=\"output-ar\"></span></td>\r\n      <td><span id=\"output-zh\"></span></td>\r\n      <td><span id=\"output-es\"></span></td>\r\n      <td><span id=\"output-it\"></span></td>\r\n    </tr>\r\n  </table>\r\n\r\n  <script>\r\n    function strTranslate() {\r\n      const input = document.getElementById(\"input-text\").value;\r\n\r\n      // German\r\n      fetch(`https://translate.googleapis.com/translate_a/single?client=gtx&sl=auto&tl=de&dt=t&q=${encodeURIComponent(input)}`)\r\n        .then(response => response.json())\r\n        .then(data => document.getElementById(\"output-de\").innerHTML = data[0][0][0]);\r\n\r\n      // French\r\n      fetch(`https://translate.googleapis.com/translate_a/single?client=gtx&sl=auto&tl=fr&dt=t&q=${encodeURIComponent(input)}`)\r\n        .then(response => response.json())\r\n        .then(data => document.getElementById(\"output-fr\").innerHTML = data[0][0][0]);\r\n\r\n      // Japanese\r\n      fetch(`https://translate.googleapis.com/translate_a/single?client=gtx&sl=auto&tl=ja&dt=t&q=${encodeURIComponent(input)}`)\r\n        .then(response => response.json())\r\n        .then(data => document.getElementById(\"output-ja\").innerHTML = data[0][0][0]);\r\n\r\n      // Russian\r\n      fetch(`https://translate.googleapis.com/translate_a/single?client=gtx&sl=auto&tl=ru&dt=t&q=${encodeURIComponent(input)}`)\r\n        .then(response => response.json())\r\n        .then(data => document.getElementById(\"output-ru\").innerHTML = data[0][0][0]);\r\n      // Arabic\r\n      fetch(`https://translate.googleapis.com/translate_a/single?client=gtx&sl=auto&tl=ar&dt=t&q=${encodeURIComponent(input)}`)\r\n        .then(response => response.json())\r\n        .then(data => document.getElementById(\"output-ar\").innerHTML = data[0][0][0]);\r\n\r\n      // Mandarin\r\n      fetch(`https://translate.googleapis.com/translate_a/single?client=gtx&sl=auto&tl=zh&dt=t&q=${encodeURIComponent(input)}`)\r\n        .then(response => response.json())\r\n        .then(data => document.getElementById(\"output-zh\").innerHTML = data[0][0][0]);\r\n\r\n      // Spanish\r\n      fetch(`https://translate.googleapis.com/translate_a/single?client=gtx&sl=auto&tl=es&dt=t&q=${encodeURIComponent(input)}`)\r\n        .then(response => response.json())\r\n        .then(data => document.getElementById(\"output-es\").innerHTML = data[0][0][0]);\r\n      \r\n      // Italian\r\n      fetch(`https://translate.googleapis.com/translate_a/single?client=gtx&sl=auto&tl=it&dt=t&q=${encodeURIComponent(input)}`)\r\n        .then(response => response.json())\r\n        .then(data => document.getElementById(\"output-it\").innerHTML = data[0][0][0]);\r\n    }\r\n  </script>\r\n</body>\r\n</html>\r\n",
      "date_published": "2023-01-31T16:00:00-08:00"
    },{
      "id": "https://ischmidls.github.io/posts/pages/occurcount/",
      "url": "https://ischmidls.github.io/posts/pages/occurcount/",
      "title": "occurance count",
      "content_html": "<!doctype html>\r\n<html lang=\"en\" dir=\"ltr\">\r\n  <head>\r\n    <meta charset=\"utf-8\">\r\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\r\n    <link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css\" integrity=\"sha384-B0vP5xmATw1+K9KRQjQERJvTumQW0nPEzvF6L/Z6nronJ3oUOFUFpCjEUQouq2+l\" crossorigin=\"anonymous\">\r\n    <!--<link rel=\"stylesheet\" href=\"css/main.css\">-->\r\n    \r\n        <title>Word Count</title>\r\n    <script src=\"https://cdn.plot.ly/plotly-2.20.0.min.js\"></script>\r\n    <script>\r\n      function countWords() {\r\n        // Get the input string\r\n        var inputString = document.getElementById(\"inputString\").value.toLowerCase();\r\n        // Split the input string into an array of words\r\n        var wordsArray = inputString.match(/[a-zA-Z0-9]+/g);\r\n        // Create an empty object to hold the word counts\r\n        var wordCounts = {};\r\n        // Loop through the words and count their occurrences\r\n        if (wordsArray === null) {\r\n          alert(\"Silly, you didn't put any words in!\")\r\n          throw new Error(\"user gave no words\")\r\n        }\r\n        for (var i = 0; i < wordsArray.length; i++) {\r\n          if (wordCounts[wordsArray[i]]) {\r\n            wordCounts[wordsArray[i]]++;\r\n          } else {\r\n            wordCounts[wordsArray[i]] = 1;\r\n          }\r\n        }\r\n        // Convert the word counts object into an array of pairs\r\n        var pairsArray = [];\r\n        for (var word in wordCounts) {\r\n          pairsArray.push([word, wordCounts[word]]);\r\n        }\r\n        // Return the array of pairs\r\n        return pairsArray;\r\n      }\r\n\r\n      function displayWordCounts() {\r\n        // Call the countWords function and get the array of pairs\r\n        try {\r\n          var pairsArray = countWords();\r\n        }\r\n        catch(err) { console.log(err.message)}\r\n        // Get the output element\r\n        var outputElement = document.getElementById(\"output\");\r\n        // Clear the output element\r\n        outputElement.innerHTML = \"<h2>Counts</h2>\";\r\n        // Loop through the pairs array and add each pair to the output element\r\n        for (var i = 0; i < pairsArray.length; i++) {\r\n          var pair = pairsArray[i];\r\n          var word = pair[0];\r\n          var count = pair[1];\r\n          outputElement.innerHTML += word + \": \" + count + \" <br> \";\r\n        }\r\n        displayPlotlyPairs(pairsArray);\r\n      }\r\n\r\n      function displayPlotlyPairs(pairsArray) {\r\n        // Extract the words and counts from the pairs array\r\n        var words = pairsArray.map(pair => pair[0]);\r\n        var counts = pairsArray.map(pair => pair[1]);\r\n        // Create the trace for the bar chart\r\n        var trace = {\r\n          x: words,\r\n          y: counts,\r\n          type: \"bar\"\r\n        };\r\n        // Create the layout for the chart\r\n        var layout = {\r\n          title: \"Word Frequencies\",\r\n          xaxis: {\r\n            title: \"Words\",\r\n            categoryorder:'total descending'\r\n          },\r\n          yaxis: {\r\n            title: \"Counts\"\r\n          },\r\n          width: 2000,\r\n          height: 1500,\r\n        };\r\n        // Create the data array\r\n        var data = [trace];\r\n        // Display the chart using Plotly.newPlot\r\n        Plotly.newPlot(\"chartDiv\", data, layout);\r\n      }\r\n    </script>\r\n    <style> \r\n    textarea {\r\n      width: 90%;\r\n      height: 80vh;\r\n    }\r\n    #output, #chartDiv {\r\n      outline:auto;\r\n      padding: 3em;\r\n      overflow: auto;\r\n    }\r\n    </style>\r\n  \r\n  </head>\r\n  <body>\r\n    <div class=\"container\" id=\"bsr-wrapper\">\r\n      <h1>Word Count</h1>\r\n    <p>Enter some text:</p>\r\n    <textarea id=\"inputString\"></textarea>\r\n    <br>\r\n    <button onclick=\"displayWordCounts()\">Count Words</button>\r\n    <br>\r\n    <h2>Plot & List!</h2>\r\n    <div id=\"chartDiv\"></div>\r\n    <div id=\"output\"></div>\r\n      \r\n    </div>\r\n    <!-- <footer  class=\"bg-light pt-5 pb-2 px-5\"><a href=\"/\">Home</a></footer> -->\r\n    \r\n    <script src=\"https://code.jquery.com/jquery-3.4.1.min.js\"></script>\r\n    <script src=\"https://code.jquery.com/ui/1.12.1/jquery-ui.js\"></script>\r\n    <script src=\"https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js\" integrity=\"sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo\" crossorigin=\"anonymous\"></script>\r\n    <script src=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js\" integrity=\"sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6\" crossorigin=\"anonymous\"></script>\r\n    <!--<script src=\"js/main.js\"></script>-->\r\n  </body>\r\n</html>\r\n",
      "date_published": "2023-01-31T16:00:00-08:00"
    },{
      "id": "https://ischmidls.github.io/posts/pages/hegel/",
      "url": "https://ischmidls.github.io/posts/pages/hegel/",
      "title": "philosophy word counts",
      "content_html": "<!DOCTYPE html>\r\n<html lang=\"en\">\r\n  <head>\r\n    <meta charset=\"UTF-8\">\r\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\r\n    <meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\">\r\n    <title>Top 20 Word Occurance in...</title>\r\n\r\n  </head>\r\n  <body>\r\n\r\n\r\n<div style=\"background-color: lightcyan;\">\r\n<b>Note from the Author<b>\r\n\r\n<p>These web pages are generated using the *index.py* file.</p>\r\n\r\n<p>The link to the Python code in the repository is <a href=\"https://github.com/ischmidls/ischmidls.github.io/tree/main/pages/hegel/index.py\">Here</a></p>\r\n\r\n<p>The data source <a href=\"https://www.kaggle.com/datasets/kouroshalizadeh/history-of-philosophy\">Here</a></p>\r\n\r\n<p>This started as a project to find the mode of the words in Hegel's Phenomenology of Spirit.</p>\r\n\r\n<p>I realized I could generalize it to the whole dataset. It got out of hand, but I retamed it.</p>\r\n<p>My Python CsvFigs class uses this flow: </p>\r\n\r\n\r\n<p>CSV --> Pandas DataFrame --> Plotly GraphObject --> text file </p>\r\n\r\n<p>For the DataFrame, I use the collections module from the Python standard library to quickly get the counts for each word from the list of sentence strings. </p>\r\n<p>Then, the imported NLTK module determines whether one of the top 20 occuring words is a noun or a verb to color the bar red in Plotly. </p>\r\n\r\n\r\n\r\n</div>\r\n\r\n<h1>Top 20 Word Occurance in...</h1>\r\n<hr>\r\n<ul>\r\n<li><a href=\"../../../source/hegel/KeynesTop 20 Word Occurance in Keynes's A General Theory Of Employment, Interest, And Money.html\" title=\"Keynes Top 20 Word Occurance in Keynes's A General Theory Of Employment, Interest, And Money\">Keynes's Top 20 Word Occurance in Keynes's A General Theory Of Employment, Interest, And Money</a></li>\r\n<li><a href=\"../../../source/hegel/FoucaultTop 20 Word Occurance in Foucault's The Birth Of The Clinic.html\" title=\"Foucault Top 20 Word Occurance in Foucault's The Birth Of The Clinic\">Foucault's Top 20 Word Occurance in Foucault's The Birth Of The Clinic</a></li>\r\n<li><a href=\"../../../source/hegel/WittgensteinTop 20 Word Occurance in Wittgenstein's On Certainty.html\" title=\"Wittgenstein Top 20 Word Occurance in Wittgenstein's On Certainty\">Wittgenstein's Top 20 Word Occurance in Wittgenstein's On Certainty</a></li>\r\n<li><a href=\"../../../source/hegel/DescartesTop 20 Word Occurance in Descartes's Meditations On First Philosophy.html\" title=\"Descartes Top 20 Word Occurance in Descartes's Meditations On First Philosophy\">Descartes's Top 20 Word Occurance in Descartes's Meditations On First Philosophy</a></li>\r\n<li><a href=\"../../../source/hegel/LockeTop 20 Word Occurance in Locke's Essay Concerning Human Understanding.html\" title=\"Locke Top 20 Word Occurance in Locke's Essay Concerning Human Understanding\">Locke's Top 20 Word Occurance in Locke's Essay Concerning Human Understanding</a></li>\r\n<li><a href=\"../../../source/hegel/WittgensteinTop 20 Word Occurance in Wittgenstein's Philosophical Investigations.html\" title=\"Wittgenstein Top 20 Word Occurance in Wittgenstein's Philosophical Investigations\">Wittgenstein's Top 20 Word Occurance in Wittgenstein's Philosophical Investigations</a></li>\r\n<li><a href=\"../../../source/hegel/RicardoTop 20 Word Occurance in Ricardo's On The Principles Of Political Economy And Taxation.html\" title=\"Ricardo Top 20 Word Occurance in Ricardo's On The Principles Of Political Economy And Taxation\">Ricardo's Top 20 Word Occurance in Ricardo's On The Principles Of Political Economy And Taxation</a></li>\r\n<li><a href=\"../../../source/hegel/BeauvoirTop 20 Word Occurance in Beauvoir's The Second Sex.html\" title=\"Beauvoir Top 20 Word Occurance in Beauvoir's The Second Sex\">Beauvoir's Top 20 Word Occurance in Beauvoir's The Second Sex</a></li>\r\n<li><a href=\"../../../source/hegel/DavisTop 20 Word Occurance in Davis's Women, Race, And Class.html\" title=\"Davis Top 20 Word Occurance in Davis's Women, Race, And Class\">Davis's Top 20 Word Occurance in Davis's Women, Race, And Class</a></li>\r\n<li><a href=\"../../../source/hegel/PlatoTop 20 Word Occurance in Plato's Plato - Complete Works.html\" title=\"Plato Top 20 Word Occurance in Plato's Plato - Complete Works\">Plato's Top 20 Word Occurance in Plato's Plato - Complete Works</a></li>\r\n<li><a href=\"../../../source/hegel/NietzscheTop 20 Word Occurance in Nietzsche's Ecce Homo.html\" title=\"Nietzsche Top 20 Word Occurance in Nietzsche's Ecce Homo\">Nietzsche's Top 20 Word Occurance in Nietzsche's Ecce Homo</a></li>\r\n<li><a href=\"../../../source/hegel/Merleau-PontyTop 20 Word Occurance in Merleau-Ponty's The Phenomenology Of Perception.html\" title=\"Merleau-Ponty Top 20 Word Occurance in Merleau-Ponty's The Phenomenology Of Perception\">Merleau-Ponty's Top 20 Word Occurance in Merleau-Ponty's The Phenomenology Of Perception</a></li>\r\n<li><a href=\"../../../source/hegel/HusserlTop 20 Word Occurance in Husserl's The Idea Of Phenomenology.html\" title=\"Husserl Top 20 Word Occurance in Husserl's The Idea Of Phenomenology\">Husserl's Top 20 Word Occurance in Husserl's The Idea Of Phenomenology</a></li>\r\n<li><a href=\"../../../source/hegel/HeideggerTop 20 Word Occurance in Heidegger's Off The Beaten Track.html\" title=\"Heidegger Top 20 Word Occurance in Heidegger's Off The Beaten Track\">Heidegger's Top 20 Word Occurance in Heidegger's Off The Beaten Track</a></li>\r\n<li><a href=\"../../../source/hegel/RussellTop 20 Word Occurance in Russell's The Analysis Of Mind.html\" title=\"Russell Top 20 Word Occurance in Russell's The Analysis Of Mind\">Russell's Top 20 Word Occurance in Russell's The Analysis Of Mind</a></li>\r\n<li><a href=\"../../../source/hegel/FoucaultTop 20 Word Occurance in Foucault's The Order Of Things.html\" title=\"Foucault Top 20 Word Occurance in Foucault's The Order Of Things\">Foucault's Top 20 Word Occurance in Foucault's The Order Of Things</a></li>\r\n<li><a href=\"../../../source/hegel/MarxTop 20 Word Occurance in Marx's Capital.html\" title=\"Marx Top 20 Word Occurance in Marx's Capital\">Marx's Top 20 Word Occurance in Marx's Capital</a></li>\r\n<li><a href=\"../../../source/hegel/NietzscheTop 20 Word Occurance in Nietzsche's Twilight Of The Idols.html\" title=\"Nietzsche Top 20 Word Occurance in Nietzsche's Twilight Of The Idols\">Nietzsche's Top 20 Word Occurance in Nietzsche's Twilight Of The Idols</a></li>\r\n<li><a href=\"../../../source/hegel/DeleuzeTop 20 Word Occurance in Deleuze's Anti-Oedipus.html\" title=\"Deleuze Top 20 Word Occurance in Deleuze's Anti-Oedipus\">Deleuze's Top 20 Word Occurance in Deleuze's Anti-Oedipus</a></li>\r\n<li><a href=\"../../../source/hegel/LewisTop 20 Word Occurance in Lewis's Lewis - Papers.html\" title=\"Lewis Top 20 Word Occurance in Lewis's Lewis - Papers\">Lewis's Top 20 Word Occurance in Lewis's Lewis - Papers</a></li>\r\n<li><a href=\"../../../source/hegel/MalebrancheTop 20 Word Occurance in Malebranche's The Search After Truth.html\" title=\"Malebranche Top 20 Word Occurance in Malebranche's The Search After Truth\">Malebranche's Top 20 Word Occurance in Malebranche's The Search After Truth</a></li>\r\n<li><a href=\"../../../source/hegel/KripkeTop 20 Word Occurance in Kripke's Naming And Necessity.html\" title=\"Kripke Top 20 Word Occurance in Kripke's Naming And Necessity\">Kripke's Top 20 Word Occurance in Kripke's Naming And Necessity</a></li>\r\n<li><a href=\"../../../source/hegel/QuineTop 20 Word Occurance in Quine's Quintessence.html\" title=\"Quine Top 20 Word Occurance in Quine's Quintessence\">Quine's Top 20 Word Occurance in Quine's Quintessence</a></li>\r\n<li><a href=\"../../../source/hegel/KantTop 20 Word Occurance in Kant's Critique Of Practical Reason.html\" title=\"Kant Top 20 Word Occurance in Kant's Critique Of Practical Reason\">Kant's Top 20 Word Occurance in Kant's Critique Of Practical Reason</a></li>\r\n<li><a href=\"../../../source/hegel/EpictetusTop 20 Word Occurance in Epictetus's Enchiridion.html\" title=\"Epictetus Top 20 Word Occurance in Epictetus's Enchiridion\">Epictetus's Top 20 Word Occurance in Epictetus's Enchiridion</a></li>\r\n<li><a href=\"../../../source/hegel/WollstonecraftTop 20 Word Occurance in Wollstonecraft's Vindication Of The Rights Of Woman.html\" title=\"Wollstonecraft Top 20 Word Occurance in Wollstonecraft's Vindication Of The Rights Of Woman\">Wollstonecraft's Top 20 Word Occurance in Wollstonecraft's Vindication Of The Rights Of Woman</a></li>\r\n<li><a href=\"../../../source/hegel/HegelTop 20 Word Occurance in Hegel's Science Of Logic.html\" title=\"Hegel Top 20 Word Occurance in Hegel's Science Of Logic\">Hegel's Top 20 Word Occurance in Hegel's Science Of Logic</a></li>\r\n<li><a href=\"../../../source/hegel/HusserlTop 20 Word Occurance in Husserl's The Crisis Of The European Sciences And Phenomenology.html\" title=\"Husserl Top 20 Word Occurance in Husserl's The Crisis Of The European Sciences And Phenomenology\">Husserl's Top 20 Word Occurance in Husserl's The Crisis Of The European Sciences And Phenomenology</a></li>\r\n<li><a href=\"../../../source/hegel/NietzscheTop 20 Word Occurance in Nietzsche's The Antichrist.html\" title=\"Nietzsche Top 20 Word Occurance in Nietzsche's The Antichrist\">Nietzsche's Top 20 Word Occurance in Nietzsche's The Antichrist</a></li>\r\n<li><a href=\"../../../source/hegel/BerkeleyTop 20 Word Occurance in Berkeley's Three Dialogues.html\" title=\"Berkeley Top 20 Word Occurance in Berkeley's Three Dialogues\">Berkeley's Top 20 Word Occurance in Berkeley's Three Dialogues</a></li>\r\n<li><a href=\"../../../source/hegel/SpinozaTop 20 Word Occurance in Spinoza's Ethics.html\" title=\"Spinoza Top 20 Word Occurance in Spinoza's Ethics\">Spinoza's Top 20 Word Occurance in Spinoza's Ethics</a></li>\r\n<li><a href=\"../../../source/hegel/MooreTop 20 Word Occurance in Moore's Philosophical Studies.html\" title=\"Moore Top 20 Word Occurance in Moore's Philosophical Studies\">Moore's Top 20 Word Occurance in Moore's Philosophical Studies</a></li>\r\n<li><a href=\"../../../source/hegel/DerridaTop 20 Word Occurance in Derrida's Writing And Difference.html\" title=\"Derrida Top 20 Word Occurance in Derrida's Writing And Difference\">Derrida's Top 20 Word Occurance in Derrida's Writing And Difference</a></li>\r\n<li><a href=\"../../../source/hegel/SpinozaTop 20 Word Occurance in Spinoza's On The Improvement Of Understanding.html\" title=\"Spinoza Top 20 Word Occurance in Spinoza's On The Improvement Of Understanding\">Spinoza's Top 20 Word Occurance in Spinoza's On The Improvement Of Understanding</a></li>\r\n<li><a href=\"../../../source/hegel/KantTop 20 Word Occurance in Kant's Critique Of Judgement.html\" title=\"Kant Top 20 Word Occurance in Kant's Critique Of Judgement\">Kant's Top 20 Word Occurance in Kant's Critique Of Judgement</a></li>\r\n<li><a href=\"../../../source/hegel/KantTop 20 Word Occurance in Kant's Critique Of Pure Reason.html\" title=\"Kant Top 20 Word Occurance in Kant's Critique Of Pure Reason\">Kant's Top 20 Word Occurance in Kant's Critique Of Pure Reason</a></li>\r\n<li><a href=\"../../../source/hegel/KripkeTop 20 Word Occurance in Kripke's Philosophical Troubles.html\" title=\"Kripke Top 20 Word Occurance in Kripke's Philosophical Troubles\">Kripke's Top 20 Word Occurance in Kripke's Philosophical Troubles</a></li>\r\n<li><a href=\"../../../source/hegel/LockeTop 20 Word Occurance in Locke's Second Treatise On Government.html\" title=\"Locke Top 20 Word Occurance in Locke's Second Treatise On Government\">Locke's Top 20 Word Occurance in Locke's Second Treatise On Government</a></li>\r\n<li><a href=\"../../../source/hegel/LeninTop 20 Word Occurance in Lenin's Essential Works Of Lenin.html\" title=\"Lenin Top 20 Word Occurance in Lenin's Essential Works Of Lenin\">Lenin's Top 20 Word Occurance in Lenin's Essential Works Of Lenin</a></li>\r\n<li><a href=\"../../../source/hegel/HeideggerTop 20 Word Occurance in Heidegger's Being And Time.html\" title=\"Heidegger Top 20 Word Occurance in Heidegger's Being And Time\">Heidegger's Top 20 Word Occurance in Heidegger's Being And Time</a></li>\r\n<li><a href=\"../../../source/hegel/HumeTop 20 Word Occurance in Hume's A Treatise Of Human Nature.html\" title=\"Hume Top 20 Word Occurance in Hume's A Treatise Of Human Nature\">Hume's Top 20 Word Occurance in Hume's A Treatise Of Human Nature</a></li>\r\n<li><a href=\"../../../source/hegel/WittgensteinTop 20 Word Occurance in Wittgenstein's Tractatus Logico-Philosophicus.html\" title=\"Wittgenstein Top 20 Word Occurance in Wittgenstein's Tractatus Logico-Philosophicus\">Wittgenstein's Top 20 Word Occurance in Wittgenstein's Tractatus Logico-Philosophicus</a></li>\r\n<li><a href=\"../../../source/hegel/HegelTop 20 Word Occurance in Hegel's Elements Of The Philosophy Of Right.html\" title=\"Hegel Top 20 Word Occurance in Hegel's Elements Of The Philosophy Of Right\">Hegel's Top 20 Word Occurance in Hegel's Elements Of The Philosophy Of Right</a></li>\r\n<li><a href=\"../../../source/hegel/Marcus AureliusTop 20 Word Occurance in Marcus Aurelius's Meditations.html\" title=\"Marcus Aurelius Top 20 Word Occurance in Marcus Aurelius's Meditations\">Marcus Aurelius's Top 20 Word Occurance in Marcus Aurelius's Meditations</a></li>\r\n<li><a href=\"../../../source/hegel/NietzscheTop 20 Word Occurance in Nietzsche's Thus Spake Zarathustra.html\" title=\"Nietzsche Top 20 Word Occurance in Nietzsche's Thus Spake Zarathustra\">Nietzsche's Top 20 Word Occurance in Nietzsche's Thus Spake Zarathustra</a></li>\r\n<li><a href=\"../../../source/hegel/PopperTop 20 Word Occurance in Popper's The Logic Of Scientific Discovery.html\" title=\"Popper Top 20 Word Occurance in Popper's The Logic Of Scientific Discovery\">Popper's Top 20 Word Occurance in Popper's The Logic Of Scientific Discovery</a></li>\r\n<li><a href=\"../../../source/hegel/MarxTop 20 Word Occurance in Marx's The Communist Manifesto.html\" title=\"Marx Top 20 Word Occurance in Marx's The Communist Manifesto\">Marx's Top 20 Word Occurance in Marx's The Communist Manifesto</a></li>\r\n<li><a href=\"../../../source/hegel/AristotleTop 20 Word Occurance in Aristotle's Aristotle - Complete Works.html\" title=\"Aristotle Top 20 Word Occurance in Aristotle's Aristotle - Complete Works\">Aristotle's Top 20 Word Occurance in Aristotle's Aristotle - Complete Works</a></li>\r\n<li><a href=\"../../../source/hegel/SmithTop 20 Word Occurance in Smith's The Wealth Of Nations.html\" title=\"Smith Top 20 Word Occurance in Smith's The Wealth Of Nations\">Smith's Top 20 Word Occurance in Smith's The Wealth Of Nations</a></li>\r\n<li><a href=\"../../../source/hegel/LeibnizTop 20 Word Occurance in Leibniz's Theodicy.html\" title=\"Leibniz Top 20 Word Occurance in Leibniz's Theodicy\">Leibniz's Top 20 Word Occurance in Leibniz's Theodicy</a></li>\r\n<li><a href=\"../../../source/hegel/DescartesTop 20 Word Occurance in Descartes's Discourse On Method.html\" title=\"Descartes Top 20 Word Occurance in Descartes's Discourse On Method\">Descartes's Top 20 Word Occurance in Descartes's Discourse On Method</a></li>\r\n<li><a href=\"../../../source/hegel/FichteTop 20 Word Occurance in Fichte's The System Of Ethics.html\" title=\"Fichte Top 20 Word Occurance in Fichte's The System Of Ethics\">Fichte's Top 20 Word Occurance in Fichte's The System Of Ethics</a></li>\r\n<li><a href=\"../../../source/hegel/DeleuzeTop 20 Word Occurance in Deleuze's Difference And Repetition.html\" title=\"Deleuze Top 20 Word Occurance in Deleuze's Difference And Repetition\">Deleuze's Top 20 Word Occurance in Deleuze's Difference And Repetition</a></li>\r\n<li><a href=\"../../../source/hegel/NietzscheTop 20 Word Occurance in Nietzsche's Beyond Good And Evil.html\" title=\"Nietzsche Top 20 Word Occurance in Nietzsche's Beyond Good And Evil\">Nietzsche's Top 20 Word Occurance in Nietzsche's Beyond Good And Evil</a></li>\r\n<li><a href=\"../../../source/hegel/HumeTop 20 Word Occurance in Hume's Dialogues Concerning Natural Religion.html\" title=\"Hume Top 20 Word Occurance in Hume's Dialogues Concerning Natural Religion\">Hume's Top 20 Word Occurance in Hume's Dialogues Concerning Natural Religion</a></li>\r\n<li><a href=\"../../../source/hegel/HegelTop 20 Word Occurance in Hegel's The Phenomenology Of Spirit.html\" title=\"Hegel Top 20 Word Occurance in Hegel's The Phenomenology Of Spirit\">Hegel's Top 20 Word Occurance in Hegel's The Phenomenology Of Spirit</a></li>\r\n<li><a href=\"../../../source/hegel/FoucaultTop 20 Word Occurance in Foucault's History Of Madness.html\" title=\"Foucault Top 20 Word Occurance in Foucault's History Of Madness\">Foucault's Top 20 Word Occurance in Foucault's History Of Madness</a></li>\r\n<li><a href=\"../../../source/hegel/BerkeleyTop 20 Word Occurance in Berkeley's A Treatise Concerning The Principles Of Human Knowledge.html\" title=\"Berkeley Top 20 Word Occurance in Berkeley's A Treatise Concerning The Principles Of Human Knowledge\">Berkeley's Top 20 Word Occurance in Berkeley's A Treatise Concerning The Principles Of Human Knowledge</a></li>\r\n<li><a href=\"../../../source/hegel/RussellTop 20 Word Occurance in Russell's The Problems Of Philosophy.html\" title=\"Russell Top 20 Word Occurance in Russell's The Problems Of Philosophy\">Russell's Top 20 Word Occurance in Russell's The Problems Of Philosophy</a></li>\r\n</ul>\r\n</body>\r\n</html>\r\n",
      "date_published": "2023-01-01T16:00:00-08:00"
    },{
      "id": "https://ischmidls.github.io/posts/pages/tabaway/",
      "url": "https://ischmidls.github.io/posts/pages/tabaway/",
      "title": "tab away",
      "content_html": "<!DOCTYPE html>\r\n<html>\r\n<head>\r\n  <title>Remove Tabs</title>\r\n</head>\r\n<body>\r\n  <label for=\"input-text\">Enter text:</label>\r\n  <textarea id=\"input-text\" rows=\"4\" cols=\"50\"></textarea><br><br>\r\n  <button onclick=\"removeTabs()\">Remove Tabs</button>\r\n  <h2>Result:</h2>\r\n  <div id=\"output\"></div>\r\n\r\n  <script>\r\n    function removeTabs() {\r\n      const input = document.getElementById(\"input-text\").value;\r\n      const output = input.replace(/\\t/g, \"\");\r\n      const formattedOutput = output.replace(/\\n/g, \"<br>\");\r\n      document.getElementById(\"output\").innerHTML = formattedOutput;\r\n    }\r\n  </script>\r\n</body>\r\n</html>\r\n",
      "date_published": "2022-12-31T16:00:00-08:00"
    },{
      "id": "https://ischmidls.github.io/posts/pages/mix/",
      "url": "https://ischmidls.github.io/posts/pages/mix/",
      "title": "mix list",
      "content_html": "<!DOCTYPE html>\r\n<html>\r\n<head>\r\n  <title>Randomly Mix Elements</title>\r\n  <script>\r\n    function mixElements() {\r\n      var inputString = document.getElementById(\"inputString\").value;\r\n      \r\n      // Split the input string into an array\r\n      var elementsArray = inputString.split(\",\");\r\n      \r\n      // Randomly shuffle the elements array\r\n      for (var i = elementsArray.length - 1; i > 0; i--) {\r\n        var j = Math.floor(Math.random() * (i + 1));\r\n        var temp = elementsArray[i];\r\n        elementsArray[i] = elementsArray[j];\r\n        elementsArray[j] = temp;\r\n      }\r\n      \r\n      // Join the elements array into a mixed-up string\r\n      var mixedString = elementsArray.join(\",\");\r\n      \r\n      // Display the mixed-up string\r\n      document.getElementById(\"result\").innerHTML = mixedString;\r\n    }\r\n  </script>\r\n</head>\r\n<body>\r\n  <label for=\"inputString\">Enter a comma-delimited string:</label>\r\n  <input type=\"text\" id=\"inputString\">\r\n  <button onclick=\"mixElements()\">Mix Elements</button>\r\n  <p id=\"result\"></p>\r\n</body>\r\n</html>\r\n",
      "date_published": "2022-12-31T16:00:00-08:00"
    },{
      "id": "https://ischmidls.github.io/posts/pages/first%20sentence/",
      "url": "https://ischmidls.github.io/posts/pages/first%20sentence/",
      "title": "first sentence",
      "content_html": "<!DOCTYPE html>\r\n<html>\r\n  <head>\r\n    <meta charset=\"UTF-8\">\r\n    <title>Wikipedia Page First Sentence</title>\r\n  </head>\r\n  <body>\r\n    <form id=\"myForm\">\r\n      <label for=\"url\">Enter Wikipedia page URL:</label>\r\n      <input type=\"text\" id=\"url\" name=\"url\">\r\n      <input type=\"submit\" value=\"Get First Sentence\">\r\n    </form>\r\n    <br>\r\n    <p>BONUS: If you know the page title, the program works with that too.</p>\r\n    <p>(e.g.) \"Gilles Deleuze\" instead of \"https://en.wikipedia.org/wiki/Gilles_Deleuze\"</p>\r\n    <br>\r\n    <div id=\"result\"></div>\r\n\r\n    <script>\r\n      const form = document.getElementById('myForm');\r\n      const resultDiv = document.getElementById('result');\r\n\r\n      form.addEventListener('submit', async (event) => {\r\n        event.preventDefault();\r\n        const urlInput = document.getElementById('url');\r\n        const url = urlInput.value;\r\n        const queryArray = url.split('/')\r\n\r\n        const proxy = new Proxy(queryArray, {\r\n            get(target, prop) {\r\n                if (!isNaN(prop)) {\r\n                    prop = parseInt(prop, 10);\r\n                    if (prop < 0) {\r\n                        prop += target.length;\r\n                    }\r\n                }\r\n                return target[prop];\r\n            }\r\n        });\r\n\r\n        const query = proxy[-1]\r\n        \r\n        const apiEndpoint = `https://en.wikipedia.org/api/rest_v1/page/summary/${query}`;\r\n        try {\r\n          const response = await fetch(apiEndpoint);\r\n          const data = await response.json();\r\n          // resultDiv.innerText = 'result'+ data.stringify\r\n          const firstSentence = data.extract.split('.')[0] + '.';\r\n          resultDiv.innerText = firstSentence;\r\n        } catch (error) {\r\n          console.error(error);\r\n          resultDiv.innerText = 'Error retrieving first sentence';\r\n        }\r\n      });\r\n    </script>\r\n  </body>\r\n</html>\r\n",
      "date_published": "2022-12-31T16:00:00-08:00"
    },{
      "id": "https://ischmidls.github.io/posts/pages/countsplit/",
      "url": "https://ischmidls.github.io/posts/pages/countsplit/",
      "title": "count split",
      "content_html": "<!DOCTYPE html>\r\n<html>\r\n  <head>\r\n    <title>Substring Splitter</title>\r\n  </head>\r\n  <body>\r\n    <label for=\"inputString\">Input string:</label>\r\n    <input type=\"text\" id=\"inputString\"><br><br>\r\n    <label for=\"maxChar\">Max characters:</label>\r\n    <input type=\"number\" id=\"maxChar\"><br><br>\r\n    <button onclick=\"splitString()\">Split string</button><br><br>\r\n    <label for=\"output\">Output:</label>\r\n    <textarea id=\"output\" rows=\"10\" cols=\"50\"></textarea>\r\n\r\n    <script>\r\n      function splitString() {\r\n        var input = document.getElementById(\"inputString\").value;\r\n        var max = parseInt(document.getElementById(\"maxChar\").value);\r\n        var substrings = [];\r\n\r\n        for (var i = 0; i < input.length; i += max) {\r\n          substrings.push(input.substring(i, i + max));\r\n        }\r\n\r\n        document.getElementById(\"output\").value = substrings.join(\"\\n\\n\\n\");\r\n      }\r\n    </script>\r\n  </body>\r\n</html>\r\n",
      "date_published": "2022-12-31T16:00:00-08:00"
    },{
      "id": "https://ischmidls.github.io/posts/pages/countlines/",
      "url": "https://ischmidls.github.io/posts/pages/countlines/",
      "title": "count lines",
      "content_html": "<!DOCTYPE html>\r\n<html>\r\n<head>\r\n\t<title>Print Substring</title>\r\n</head>\r\n<body>\r\n\t<label for=\"end\">Enter an integer (N):</label>\r\n\t<input type=\"number\" id=\"end\">\r\n\t<button onclick=\"printSubstring()\">Print Substring</button>\r\n    <p><em>Enumerate paragraphs with (§N) where N ∈ 0, 1, 2, 3, ...</em></p>\r\n\t<p id=\"output\"></p>\r\n\t<script>\r\n\t\tfunction printSubstring() {\r\n\t\t\tconst end = document.getElementById(\"end\").value;\r\n\t\t\tlet output = \"\";\r\n\t\t\tfor (let i = 1; i <= end; i++) {\r\n\t\t\t\toutput += `(§${i})<br>`;\r\n\t\t\t}\r\n\t\t\tdocument.getElementById(\"output\").innerHTML = output;\r\n\t\t}\r\n\t</script>\r\n</body>\r\n</html>\r\n",
      "date_published": "2022-12-31T16:00:00-08:00"
    },{
      "id": "https://ischmidls.github.io/posts/functions/",
      "url": "https://ischmidls.github.io/posts/functions/",
      "title": "functions intro",
      "content_html": "<p>Below is a list of functions, then some graphs of these functions</p>\r\n<h2>functions<h2>\r\n    <div class=\"card\"><div class=\"card-body\">\r\n\r\n    \r\n    <ol>\r\n        <li>constant function: \\( f(x) = c \\), where \\( c \\) is a constant.</li>\r\n        <li>identity function: \\( f(x) = x \\).</li>\r\n        <li>polynomial functions: \\( f(x) = a_nx^n + a_{n-1}x^{n-1} + \\ldots + a_1x + a_0 \\), where \\( a_i \\) are constants and \\( n \\) is a non-negative integer.</li>\r\n        <li>exponential function: \\( f(x) = e^x \\).</li>\r\n        <li>logarithmic function: \\( f(x) = \\log_a(x) \\), where \\( a \\) is a positive constant and \\( x \\) is the argument.</li>\r\n        <li>\r\n          trigonometric functions:\r\n          <ul>\r\n            <li>sine function: \\( f(x) = \\sin(x) \\).</li>\r\n            <li>cosine function: \\( f(x) = \\cos(x) \\).</li>\r\n            <li>tangent function: \\( f(x) = \\tan(x) \\).</li>\r\n            <li>cosecant function: \\( f(x) = \\csc(x) \\).</li>\r\n            <li>secant function: \\( f(x) = \\sec(x) \\).</li>\r\n            <li>cotangent function: \\( f(x) = \\cot(x) \\).</li>\r\n          </ul>\r\n        </li>\r\n        <li>\r\n          inverse trigonometric functions:\r\n          <ul>\r\n            <li>arcsine function: \\( f(x) = \\arcsin(x) \\).</li>\r\n            <li>arccosine function: \\( f(x) = \\arccos(x) \\).</li>\r\n            <li>arctangent function: \\( f(x) = \\arctan(x) \\).</li>\r\n            <li>arccosecant function: \\( f(x) = \\text{arccsc}(x) \\).</li>\r\n            <li>arcsecant function: \\( f(x) = \\text{arcsec}(x) \\).</li>\r\n            <li>arccotangent function: \\( f(x) = \\text{arccot}(x) \\).</li>\r\n          </ul>\r\n        </li>\r\n        <li>\r\n          hyperbolic functions:\r\n          <ul>\r\n            <li>hyperbolic sine function: \\( f(x) = \\sinh(x) \\).</li>\r\n            <li>hyperbolic cosine function: \\( f(x) = \\cosh(x) \\).</li>\r\n            <li>hyperbolic tangent function: \\( f(x) = \\tanh(x) \\).</li>\r\n            <li>hyperbolic cosecant function: \\( f(x) = \\text{csch}(x) \\).</li>\r\n            <li>hyperbolic secant function: \\( f(x) = \\text{sech}(x) \\).</li>\r\n            <li>hyperbolic cotangent function: \\( f(x) = \\coth(x) \\).</li>\r\n          </ul>\r\n        </li>\r\n        <li>\r\n          inverse hyperbolic functions:\r\n          <ul>\r\n            <li>inverse hyperbolic sine function: \\( f(x) = \\text{arcsinh}(x) \\).</li>\r\n            <li>inverse hyperbolic cosine function: \\( f(x) = \\text{arccosh}(x) \\).</li>\r\n            <li>inverse hyperbolic tangent function: \\( f(x) = \\text{arctanh}(x) \\).</li>\r\n            <li>inverse hyperbolic cosecant function: \\( f(x) = \\text{arccsch}(x) \\).</li>\r\n            <li>inverse hyperbolic secant function: \\( f(x) = \\text{arcsech}(x) \\).</li>\r\n            <li>inverse hyperbolic cotangent function: \\( f(x) = \\text{arccoth}(x) \\).</li>\r\n          </ul>\r\n        </li>\r\n      </ol>\r\n    </div></div><br>\r\n    <h2>plots</h2>\r\n    <p>This began as an attempt to explain \"functions\" to my dad and my brother. It did not get far, but it is something. Enjoy?</p>\r\n    <hr />\r\n    <p><b>identity \\(y=x\\)</b></p>\r\n    <iframe title=\"x\" src=\"https://www.desmos.com/calculator/kahamkom8c?embed\" width=\"500\" height=\"500\" style=\"border: 1px solid #ccc\" frameborder=0></iframe>\r\n    <hr />\r\n    <p><b>square \\(y=x^2\\)</b></p>\r\n    <iframe title=\"x^2\" src=\"https://www.desmos.com/calculator/syis78tlpo?embed\" width=\"500\" height=\"500\" style=\"border: 1px solid #ccc\" frameborder=0></iframe>\r\n    <hr />\r\n    <p><b>cube \\(y=x^3\\)</b></p>\r\n    <iframe title=\"x^3\" src=\"https://www.desmos.com/calculator/1epsvqyloe?embed\" width=\"500\" height=\"500\" style=\"border: 1px solid #ccc\" frameborder=0></iframe>\r\n    <hr />\r\n    <p><b>square root \\(y=\\sqrt{x}\\)</b></p>\r\n    <iframe title=\"sqrt x\"src=\"https://www.desmos.com/calculator/efdii2fk66?embed\" width=\"500\" height=\"500\" style=\"border: 1px solid #ccc\" frameborder=0></iframe>\r\n    <hr />\r\n    <p><b>cube root \\(y=\\sqrt{3}{x}\\)</b></p>\r\n    <iframe title=\"cbrt x\" src=\"https://www.desmos.com/calculator/gvhocgvslv?embed\" width=\"500\" height=\"500\" style=\"border: 1px solid #ccc\" frameborder=0></iframe>\r\n    <hr />\r\n    <p><b>exponent \\(y=e^x\\)</b></p>\r\n    <iframe title=\"exp x\"src=\"https://www.desmos.com/calculator/yao6fsrndo?embed\" width=\"500\" height=\"500\" style=\"border: 1px solid #ccc\" frameborder=0></iframe>\r\n    <hr />\r\n    <p><b>natural log \\(y=\\ln (x) \\)</b></p>\r\n    <iframe title=\"ln x\"src=\"https://www.desmos.com/calculator/fmniheitob?embed\" width=\"500\" height=\"500\" style=\"border: 1px solid #ccc\" frameborder=0></iframe>\r\n    <hr />\r\n    <p><b>rational \\(y= \\frac{1}{x} \\)</b></p>\r\n    <iframe title=\"1/x\" src=\"https://www.desmos.com/calculator/h4kv4ohhwx?embed\" width=\"500\" height=\"500\" style=\"border: 1px solid #ccc\" frameborder=0></iframe>\r\n    <hr />\r\n    <p><b>sine \\(y= \\sin{x} \\)</b></p>\r\n    <iframe title=\"sin x\" src=\"https://www.desmos.com/calculator/drshd4d2ta?embed\" width=\"500\" height=\"500\" style=\"border: 1px solid #ccc\" frameborder=0></iframe>\r\n    <hr />\r\n    <p><b>cosine \\(y= \\cos{x} \\)</b></p>\r\n    <iframe title=\"cos x\" src=\"https://www.desmos.com/calculator/z2srullydd?embed\" width=\"500\" height=\"500\" style=\"border: 1px solid #ccc\" frameborder=0></iframe>\r\n    <hr />\r\n    <p><b>absolute value \\(y= \\operatorname{abs}{x} \\)</b></p>\r\n    <iframe title=\"abs x\" src=\"https://www.desmos.com/calculator/3mqmxivk8e?embed\" width=\"500\" height=\"500\" style=\"border: 1px solid #ccc\" frameborder=0></iframe>\r\n    <hr />\r\n    <p><b>floor \\(y= \\operatorname{floor}{x} \\)</b></p>\r\n    <iframe title=\"floor x\" src=\"https://www.desmos.com/calculator/ebosxd0bzm?embed\" width=\"500\" height=\"500\" style=\"border: 1px solid #ccc\" frameborder=0></iframe>\r\n    <hr />\r\n    <p><b>tranformations</b></p>\r\n    <p>In the scroll menu below, use the equation and sliders to investigate each of the functions.  Change the letter of the function notation to see each function's transformations. For example delete the f and replace with a g to change the parent from  a linear to a quadratic.</p>\r\n    <iframe width=\"100%\" height=\"1080px\" src=\"https://www.desmos.com/calculator/gjxaovcfec\"></iframe>\r\n    <hr />\r\n    </div>\r\n    <script src=\"https://code.jquery.com/jquery-3.4.1.min.js\"></script>\r\n    <script src=\"https://code.jquery.com/ui/1.12.1/jquery-ui.js\"></script>\r\n    <script src=\"https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js\" integrity=\"sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo\" crossorigin=\"anonymous\"></script>\r\n    <script src=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js\" integrity=\"sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6\" crossorigin=\"anonymous\"></script>",
      "date_published": "2022-07-02T17:00:00-07:00"
    },{
      "id": "https://ischmidls.github.io/posts/pages/mathtax/",
      "url": "https://ischmidls.github.io/posts/pages/mathtax/",
      "title": "math education taxonomy",
      "content_html": "<!doctype html>\r\n<html lang=\"en\" dir=\"ltr\">\r\n  <head>\r\n    <meta charset=\"utf-8\">\r\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\r\n    <link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css\" integrity=\"sha384-B0vP5xmATw1+K9KRQjQERJvTumQW0nPEzvF6L/Z6nronJ3oUOFUFpCjEUQouq2+l\" crossorigin=\"anonymous\">\r\n    <title>Math Taxonomy</title>\r\n    <style> img {width: 90vw;}</style>\r\n  </head>\r\n  <body>\r\n    <div class=\"container\" id=\"bsr-wrapper\">\r\n        <main  class=\"tmpl-post\" >\r\n    <h1>Math Taxonomy</h1>\r\n\r\n<p>Izak, <time datetime=\"2022-07-16\">16 Jul 2022</time></p>\r\n\r\n\r\n      <hr>\r\n        <h2>My Final Visual</h2>\r\n        <p>Tap or click to interact!</p>\r\n        <!-- PLOTLY OUTPUT -->\r\n        <div>                        <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\r\n            <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"a9240c6c-37ff-4f48-b622-c68e751e2bd8\" class=\"plotly-graph-div\" style=\"height:100%; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"a9240c6c-37ff-4f48-b622-c68e751e2bd8\")) {                    Plotly.newPlot(                        \"a9240c6c-37ff-4f48-b622-c68e751e2bd8\",                        [{\"ids\":[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"293\",\"294\",\"295\",\"296\",\"297\",\"13\",\"91\",\"298\",\"299\",\"300\",\"301\",\"302\",\"303\",\"304\",\"305\",\"306\",\"92\",\"307\",\"308\",\"309\",\"310\",\"311\",\"312\",\"93\",\"313\",\"314\",\"315\",\"316\",\"317\",\"94\",\"95\",\"318\",\"319\",\"320\",\"321\",\"322\",\"14\",\"96\",\"97\",\"98\",\"99\",\"15\",\"100\",\"323\",\"324\",\"325\",\"101\",\"326\",\"327\",\"328\",\"102\",\"329\",\"330\",\"331\",\"332\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"16\",\"110\",\"111\",\"112\",\"17\",\"113\",\"114\",\"115\",\"116\",\"18\",\"19\",\"20\",\"117\",\"118\",\"119\",\"333\",\"334\",\"335\",\"336\",\"337\",\"338\",\"339\",\"340\",\"120\",\"341\",\"342\",\"343\",\"344\",\"345\",\"346\",\"347\",\"121\",\"122\",\"123\",\"124\",\"21\",\"125\",\"126\",\"127\",\"128\",\"129\",\"130\",\"131\",\"22\",\"132\",\"133\",\"134\",\"135\",\"136\",\"23\",\"137\",\"138\",\"348\",\"349\",\"350\",\"139\",\"140\",\"141\",\"142\",\"143\",\"144\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"145\",\"146\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"037\",\"38\",\"39\",\"147\",\"148\",\"149\",\"150\",\"351\",\"352\",\"353\",\"354\",\"151\",\"355\",\"356\",\"357\",\"358\",\"359\",\"152\",\"153\",\"360\",\"361\",\"362\",\"363\",\"154\",\"364\",\"365\",\"366\",\"367\",\"40\",\"155\",\"156\",\"157\",\"158\",\"159\",\"160\",\"161\",\"41\",\"162\",\"163\",\"164\",\"165\",\"166\",\"167\",\"168\",\"169\",\"170\",\"42\",\"43\",\"44\",\"45\",\"171\",\"172\",\"173\",\"174\",\"46\",\"175\",\"176\",\"177\",\"178\",\"179\",\"180\",\"181\",\"47\",\"48\",\"182\",\"183\",\"184\",\"185\",\"186\",\"187\",\"49\",\"188\",\"189\",\"190\",\"191\",\"192\",\"193\",\"50\",\"194\",\"195\",\"196\",\"197\",\"198\",\"199\",\"200\",\"201\",\"51\",\"52\",\"53\",\"54\",\"202\",\"203\",\"204\",\"205\",\"206\",\"207\",\"55\",\"208\",\"209\",\"210\",\"211\",\"212\",\"213\",\"214\",\"56\",\"215\",\"216\",\"217\",\"218\",\"219\",\"220\",\"221\",\"222\",\"57\",\"223\",\"224\",\"225\",\"226\",\"227\",\"58\",\"228\",\"229\",\"230\",\"231\",\"232\",\"59\",\"233\",\"234\",\"235\",\"236\",\"237\",\"238\",\"239\",\"60\",\"61\",\"62\",\"240\",\"241\",\"242\",\"243\",\"244\",\"245\",\"246\",\"247\",\"248\",\"63\",\"249\",\"250\",\"251\",\"252\",\"253\",\"254\",\"255\",\"256\",\"257\",\"64\",\"258\",\"259\",\"260\",\"261\",\"262\",\"65\",\"263\",\"264\",\"265\",\"266\",\"267\",\"268\",\"269\",\"270\",\"271\",\"66\",\"272\",\"273\",\"274\",\"67\",\"275\",\"368\",\"369\",\"276\",\"370\",\"371\",\"68\",\"277\",\"278\",\"279\",\"280\",\"281\",\"282\",\"283\",\"284\",\"285\",\"69\",\"286\",\"372\",\"373\",\"374\",\"375\",\"287\",\"376\",\"377\",\"378\",\"288\",\"379\",\"380\",\"289\",\"381\",\"382\",\"383\",\"290\",\"384\",\"385\",\"386\",\"291\",\"292\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\"],\"labels\":[\"Math Topics\",\"Numbers<br>and<br>Computation\",\"Logic<br>and<br>Foundations\",\"Algebra<br>and<br>Number<br>Theory\",\"Discrete<br>Mathematics\",\"Geometry<br>and<br>Topology\",\"Calculus\",\"Analysis\",\"Differential<br>and<br>Difference<br>Equations\",\"Statistics<br>and<br>Probability\",\"Applied<br>Mathematics\",\"Mathematics<br>History\",\"Number<br>Concepts\",\"Natural\",\"Integers\",\"Rational\",\"Irrational\",\"Algebraic\",\"Real\",\"Complex\",\"Famous<br>Numbers\",\"zero\",\"pi\",\"e\",\"i\",\"Golden<br>Mean\",\"Arithmetic\",\"Operations\",\"Addition\",\"Subtraction\",\"Multiplication\",\"Division\",\"Roots\",\"Factorials\",\"Factoring\",\"Properties<br>of<br>Operations\",\"Estimation\",\"Fractions\",\"Addition\",\"Subtraction\",\"Multiplication\",\"Division\",\"Ratio<br>and<br>Proportion\",\"Equivalent<br>Fractions\",\"Decimals\",\"Addition\",\"Subtraction\",\"Multiplication\",\"Division\",\"Percents\",\"Comparison<br>of<br>numbers\",\"Exponents\",\"Multiplication\",\"Division\",\"Powers\",\"Integer<br>Exponents\",\"Rational<br>Exponents\",\"Patterns<br>and<br>Sequences\",\"Number<br>Patterns\",\"Fibonacci<br>Sequence\",\"Arithmetic<br>Sequence\",\"Geometric<br>Sequence\",\"Measurement\",\"Units<br>of<br>Measurement\",\"Metric<br>System\",\"Standard<br>Units\",\"Nonstandard<br>Units\",\"Linear<br>Measure\",\"Distance\",\"Circumference\",\"Perimeter\",\"Area\",\"Area<br>of<br>Polygons\",\"Area<br>of<br>Circles\",\"Surface<br>Area\",\"Nonstandard<br>Shapes\",\"Volume\",\"Weight<br>and<br>Mass\",\"Temperature\",\"Time\",\"Speed\",\"Money\",\"Scale\",\"Logic\",\"Venn<br>Diagrams\",\"Propositional<br>and<br>Predicate<br>Logic\",\"Methods<br>of<br>Proof\",\"Set<br>Theory\",\"Sets<br>and<br>Set<br>Operations\",\"Relations<br>and<br>Functions\",\"Cardinality\",\"Axiom<br>of<br>Choice\",\"Computability<br>and<br>Decidability\",\"Model<br>Theory\",\"Algebra\",\"Graphing<br>Techniques\",\"Algebraic<br>Manipulation\",\"Functions\",\"Linear\",\"Quadratic\",\"Polynomial\",\"Rational\",\"Exponential\",\"Logarithmic\",\"Piece-wise\",\"Step\",\"Equations\",\"Linear\",\"Quadratic\",\"Polynomial\",\"Rational\",\"Exponential\",\"Logarithmic\",\"Systems\",\"Inequalities\",\"Matrices\",\"Sequences<br>and<br>Series\",\"Algebraic<br>Proof\",\"Linear<br>Algebra\",\"Systems<br>of<br>Linear<br>Equations\",\"Matrix<br>algebra\",\"Vectors<br>in<br>R\",\"Vector<br>Spaces\",\"Linear<br>Transformations\",\"Eigenvalues<br>and<br>Eigenvectors\",\"Inner<br>Product<br>Spaces\",\"Abstract<br>Algebra\",\"Groups\",\"Rings<br>and<br>Ideals\",\"Fields\",\"Galois<br>Theory\",\"Multilinear<br>Algebra\",\"Number<br>Theory\",\"Integers\",\"Primes\",\"Divisibility\",\"Factorization\",\"Distributions<br>of<br>Primes\",\"Congruences\",\"Diophantine<br>Equations\",\"Irrational<br>Numbers\",\"Famous<br>Problems\",\"Coding<br>Theory\",\"Cryptography\",\"Category<br>Theory\",\"K-Theory\",\"Homological<br>Algebra\",\"Modular<br>Arithmetic\",\"Cellular<br>Automata\",\"Combinatorics\",\"Combinations\",\"Permutations\",\"Game<br>Theory\",\"Algorithms\",\"Recursion\",\"Graph<br>Theory\",\"Linear<br>Programming\",\"Order<br>and<br>Lattices\",\"Theory<br>of<br>Computation\",\"<br>Chaos\",\"Geometric<br>Proof\",\"Plane<br>Geometry\",\"Plane<br>Measurement\",\"Lines<br>and<br>Planes\",\"Angles\",\"Triangles\",\"Properties\",\"Congruence\",\"Similarity\",\"Pythagorean<br>Theorem\",\"Polygons\",\"Properties\",\"Regular\",\"Irregular\",\"Congruence\",\"Similarity\",\"Circles\",\"Patterns\",\"Geometric<br>Patterns\",\"Tilings<br>and<br>Tessellations\",\"Symmetry\",\"Golden<br>Ratio\",\"Transformations\",\"Translation\",\"Rotation\",\"Reflection\",\"Scaling\",\"Solid<br>Geometry\",\"Dihedral<br>Angles\",\"Spheres\",\"Cones\",\"Cylinders\",\"Pyramids\",\"Prisms\",\"Polyhedra\",\"Analytic<br>Geometry\",\"Cartesian<br>Coordinates\",\"Lines\",\"Circles\",\"Planes\",\"Conics\",\"Polar<br>Coordinates\",\"Parametric<br>Curves\",\"Surfaces\",\"Distance<br>Formula\",\"Projective<br>Geometry\",\"Differential<br>Geometry\",\"Algebraic<br>Geometry\",\"Topology\",\"Point<br>Set<br>Topology\",\"General<br>Topology\",\"Differential<br>Topology\",\"Algebraic<br>Topology\",\"Trigonometry\",\"Angles\",\"Trigonometric<br>Functions\",\"Inverse<br>Trigonometric<br>Functions\",\"Trigonometric<br>Identities\",\"Trigonometric<br>Equations\",\"Roots<br>of<br>Unity\",\"Spherical<br>Trigonometry\",\"Fractal<br>Geometry\",\"Single<br>Variable\",\"(Single)<br>Functions\",\"(Single)<br>Limits\",\"(Single)<br>Continuity\",\"(Single)<br>Differentiation\",\"(Single)<br>Integration\",\"(Single)<br>Series\",\"Several<br>Variables\",\"Functions<br>of<br>Several<br>Variables\",\"(Several)<br>Limits\",\"(Several)<br>Continuity\",\"Partial<br>Derivatives\",\"Multiple<br>integrals\",\"Taylor<br>Series\",\"Advanced<br>Calculus\",\"Vector<br>Valued<br>Functions\",\"Line<br>Integrals\",\"Surface<br>Integrals\",\"Stokes<br>Theorem\",\"Curvilinear<br>Coordinates\",\"Linear<br>spaces\",\"Fourier<br>Series\",\"Orthogonal<br>Functions\",\"Tensor<br>Calculus\",\"Calculus<br>of<br>Variations\",\"Operational<br>Calculus\",\"Real<br>Analysis\",\"(Analysis)<br>Metric<br>Spaces\",\"(Analysis)<br>Convergence\",\"(Analysis)<br>Continuity\",\"(Analysis)<br>Differentiation\",\"(Analysis)<br>Integration\",\"Measure<br>Theory\",\"Complex<br>Analysis\",\"(Complex)<br>Convergence\",\"Infinite<br>Series\",\"Analytic<br>Functions\",\"Integration\",\"Contour<br>Integrals\",\"Conformal<br>Mappings\",\"Several<br>Complex<br>Variables\",\"Numerical<br>Analysis\",\"Computer<br>Arithmetic\",\"Solutions<br>of<br>Equations\",\"Solutions<br>of<br>Systems\",\"Interpolation\",\"Numerical<br>Differentiation\",\"Numerical<br>Integration\",\"Numerical<br>Solutions<br>of<br>ODEs\",\"Numerical<br>Solutions<br>of<br>PDEs\",\"Integral<br>Transforms\",\"Fourier<br>Transforms\",\"Laplace<br>Transforms\",\"Hankel<br>Transforms\",\"Wavelets\",\"Other<br>Transforms\",\"Signal<br>Analysis\",\"Sampling<br>Theory\",\"Filters\",\"Noise\",\"Data<br>Compression\",\"Image<br>Processing\",\"Functional<br>Analysis\",\"Hilbert<br>Spaces\",\"Banach<br>Spaces\",\"Topological<br>Spaces\",\"Locally<br>Convex<br>Spaces\",\"Bounded<br>Operators\",\"Spectral<br>Theorem\",\"Unbounded<br>Operators\",\"Harmonic<br>Analysis\",\"Global<br>Analysis\",\"Ordinary<br>Differential<br>Equations\",\"First<br>Order\",\"Second<br>Order\",\"Linear<br>Oscillations\",\"Nonlinear<br>Oscillations\",\"Systems<br>of<br>Differential<br>Equations\",\"Sturm<br>Liouville<br>Problems\",\"Special<br>Functions\",\"Power<br>Series<br>Methods\",\"Laplace<br>Transforms\",\"Partial<br>Differential<br>Equations\",\"First<br>Order\",\"Elliptic\",\"Parabolic\",\"Hyperbolic\",\"Integral<br>Transforms\",\"Integral<br>Equations\",\"Potential<br>Theory\",\"Nonlinear<br>Equations\",\"Symmetries<br>and<br>Integrability\",\"Difference<br>Equations\",\"First<br>Order\",\"Second<br>Order\",\"Linear<br>Systems\",\"Z<br>Transforms\",\"Orthogonal<br>Polynomials\",\"Dynamical<br>Systems\",\"D<br>Maps\",\"D<br>Maps\",\"Lyapunov<br>Exponents\",\"Bifurcations\",\"Fractals\",\"Differentiable<br>Dynamics\",\"Conservative<br>Dynamics\",\"Chaos\",\"Complex<br>Dynamical<br>Systems\",\"Data<br>Collection\",\"Experimental<br>Design\",\"Sampling<br>and<br>Surveys\",\"Data<br>and<br>Measurement<br>Issues\",\"Data<br>Summary<br>and<br>Presentation\",\"Summary<br>Statistics\",\"Measures<br>of<br>Central<br>Tendencies\",\"Measures<br>of<br>Spread\",\"Data<br>Representation\",\"Graphs<br>and<br>Plots\",\"Tables\",\"Statistical<br>Inference<br>and<br>Techniques\",\"Sampling<br>Distributions\",\"Regression<br>and<br>Correlation\",\"Confidence<br>Intervals\",\"Hypothesis<br>Tests\",\"Statistical<br>Quality<br>Control\",\"Non-parametric<br>Techniques\",\"Multivariate<br>Techniques\",\"Survival<br>Analysis\",\"Bayesian<br>Statistics\",\"Probability\",\"Elementary<br>Probability\",\"Sample<br>Space<br>and<br>Sets\",\"General<br>Rules\",\"Combinations<br>and<br>Permutations\",\"Random<br>Variables\",\"Univariate<br>Distributions\",\"Discrete<br>Distributions\",\"Continuous<br>Distributions\",\"Expected<br>Value\",\"Limit<br>Theorems\",\"Central<br>Limit<br>Theorem\",\"Law<br>of<br>Large<br>Numbers\",\"Multivariate<br>Distributions\",\"Joint\",\"Conditional\",\"Expectations\",\"Stochastic<br>Processes\",\"Brownian<br>Motion\",\"Markov<br>Chains\",\"Queuing<br>Theory\",\"Probability<br>Measures\",\"Simulation\",\"Mathematical<br>Physics\",\"Mathematical<br>Economics\",\"Mathematical<br>Biology\",\"Mathematics<br>for<br>Business\",\"Engineering<br>Mathematics\",\"Mathematical<br>Sociology\",\"Mathematics<br>for<br>Social<br>Sciences\",\"Mathematics<br>for<br>Computer<br>Science\",\"Mathematics<br>for<br>Humanities\",\"Consumer<br>Mathematics\",\"General\",\"Famous<br>Problems\",\"Biographies<br>of<br>Mathematicians\"],\"parents\":[\"\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"1\",\"12\",\"12\",\"12\",\"12\",\"12\",\"12\",\"12\",\"12\",\"90\",\"90\",\"90\",\"90\",\"90\",\"1\",\"13\",\"91\",\"91\",\"91\",\"91\",\"91\",\"91\",\"91\",\"91\",\"91\",\"13\",\"92\",\"92\",\"92\",\"92\",\"92\",\"92\",\"13\",\"93\",\"93\",\"93\",\"93\",\"93\",\"13\",\"13\",\"95\",\"95\",\"95\",\"95\",\"95\",\"1\",\"14\",\"14\",\"14\",\"14\",\"1\",\"15\",\"100\",\"100\",\"100\",\"15\",\"101\",\"101\",\"101\",\"15\",\"102\",\"102\",\"102\",\"102\",\"15\",\"15\",\"15\",\"15\",\"15\",\"15\",\"15\",\"2\",\"16\",\"16\",\"16\",\"2\",\"17\",\"17\",\"17\",\"17\",\"2\",\"2\",\"3\",\"20\",\"20\",\"20\",\"119\",\"119\",\"119\",\"119\",\"119\",\"119\",\"119\",\"119\",\"20\",\"120\",\"120\",\"120\",\"120\",\"120\",\"120\",\"120\",\"20\",\"20\",\"20\",\"20\",\"3\",\"21\",\"21\",\"21\",\"21\",\"21\",\"21\",\"21\",\"3\",\"22\",\"22\",\"22\",\"22\",\"22\",\"3\",\"23\",\"23\",\"138\",\"138\",\"138\",\"23\",\"23\",\"23\",\"23\",\"23\",\"23\",\"3\",\"3\",\"3\",\"3\",\"4\",\"4\",\"29\",\"29\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"4\",\"5\",\"5\",\"39\",\"39\",\"39\",\"39\",\"150\",\"150\",\"150\",\"150\",\"39\",\"151\",\"151\",\"151\",\"151\",\"151\",\"39\",\"39\",\"153\",\"153\",\"153\",\"153\",\"39\",\"154\",\"154\",\"154\",\"154\",\"5\",\"40\",\"40\",\"40\",\"40\",\"40\",\"40\",\"40\",\"5\",\"41\",\"41\",\"41\",\"41\",\"41\",\"41\",\"41\",\"41\",\"41\",\"5\",\"5\",\"5\",\"5\",\"45\",\"45\",\"45\",\"45\",\"5\",\"46\",\"46\",\"46\",\"46\",\"46\",\"46\",\"46\",\"5\",\"6\",\"48\",\"48\",\"48\",\"48\",\"48\",\"48\",\"6\",\"49\",\"49\",\"49\",\"49\",\"49\",\"49\",\"6\",\"50\",\"50\",\"50\",\"50\",\"50\",\"50\",\"50\",\"50\",\"6\",\"6\",\"6\",\"7\",\"54\",\"54\",\"54\",\"54\",\"54\",\"54\",\"7\",\"55\",\"55\",\"55\",\"55\",\"55\",\"55\",\"55\",\"7\",\"56\",\"56\",\"56\",\"56\",\"56\",\"56\",\"56\",\"56\",\"7\",\"57\",\"57\",\"57\",\"57\",\"57\",\"7\",\"58\",\"58\",\"58\",\"58\",\"58\",\"7\",\"59\",\"59\",\"59\",\"59\",\"59\",\"59\",\"59\",\"7\",\"7\",\"8\",\"62\",\"62\",\"62\",\"62\",\"62\",\"62\",\"62\",\"62\",\"62\",\"8\",\"63\",\"63\",\"63\",\"63\",\"63\",\"63\",\"63\",\"63\",\"63\",\"8\",\"64\",\"64\",\"64\",\"64\",\"64\",\"8\",\"65\",\"65\",\"65\",\"65\",\"65\",\"65\",\"65\",\"65\",\"65\",\"9\",\"66\",\"66\",\"66\",\"9\",\"67\",\"275\",\"275\",\"67\",\"276\",\"276\",\"9\",\"68\",\"68\",\"68\",\"68\",\"68\",\"68\",\"68\",\"68\",\"68\",\"9\",\"69\",\"286\",\"286\",\"286\",\"286\",\"69\",\"287\",\"287\",\"287\",\"69\",\"288\",\"288\",\"69\",\"289\",\"289\",\"289\",\"69\",\"290\",\"290\",\"290\",\"69\",\"69\",\"10\",\"10\",\"10\",\"10\",\"10\",\"10\",\"10\",\"10\",\"10\",\"10\",\"11\",\"11\",\"11\"],\"type\":\"treemap\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"margin\":{\"t\":50,\"l\":25,\"r\":25,\"b\":25}},                        {\"responsive\": true}                    )                };                            </script>        </div>   \r\n        <!-- END PLOTLY OUTPUT -->\r\n        <hr />\r\n        <h3>View my Code</h3>\r\n        <script src=\"https://gist.github.com/ischmidl-nd/c8036ea9b98557ca80bbf5e652d14655.js\"></script>\r\n        <hr />\r\n        <h3>View the Original Taxonomy</h3>\r\n        <div  style=\"border-style:inset; height: 75vh; overflow-y: auto;\">\r\n          <h3 class=\"book-heading\">Subject Taxonomy</h3>\r\n                <h3>Core Subject Taxonomy for Mathematical Sciences Education</h3>\r\n                <ul>\r\n                  <li><a href=\"https://www.math.duke.edu/education/ConferenceGroup04/\" target=\"_blank\">Mathematical Sciences Conference Group on Digital Educational Resources</a></li>\r\n                  <li><a href=\"/internal-archive?url=/gateway/index.html\" target=\"_blank\" title=\"This page has been archived internally\">Math Gateway Partners Group</a></li>\r\n                </ul>\r\n                <p>This taxonomy is based on the Math NSDL Taxonomy Committee Report, April 2, 2002, with draft changes proposed for Section 9 by CAUSE, May 16, 2004. Further changes to Section 9 were approved by the <a href=\"/internal-archive?url=/gateway/index.html\" target=\"_blank\" title=\"This page has been archived internally\">Math Gateway Partners Group</a>, April 29, 2005</p>\r\n                <p>The first two levels of this taxonomy are used for classifying JOMA documents by subject matter.&#160; A fuller indication of the meanings of those levels may be obtained by scanning the deeper levels.&#160;</p>\r\n                <ol style=\"list-style-type:none\">\r\n                  <li>1.0 Numbers and Computation\r\n                    <ol style=\"list-style-type:none\">\r\n                      <li>1.1 Number Concepts\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>1.1.1 Natural</li>\r\n                          <li>1.1.2 Integers</li>\r\n                          <li>1.1.3 Rational</li>\r\n                          <li>1.1.4 Irrational</li>\r\n                          <li>1.1.5 Algebraic</li>\r\n                          <li>1.1.6 Real</li>\r\n                          <li>1.1.7 Complex</li>\r\n                          <li>1.1.8 Famous Numbers\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>1.1.8.1 0</li>\r\n                              <li>1.1.8.2 pi</li>\r\n                              <li>1.1.8.3 e</li>\r\n                              <li>1.1.8.4 i</li>\r\n                              <li>1.1.8.5 Golden Mean</li>\r\n                            </ol>\r\n                          </li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>1.2 Arithmetic\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>1.2.1 Operations\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>1.2.1.1 Addition</li>\r\n                              <li>1.2.1.2 Subtraction</li>\r\n                              <li>1.2.1.3 Multiplication</li>\r\n                              <li>1.2.1.4 Division</li>\r\n                              <li>1.2.1.5 Roots</li>\r\n                              <li>1.2.1.6 Factorials</li>\r\n                              <li>1.2.1.7 Factoring</li>\r\n                              <li>1.2.1.8 Properties of Operations</li>\r\n                              <li>1.2.1.9 Estimation</li>\r\n                            </ol>\r\n                          </li>\r\n                          <li>1.2.2 Fractions\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>1.2.2.1 Addition</li>\r\n                              <li>1.2.2.2 Subtraction</li>\r\n                              <li>1.2.2.3 Multiplication</li>\r\n                              <li>1.2.2.4 Division</li>\r\n                              <li>1.2.2.5 Ratio and Proportion</li>\r\n                              <li>1.2.2.6 Equivalent Fractions</li>\r\n                            </ol>\r\n                          </li>\r\n                          <li>1.2.3 Decimals\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>1.2.3.1 Addition</li>\r\n                              <li>1.2.3.2 Subtraction</li>\r\n                              <li>1.2.3.3 Multiplication</li>\r\n                              <li>1.2.3.4 Division</li>\r\n                              <li>1.2.3.5 Percents</li>\r\n                            </ol>\r\n                          </li>\r\n                          <li>1.2.4 Comparison of numbers</li>\r\n                          <li>1.2.5 Exponents\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>1.2.5.1 Multiplication</li>\r\n                              <li>1.2.5.2 Division</li>\r\n                              <li>1.2.5.3 Powers</li>\r\n                              <li>1.2.5.4 Integer Exponents</li>\r\n                              <li>1.2.5.5 Rational Exponents</li>\r\n                            </ol>\r\n                          </li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>1.3 Patterns and Sequences\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>1.3.1 Number Patterns</li>\r\n                          <li>1.3.2 Fibonacci Sequence</li>\r\n                          <li>1.3.3 Arithmetic Sequence</li>\r\n                          <li>1.3.4 Geometric Sequence</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>1.4 Measurement\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>1.4.1 Units of Measurement\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>1.4.1.1 Metric System</li>\r\n                              <li>1.4.1.2 Standard Units</li>\r\n                              <li>1.4.1.3 Nonstandard Units</li>\r\n                            </ol>\r\n                          </li>\r\n                          <li>1.4.2 Linear Measure\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>1.4.2.1 Distance</li>\r\n                              <li>1.4.2.2 Circumference</li>\r\n                              <li>1.4.2.3 Perimeter</li>\r\n                            </ol>\r\n                          </li>\r\n                          <li>1.4.3 Area\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>1.4.3.1 Area of Polygons</li>\r\n                              <li>1.4.3.2 Area of Circles</li>\r\n                              <li>1.4.3.3 Surface Area</li>\r\n                              <li>1.4.3.4 Nonstandard Shapes</li>\r\n                            </ol>\r\n                          </li>\r\n                          <li>1.4.4 Volume</li>\r\n                          <li>1.4.5 Weight and Mass</li>\r\n                          <li>1.4.6 Temperature</li>\r\n                          <li>1.4.7 Time</li>\r\n                          <li>1.4.8 Speed</li>\r\n                          <li>1.4.9 Money</li>\r\n                          <li>1.4.10 Scale</li>\r\n                        </ol>\r\n                      </li>\r\n                    </ol>\r\n                  </li>\r\n                  <li>2.0 Logic and Foundations\r\n                    <ol style=\"list-style-type:none\">\r\n                      <li>2.1 Logic\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>2.1.1 Venn Diagrams</li>\r\n                          <li>2.1.2 Propositional and Predicate Logic</li>\r\n                          <li>2.1.3 Methods of Proof</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>2.2 Set Theory\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>2.2.1 Sets and Set Operations</li>\r\n                          <li>2.2.2 Relations and Functions</li>\r\n                          <li>2.2.3 Cardinality</li>\r\n                          <li>2.2.4 Axiom of Choice</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>2.3 Computability and Decidability</li>\r\n                      <li>2.4 Model Theory</li>\r\n                    </ol>\r\n                  </li>\r\n                  <li>3.0 Algebra and Number Theory\r\n                    <ol style=\"list-style-type:none\">\r\n                      <li>3.1 Algebra\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>3.1.1 Graphing Techniques</li>\r\n                          <li>3.1.2 Algebraic Manipulation</li>\r\n                          <li>3.1.3 Functions\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>3.1.3.1 Linear</li>\r\n                              <li>3.1.3.2 Quadratic</li>\r\n                              <li>3.1.3.3 Polynomial</li>\r\n                              <li>3.1.3.4 Rational</li>\r\n                              <li>3.1.3.5 Exponential</li>\r\n                              <li>3.1.3.6 Logarithmic</li>\r\n                              <li>3.1.3.7 Piece-wise</li>\r\n                              <li>3.1.3.8 Step</li>\r\n                            </ol>\r\n                          </li>\r\n                          <li>3.1.4 Equations\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>3.1.4.1 Linear</li>\r\n                              <li>3.1.4.2 Quadratic</li>\r\n                              <li>3.1.4.3 Polynomial</li>\r\n                              <li>3.1.4.4 Rational</li>\r\n                              <li>3.1.4.5 Exponential</li>\r\n                              <li>3.1.4.6 Logarithmic</li>\r\n                              <li>3.1.4.7 Systems</li>\r\n                            </ol>\r\n                          </li>\r\n                          <li>3.1.5 Inequalities</li>\r\n                          <li>3.1.6 Matrices</li>\r\n                          <li>3.1.7 Sequences and Series</li>\r\n                          <li>3.1.8 Algebraic Proof</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>3.2 Linear Algebra\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>3.2.1 Systems of Linear Equations</li>\r\n                          <li>3.2.2 Matrix algebra</li>\r\n                          <li>3.2.3 Vectors in R3</li>\r\n                          <li>3.2.4 Vector Spaces</li>\r\n                          <li>3.2.5 Linear Transformations</li>\r\n                          <li>3.2.6 Eigenvalues and Eigenvectors</li>\r\n                          <li>3.2.7 Inner Product Spaces</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>3.3 Abstract Algebra\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>3.3.1 Groups</li>\r\n                          <li>3.3.2 Rings and Ideals</li>\r\n                          <li>3.3.3 Fields</li>\r\n                          <li>3.3.4 Galois Theory</li>\r\n                          <li>3.3.5 Multilinear Algebra</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>3.4 Number Theory\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>3.4.1 Integers</li>\r\n                          <li>3.4.2 Primes</li>\r\n                          <li>3.4.2.1 Divisibility</li>\r\n                          <li>3.4.2.2 Factorization</li>\r\n                          <li>3.4.2.3 Distributions of Primes</li>\r\n                          <li>3.4.3 Congruences</li>\r\n                          <li>3.4.4 Diophantine Equations</li>\r\n                          <li>3.4.5 Irrational Numbers</li>\r\n                          <li>3.4.6 Famous Problems</li>\r\n                          <li>3.4.7 Coding Theory</li>\r\n                          <li>3.4.8 Cryptography</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>3.5 Category Theory</li>\r\n                      <li>3.6 K-Theory</li>\r\n                      <li>3.7 Homological Algebra</li>\r\n                      <li>3.8 Modular Arithmetic</li>\r\n                    </ol>\r\n                  </li>\r\n                  <li>4.0 Discrete Mathematics\r\n                    <ol style=\"list-style-type:none\">\r\n                      <li>4.1 Cellular Automata</li>\r\n                      <li>4.2 Combinatorics\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>4.2.1 Combinations</li>\r\n                          <li>4.2.2 Permutations</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>4.3 Game Theory</li>\r\n                      <li>4.4 Algorithms</li>\r\n                      <li>4.5 Recursion</li>\r\n                      <li>4.6 Graph Theory</li>\r\n                      <li>4.7 Linear Programming</li>\r\n                      <li>4.8 Order and Lattices</li>\r\n                      <li>4.9 Theory of Computation</li>\r\n                      <li>4.10 Chaos</li>\r\n                    </ol>\r\n                  </li>\r\n                  <li>5.0 Geometry and Topology\r\n                    <ol style=\"list-style-type:none\">\r\n                      <li>5.1 Geometric Proof</li>\r\n                      <li>5.2 Plane Geometry\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>5.2.1 Measurement</li>\r\n                          <li>5.2.2 Lines and Planes</li>\r\n                          <li>5.2.3 Angles</li>\r\n                          <li>5.2.4 Triangles\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>5.2.4.1 Properties</li>\r\n                              <li>5.2.4.2 Congruence</li>\r\n                              <li>5.2.4.3 Similarity</li>\r\n                              <li>5.2.4.4 Pythagorean Theorem</li>\r\n                            </ol>\r\n                          </li>\r\n                          <li>5.2.5 Polygons\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>5.2.5.1 Properties</li>\r\n                              <li>5.2.5.2 Regular</li>\r\n                              <li>5.2.5.3 Irregular</li>\r\n                              <li>5.2.5.4 Congruence</li>\r\n                              <li>5.2.5.5 Similarity</li>\r\n                            </ol>\r\n                          </li>\r\n                          <li>5.2.6 Circles</li>\r\n                          <li>5.2.7 Patterns\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>5.2.7.1 Geometric Patterns</li>\r\n                              <li>5.2.7.2 Tilings and Tessellations</li>\r\n                              <li>5.2.7.3 Symmetry</li>\r\n                              <li>5.2.7.4 Golden Ratio</li>\r\n                            </ol>\r\n                          </li>\r\n                          <li>5.2.8 Transformations\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>5.2.8.1 Translation</li>\r\n                              <li>5.2.8.2 Rotation</li>\r\n                              <li>5.2.8.3 Reflection</li>\r\n                              <li>5.2.8.4 Scaling</li>\r\n                            </ol>\r\n                          </li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>5.3 Solid Geometry\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>5.3.1 Dihedral Angles</li>\r\n                          <li>5.3.2 Spheres</li>\r\n                          <li>5.3.3 Cones</li>\r\n                          <li>5.3.4 Cylinders</li>\r\n                          <li>5.3.5 Pyramids</li>\r\n                          <li>5.3.6 Prisms</li>\r\n                          <li>5.3.7 Polyhedra</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>5.4 Analytic Geometry\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>5.4.1 Cartesian Coordinates</li>\r\n                          <li>5.4.2 Lines</li>\r\n                          <li>5.4.3 Circles</li>\r\n                          <li>5.4.4 Planes</li>\r\n                          <li>5.4.5 Conics</li>\r\n                          <li>5.4.6 Polar Coordinates</li>\r\n                          <li>5.4.7 Parametric Curves</li>\r\n                          <li>5.4.8 Surfaces</li>\r\n                          <li>5.4.9 Distance Formula</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>5.5 Projective Geometry</li>\r\n                      <li>5.6 Differential Geometry</li>\r\n                      <li>5.7 Algebraic Geometry</li>\r\n                      <li>5.8 Topology\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>5.8.1 Point Set Topology</li>\r\n                          <li>5.8.2 General Topology</li>\r\n                          <li>5.8.3 Differential Topology</li>\r\n                          <li>5.8.4 Algebraic Topology</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>5.9 Trigonometry\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>5.9.1 Angles</li>\r\n                          <li>5.9.2 Trigonometric Functions</li>\r\n                          <li>5.9.3 Inverse Trigonometric Functions</li>\r\n                          <li>5.9.4 Trigonometric Identities</li>\r\n                          <li>5.9.5 Trigonometric Equations</li>\r\n                          <li>5.9.6 Roots of Unity</li>\r\n                          <li>5.9.7 Spherical Trigonometry</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>5.10 Fractal Geometry</li>\r\n                    </ol>\r\n                  </li>\r\n                  <li>6.0 Calculus\r\n                    <ol style=\"list-style-type:none\">\r\n                      <li>6.1 Single Variable\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>6.1.1 Functions</li>\r\n                          <li>6.1.2 Limits</li>\r\n                          <li>6.1.3 Continuity</li>\r\n                          <li>6.1.4 Differentiation</li>\r\n                          <li>6.1.5 Integration</li>\r\n                          <li>6.1.6 Series</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>6.2 Several Variables\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>6.2.1 Functions of Several Variables</li>\r\n                          <li>6.2.2 Limits</li>\r\n                          <li>6.2.3 Continuity</li>\r\n                          <li>6.2.4 Partial Derivatives</li>\r\n                          <li>6.2.5 Multiple integrals</li>\r\n                          <li>6.2.6 Taylor Series</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>6.3 Advanced Calculus\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>6.3.1 Vector Valued Functions</li>\r\n                          <li>6.3.2 Line Integrals</li>\r\n                          <li>6.3.3 Surface Integrals</li>\r\n                          <li>6.3.4 Stokes Theorem</li>\r\n                          <li>6.3.5 Curvilinear Coordinates</li>\r\n                          <li>6.3.6 Linear spaces</li>\r\n                          <li>6.3.7 Fourier Series</li>\r\n                          <li>6.3.8 Orthogonal Functions</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>6.4 Tensor Calculus</li>\r\n                      <li>6.5 Calculus of Variations</li>\r\n                      <li>6.6 Operational Calculus</li>\r\n                    </ol>\r\n                  </li>\r\n                  <li>7.0 Analysis\r\n                    <ol style=\"list-style-type:none\">\r\n                      <li>7.1 Real Analysis\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>7.1.1 Metric Spaces</li>\r\n                          <li>7.1.2 Convergence</li>\r\n                          <li>7.1.3 Continuity</li>\r\n                          <li>7.1.4 Differentiation</li>\r\n                          <li>7.1.5 Integration</li>\r\n                          <li>7.1.6 Measure Theory</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>7.2 Complex Analysis\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>7.2.1 Convergence</li>\r\n                          <li>7.2.2 Infinite Series</li>\r\n                          <li>7.2.3 Analytic Functions</li>\r\n                          <li>7.2.4 Integration</li>\r\n                          <li>7.2.5 Contour Integrals</li>\r\n                          <li>7.2.6 Conformal Mappings</li>\r\n                          <li>7.2.7 Several Complex Variables</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>7.3 Numerical Analysis\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>7.3.1 Computer Arithmetic</li>\r\n                          <li>7.3.2 Solutions of Equations</li>\r\n                          <li>7.3.3 Solutions of Systems</li>\r\n                          <li>7.3.4 Interpolation</li>\r\n                          <li>7.3.5 Numerical Differentiation</li>\r\n                          <li>7.3.6 Numerical Integration</li>\r\n                          <li>7.3.7 Numerical Solutions of ODEs</li>\r\n                          <li>7.3.8 Numerical Solutions of PDEs</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>7.4 Integral Transforms\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>7.4.1 Fourier Transforms</li>\r\n                          <li>7.4.2 Laplace Transforms</li>\r\n                          <li>7.4.3 Hankel Transforms</li>\r\n                          <li>7.4.4 Wavelets</li>\r\n                          <li>7.4.5 Other Transforms</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>7.5 Signal Analysis\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>7.5.1 Sampling Theory</li>\r\n                          <li>7.5.2 Filters</li>\r\n                          <li>7.5.3 Noise</li>\r\n                          <li>7.5.4 Data Compression</li>\r\n                          <li>7.5.5 Image Processing</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>7.6 Functional Analysis\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>7.6.1 Hilbert Spaces</li>\r\n                          <li>7.6.2 Banach Spaces</li>\r\n                          <li>7.6.3 Topological Spaces</li>\r\n                          <li>7.6.4 Locally Convex Spaces</li>\r\n                          <li>7.6.5 Bounded Operators</li>\r\n                          <li>7.6.6 Spectral Theorem</li>\r\n                          <li>7.6.7 Unbounded Operators</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>7.7 Harmonic Analysis</li>\r\n                      <li>7.8 Global Analysis</li>\r\n                    </ol>\r\n                  </li>\r\n                  <li>8.0 Differential and Difference Equations\r\n                    <ol style=\"list-style-type:none\">\r\n                      <li>8.1 Ordinary Differential Equations\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>8.1.1 First Order</li>\r\n                          <li>8.1.2 Second Order</li>\r\n                          <li>8.1.3 Linear Oscillations</li>\r\n                          <li>8.1.4 Nonlinear Oscillations</li>\r\n                          <li>8.1.5 Systems of Differential Equations</li>\r\n                          <li>8.1.6 Sturm Liouville Problems</li>\r\n                          <li>8.1.7 Special Functions</li>\r\n                          <li>8.1.8 Power Series Methods</li>\r\n                          <li>8.1.9 Laplace Transforms</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>8.2 Partial Differential Equations\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>8.2.1 First Order</li>\r\n                          <li>8.2.2 Elliptic</li>\r\n                          <li>8.2.3 Parabolic</li>\r\n                          <li>8.2.4 Hyperbolic</li>\r\n                          <li>8.2.5 Integral Transforms</li>\r\n                          <li>8.2.6 Integral Equations</li>\r\n                          <li>8.2.7 Potential Theory</li>\r\n                          <li>8.2.8 Nonlinear Equations</li>\r\n                          <li>8.2.9 Symmetries and Integrability</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>8.3 Difference Equations\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>8.3.1 First Order</li>\r\n                          <li>8.3.2 Second Order</li>\r\n                          <li>8.3.3 Linear Systems</li>\r\n                          <li>8.3.4 Z Transforms</li>\r\n                          <li>8.3.5 Orthogonal Polynomials</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>8.4 Dynamical Systems\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>8.4.1 1D Maps</li>\r\n                          <li>8.4.2 2D Maps</li>\r\n                          <li>8.4.3 Lyapunov Exponents</li>\r\n                          <li>8.4.4 Bifurcations</li>\r\n                          <li>8.4.5 Fractals</li>\r\n                          <li>8.4.6 Differentiable Dynamics</li>\r\n                          <li>8.4.7 Conservative Dynamics</li>\r\n                          <li>8.4.8 Chaos</li>\r\n                          <li>8.4.9 Complex Dynamical Systems</li>\r\n                        </ol>\r\n                      </li>\r\n                    </ol>\r\n                  </li>\r\n                  <li>9.0 Statistics and Probability\r\n                    <ol style=\"list-style-type:none\">\r\n                      <li>9.1 Data Collection\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>9.1.1 Experimental Design</li>\r\n                          <li>9.1.2 Sampling and Surveys</li>\r\n                          <li>9.1.3 Data and Measurement Issues</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>9.2 Data Summary and Presentation\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>9.2.1 Summary Statistics\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>9.2.1.1 Measures of Central Tendencies</li>\r\n                              <li>9.2.1.2 Measures of Spread</li>\r\n                            </ol>\r\n                          </li>\r\n                          <li>9.2.2 Data Representation\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>9.2.2.1 Graphs and Plots</li>\r\n                              <li>9.2.2.2 Tables</li>\r\n                            </ol>\r\n                          </li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>9.3 Statistical Inference and Techniques\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>9.3.1 Sampling Distributions</li>\r\n                          <li>9.3.2 Regression and Correlation</li>\r\n                          <li>9.3.3 Confidence Intervals</li>\r\n                          <li>9.3.4 Hypothesis Tests</li>\r\n                          <li>9.3.5 Statistical Quality Control</li>\r\n                          <li>9.3.6 Non-parametric Techniques</li>\r\n                          <li>9.3.7 Multivariate Techniques</li>\r\n                          <li>9.3.8 Survival Analysis</li>\r\n                          <li>9.3.9 Bayesian Statistics</li>\r\n                        </ol>\r\n                      </li>\r\n                      <li>9.4 Probability\r\n                        <ol style=\"list-style-type:none\">\r\n                          <li>9.4.1 Elementary Probability\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>9.4.1.1 Sample Space and Sets</li>\r\n                              <li>9.4.1.2 General Rules</li>\r\n                              <li>9.4.1.3 Combinations and Permutations</li>\r\n                              <li>9.4.1.4 Random Variables</li>\r\n                            </ol>\r\n                          </li>\r\n                          <li>9.4.2 Univariate Distributions\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>9.4.2.1 Discrete Distributions</li>\r\n                              <li>9.4.2.2 Continuous Distributions</li>\r\n                              <li>9.4.2.3 Expected Value</li>\r\n                            </ol>\r\n                          </li>\r\n                          <li>9.4.3 Limit Theorems\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>9.4.3.1 Central Limit Theorem</li>\r\n                              <li>9.4.3.2 Law of Large Numbers</li>\r\n                            </ol>\r\n                          </li>\r\n                          <li>9.4.4 Multivariate Distributions\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>9.4.4.1 Joint</li>\r\n                              <li>9.4.4.2 Conditional</li>\r\n                              <li>9.4.4.3 Expectations</li>\r\n                            </ol>\r\n                          </li>\r\n                          <li>9.4.5 Stochastic Processes\r\n                            <ol style=\"list-style-type:none\">\r\n                              <li>9.4.5.1 Brownian Motion</li>\r\n                              <li>9.4.5.2 Markov Chains</li>\r\n                              <li>9.4.5.3 Queuing Theory</li>\r\n                            </ol>\r\n                          </li>\r\n                          <li>9.4.6 Probability Measures</li>\r\n                          <li>9.4.7 Simulation</li>\r\n                        </ol>\r\n                      </li>\r\n                    </ol>\r\n                  </li>\r\n                  <li>10.0 Applied Mathematics\r\n                    <ol style=\"list-style-type:none\">\r\n                      <li>10.1 Mathematical Physics</li>\r\n                      <li>10.2 Mathematical Economics</li>\r\n                      <li>10.3 Mathematical Biology</li>\r\n                      <li>10.4 Mathematics for Business</li>\r\n                      <li>10.5 Engineering Mathematics</li>\r\n                      <li>10.6 Mathematical Sociology</li>\r\n                      <li>10.7 Mathematics for Social Sciences</li>\r\n                      <li>10.8 Mathematics for Computer Science</li>\r\n                      <li>10.9 Mathematics for Humanities</li>\r\n                      <li>10.10 Consumer Mathematics</li>\r\n                    </ol>\r\n                  </li>\r\n                  <li>11.0 Mathematics History\r\n                    <ol style=\"list-style-type:none\">\r\n                      <li>11.1 General</li>\r\n                      <li>11.2 Famous Problems</li>\r\n                      <li>11.3 Biographies of Mathematicians</li>\r\n                    </ol>\r\n                  </li>\r\n                </ol>\r\n              </div>\r\n            <!--END CONTAINER FLUID-->\r\n            <script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.2.0-beta1/dist/js/bootstrap.bundle.min.js\" integrity=\"sha384-pprn3073KE6tl6bjs2QrFaJGz5/SUsLqktiwsUTF55Jfv3qYSDhgCecCxMW52nD2\" crossorigin=\"anonymous\"></script>\r\n\r\n<hr>\r\n<h1>Inspirations</h1>\r\n  <h2>Quanta's Map</h2>\r\n  <a href=\"https://mathmap.quantamagazine.org/map\">source</a><br>\r\n  <iframe height=600 width=1000 src=\"https://mathmap.quantamagazine.org/map\"></iframe>\r\n      \r\n  <h2>Domain of Science Map</h2>\r\n  <a data-flickr-embed=\"true\" href=\"https://www.flickr.com/photos/95869671@N08/32264483720\" title=\"Map of Mathematics Poster\"><img src=\"https://live.staticflickr.com/272/32264483720_0be41eb9d1_k.jpg\" width=\"1000\" alt=\"Map of Mathematics Poster\"></a>\r\n  <script async src=\"//embedr.flickr.com/assets/client-code.js\" charset=\"utf-8\"></script>\r\n\r\n  <h2>University of Michigan</h2>\r\n    <img width=\"1000\" alt=\"university of michigan math tracks\" src=\"/img/mathtax/university of michigan.png\">\r\n  <h2>Swarthmore</h2>\r\n    <img width=\"1000\" alt=\"swarthmore math tracks\" src=\"/img/mathtax/swarthmore.png\">\r\n  <h2>Binghamton</h2>\r\n    <img width=\"1000\" alt=\"binghamton math tracks\" src=\"/img/mathtax/binghamton.png\">\r\n  </main>\r\n\r\n      \r\n    </div>\r\n    <!-- <footer  class=\"bg-light pt-5 pb-2 px-5\"><hr><a href=\"/\">Home</a></footer> -->\r\n    \r\n    \r\n    <script src=\"https://code.jquery.com/jquery-3.4.1.min.js\"></script>\r\n    <script src=\"https://code.jquery.com/ui/1.12.1/jquery-ui.js\"></script>\r\n    <script src=\"https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js\" integrity=\"sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo\" crossorigin=\"anonymous\"></script>\r\n    <script src=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js\" integrity=\"sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6\" crossorigin=\"anonymous\"></script>\r\n\r\n  </body>\r\n</html>\r\n",
      "date_published": "2022-06-30T17:00:00-07:00"
    }
  ]
}
