<!doctype html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css" integrity="sha384-B0vP5xmATw1+K9KRQjQERJvTumQW0nPEzvF6L/Z6nronJ3oUOFUFpCjEUQouq2+l" crossorigin="anonymous">
    <!--<link rel="stylesheet" href="css/main.css">-->
    <title>Linear Algebra Theorems</title>
    <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <!--<script type="text/javascript">-->
    <!--    MathJax = {-->
    <!--        tex: {-->
    <!--            inlineMath: [['$', '$'], ['\\(', '\\)']]-->
    <!--        },-->
    <!--        svg: {-->
    <!--            fontCache: 'global'-->
    <!--        }-->
    <!--    };-->
    <!--</script>-->
  </head><body>
    <div class="container" id="bsr-wrapper">
      
      
      <div style="background-color: coral;">Under construction 6/19/23-...</div>
      
      
      <h1>Linear Algebra Theorems</h1>
      <p>The six central theorems of linear algebra come from Gilbert Strang's <strong>Introduction to Linear Algebra, 5th Ed</strong>, and I have provided an example for each.
      </p>
      <ul>
        <li><a href="#dimension-theorem">Dimension Theorem</a></li>
        <li><a href="#counting-theorem">Counting Theorem</a></li>
        <li><a href="#rank-theorem">Rank Theorem</a></li>
        <li><a href="#fundamental-theorem">Fundamental Theorem</a></li>
        <li><a href="#singular-value-decomposition">Singular Value Decomposition</a></li>
        <li><a href="#spectral-theorem">Spectral Theorem</a></li>
      </ul>
      <hr>      
      <h2 id="dimension-theorem">Dimension Theorem</h2>
      <p>All bases for a vector space have the same number of vectors.</p>
      <p>Mathematically: \( \text{dim}(V) = \text{dim}(W) \) for any bases \( V \) and \( W \) of the vector space.</p>
      <div>
        <p>
          <strong>Example:</strong>
        </p>
        <p>Let's consider a vector space \(V = \mathbb{R}^2\) over the field \(F = \mathbb{R}\) (the set of real numbers). In this case, vectors in \(V\) are ordered pairs \((x, y)\) where \(x\) and \(y\) are real numbers.</p>
        <p>Now, let's find two different bases for \(V\) and observe that they have the same number of vectors.</p>
        <p>
          <em>Basis 1:</em>
        </p>
        <p>We can choose the following two vectors as a basis for \(V\):</p>
        <p>\(\mathbf{v}_1 = \begin{pmatrix} 1 \\ 0 \end{pmatrix}\)</p>
        <p>\(\mathbf{v}_2 = \begin{pmatrix} 0 \\ 1 \end{pmatrix}\)</p>
        <p>These vectors are linearly independent (meaning no non-trivial linear combination of them yields the zero vector) and span the entire vector space \(V\).</p>
        <p>
          <em>Basis 2:</em>
        </p>
        <p>Alternatively, we can choose the following two vectors as another basis for \(V\):</p>
        <p>\(\mathbf{u}_1 = \begin{pmatrix} 2 \\ 1 \end{pmatrix}\)</p>
        <p>\(\mathbf{u}_2 = \begin{pmatrix} -1 \\ 3 \end{pmatrix}\)</p>
        <p>Again, these vectors are linearly independent and span the entire vector space \(V\).</p>
        <p>Both Basis 1 and Basis 2 consist of two vectors each. This example demonstrates that all bases for \(V\) have the same number of vectors, which in this case is 2. This property holds true for any vector space, indicating that the number of vectors in a basis is a fundamental characteristic of the vector space itself.</p>
      </div>
      <hr><h2 id="counting-theorem">Counting Theorem</h2>
      <p>Dimension of column space + dimension of nullspace = number of columns.</p>
      <p>Mathematically: \( \text{dim}(\text{col}(A)) + \text{dim}(\text{null}(A)) = \text{cols}(A) \)</p>
      <div>
        <p>
          <strong>Example:</strong>
        </p>
        <p>Consider the matrix:</p>
        <p>\[ A = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \\ \end{bmatrix} \]</p>
        <p>Let's calculate the dimension of the column space (step 1) and the dimension of the nullspace (step 2) of \(A\), and verify the theorem.</p>
        <p>
          <em>Solution:</em>
        </p>
        <p>STEP 1: To find the column space of \(A\), we reduce \(A\) to echelon form:</p>
        <p>\[ \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \\ \end{bmatrix} \xrightarrow{\text{Row operations}} \begin{bmatrix} 1 & 0 & -1 \\ 0 & 1 & 2 \\ 0 & 0 & 0 \\ \end{bmatrix} \]</p>
        <div class="card">
          <div class="card-body">
              <p>
                  Do row reduction:
                  \[
                  \begin{pmatrix}
                  1 & 2 & 3 \\
                  4 & 5 & 6 \\
                  7 & 8 & 9 \\
                  \end{pmatrix}
                  \]
              </p>
              <p>Subtract a multiple of one row from another.<br>
              Subtract \(4 \times\) (row 1) from row 2:<br>
              \[
              \begin{pmatrix}
              1 & 2 & 3 \\
              0 & -3 & -6 \\
              7 & 8 & 9 \\
              \end{pmatrix}
              \]
              </p>
              <p>Subtract a multiple of one row from another.<br>
              Subtract \(7 \times\) (row 1) from row 3:<br>
              \[
              \begin{pmatrix}
              1 & 2 & 3 \\
              0 & -3 & -6 \\
              0 & -6 & -12 \\
              \end{pmatrix}
              \]
              </p>
              <p>Swap two rows.<br>
              Swap row 2 with row 3:<br>
              \[
              \begin{pmatrix}
              1 & 2 & 3 \\
              0 & -6 & -12 \\
              0 & -3 & -6 \\
              \end{pmatrix}
              \]
              </p>
              <p>Subtract a multiple of one row from another.<br>
              Subtract \(\frac{1}{2} \times\) (row 2) from row 3:<br>
              \[
              \begin{pmatrix}
              1 & 2 & 3 \\
              0 & -6 & -12 \\
              0 & 0 & 0 \\
              \end{pmatrix}
              \]
              </p>
              <p>Divide row 2 by a scalar.<br>
              Divide row 2 by -6:<br>
              \[
              \begin{pmatrix}
              1 & 2 & 3 \\
              0 & 1 & 2 \\
              0 & 0 & 0 \\
              \end{pmatrix}
              \]
              </p>
              <p>Subtract a multiple of one row from another.<br>
              Subtract \(2 \times\) (row 2) from row 1:<br>
              \[
              \begin{pmatrix}
              1 & 0 & -1 \\
              0 & 1 & 2 \\
              0 & 0 & 0 \\
              \end{pmatrix}
              \]
              </p>
              <p>Verify matrix is reduced.<br>
              This matrix is now in reduced row echelon form.<br>
              All nonzero rows are above rows of all zeros:<br>
              \[
              \begin{pmatrix}
              1 & 0 & -1 \\
              0 & 1 & 2 \\
              0 & 0 & 0 \\
              \end{pmatrix}
              \]
              </p>
              <p>Verify pivots and their positions.<br>
              Each pivot is 1 and is strictly to the right of every pivot above it:<br>
              \[
              \begin{pmatrix}
              1 & 0 & -1 \\
              0 & 1 & 2 \\
              0 & 0 & 0 \\
              \end{pmatrix}
              \]
              </p>       
        </div>
      </div>
        <br>
        <p>The pivot columns are the first two columns, and they form a basis for the column space of \(A\). So, the dimension of the column space is \(2\).</p>
        <p>STEP 2: Now, let's find the nullspace of \(A\) by solving the homogeneous equation \(A\mathbf{x} = \mathbf{0}\):</p>
        <p>\[ \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \\ \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ 0 \\ \end{bmatrix} \]</p>
        <div class="card">
        <div class="card-body">
          <p>The null space of a matrix \(M\) is the set of solutions \(v\) to the homogeneous equation \(M \cdot v = 0\).<br>
          The null space of matrix \(M = \begin{pmatrix} 1 & 0 & -1 \\ 0 & 1 & 2 \\ 0 & 0 & 0 \end{pmatrix}\) is the set of all vectors \(v = (x_1, x_2, x_3)\) such that \(M \cdot v = 0\):<br>
          \(\begin{pmatrix} 1 & 0 & -1 \\ 0 & 1 & 2 \\ 0 & 0 & 0 \end{pmatrix} \cdot \begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix}\)
          </p>
          <p>Identify free variables.<br>
          Free variables in the null space \((x_1, x_2, x_3)\) correspond to the columns in \(\begin{pmatrix} 1 & 0 & -1 \\ 0 & 1 & 2 \\ 0 & 0 & 0 \end{pmatrix}\) which have no pivot.<br>
          Column 3 is the only column with no pivot, so we may take \(x_3\) to be the only free variable.
          </p>
          <p>Perform matrix multiplication.<br>
          Multiply out the reduced matrix \(\begin{pmatrix} 1 & 0 & -1 \\ 0 & 1 & 2 \\ 0 & 0 & 0 \end{pmatrix}\) with the proposed solution vector \((x_1, x_2, x_3)\):<br>
          \(\begin{pmatrix} 1 & 0 & -1 \\ 0 & 1 & 2 \\ 0 & 0 & 0 \end{pmatrix} \cdot \begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix} = \begin{pmatrix} x_1 - x_3 \\ x_2 + 2x_3 \\ 0 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix}\)
          </p>
          <p>Convert to a system and solve in terms of the free variables.<br>
          Solve the equations \(x_1 - x_3 = 0\), \(x_2 + 2x_3 = 0\), and \(0 = 0\) for \(x_1\) and \(x_2\):<br>
          \(\{x_1 = x_3, x_2 = -2x_3, 0 = 0\) for \(x_1\) and \(x_2 \}\)
          </p>
          <p>Replace the pivot variables with free variable expressions.<br>
          Rewrite \(v\) in terms of the free variable \(x_3\), and assign it an arbitrary real value of \(x\):<br>
          \(v = (x_1, x_2, x_3) = (x_3, -2x_3, x_3) = (x, -2x, x)\) for \(x \in \mathbb{R}\)
          </p>
          <p>
          Rewrite the solution vector \(v = (x, -2x, x)\) in set notation:<br>
          Answer: \(\{(x, -2x, x) : x \in \mathbb{R}\}\)
          </p>
      </div>
      </div>
      
      <br>
      <p>Thus, the <em>nullity</em> &mdash;or dimension of the nullspace&mdash; is one, \( \text{dim}(\text{null}(A)) = 1 \)</p>
      
      <p>Now, by the theorem, the dimension of the column space plus the dimension of the nullspace should be equal to the number of columns, \( \text{dim}(\text{col}(A)) + \text{dim}(\text{null}(A)) = \text{cols}(A) \)</p>
        <p>Here, \( \text{dim}(\text{col}(A)) = 2 \) and \( \text{dim}(\text{null}(A)) = 1 \) so \( \text{cols}(A) = 2 + 1 = 3 \). The number of columns in \(A\) is also \(3\).</p>
      </div>
      <hr><h2 id="rank-theorem">Rank Theorem</h2>
      <p>Dimension of column space = dimension of row space.</p>
      <p>Mathematically: \( \text{dim}(\text{col}(A)) = \text{dim}(\text{row}(A)) \)</p>
      <div>
        <p>
          <strong>Example:</strong>
        </p>
        <p>Consider the matrix:</p>
        <p>\[ A = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \\ \end{bmatrix} \]</p>
        <p>Let's calculate the dimensions of the column space and the row space of \(A\), and verify the theorem.</p>
        <p>
          <em>Solution:</em>
        </p>
        <p>To find the column space of \(A\), we reduce \(A\) to echelon form:</p>
        <p>\[ \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \\ \end{bmatrix} \xrightarrow{\text{Row operations}} \begin{bmatrix} 1 & 0 & -1 \\ 0 & 1 & 2 \\ 0 & 0 & 0 \\ \end{bmatrix} \]</p>
        <p>The pivot columns are the first column and the second column, and they form a basis for the column space of \(A\). So, the dimension of the column space is 2.</p>
        <p>To find the row space of \(A\), we observe that the rows of the echelon form with pivots correspond to the nonzero rows of \(A\). So, the row space of \(A\) is spanned by the following two rows:</p>
        <p>\[ \begin{bmatrix} 1 & 2 & 3 \\ \end{bmatrix} \quad \text{and} \quad \begin{bmatrix} 0 & -3 & -6 \\ \end{bmatrix} \]</p>
        <p>These rows are linearly independent, and they form a basis for the row space of \(A\). So, the dimension of the row space is also 2.</p>
        <p>According to the theorem, the dimension of the column space should be equal to the dimension of the row space.</p>
        <p>Here, the dimension of the column space is 2, and the dimension of the row space is also 2.</p>
      </div>

      <hr><h2 id="fundamental-theorem">Fundamental Theorem</h2>
      <p>The row space and nullspace of \( A \) are orthogonal complements in \( \mathbb{R}^n \).</p>
      <p>Mathematically: \( \text{row}(A) \perp \text{null}(A) \) in \( \mathbb{R}^n \)</p>
      <div>
        <p>
          <strong>Example:</strong>
        </p>
        <p>Consider the matrix:</p>
        <p>\[ A = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \\ \end{bmatrix} \]</p>
        <p>Let's calculate the row space (step 1) and the nullspace (step 2) of \(A\) and verify that they are orthogonal complements in \(\mathbb{R}^n\).</p>
        <p>
          <em>Solution:</em>
        </p>
        <div>
          <p>This one requires a bit more thinking outside of the row reduction algorithm</p>
          <p>
              Recall from <em>The Counting Theorem</em>, the null space has dimension equal to the difference between the number of columns and the dimension of the row space, \(\text{col}(A) - \text{dim}(\text{row}(A))\).
              It follows that the orthogonal complement of the null space has dimension \(\text{col}(A) - (\text{col}(A) - \text{dim}(\text{row}(A)))=\text{dim}(\text{row}(A))\).
              Now, the row space is an \(r\) dimensional subspace of the orthogonal complement of the null space, which in turn has dimension \(r\).
              The only \(r\) dimensional subspace of an \(r\)-dimensional space is the entirety of the space itself, \( A \subseteq B, \ \text{dim}(A)=\text{dim}(B) \vdash B \subseteq A, \ A=B\).
              So, the row-space is not only a subspace of the orthogonal complement but comprises the entirety of the orthogonal complement.
          </p>
      </div>      
        <p>Also Recall from <em>The Counting Theorem</em>, the nullspace is \(\{(x, -2x, x) : x \in \mathbb{R}\}\).</p>
        <p>Similarly, from the row reduction for the column space, we know that the basis for the row space is \( \{ \begin{bmatrix} 1 & 0 & -1 \end{bmatrix}, \ \begin{bmatrix} 0 & 1 & 2 \end{bmatrix}  \} \) </p>
      </div>
      <hr><h2 id="singular-value-decomposition">Singular Value Decomposition</h2>
      <p>There are orthonormal bases (\( v \)'s and \( u \)'s) for the row and column spaces so that \( Av_i = \sigma_iu_i \).</p>
      <p>Mathematically: \( A = U\Sigma V^T \) where \( U \) and \( V \) are orthonormal matrices, and \( \Sigma \) is a diagonal matrix of singular values.</p>
      <div>
        <p>
          <strong>Example:</strong>
        </p>
        <p>Consider the matrix:</p>
        <p>\[ A = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \\ \end{bmatrix} \]</p>
        <p>Let's find orthonormal bases for the row space and the column space of \(A\) and verify the given relationship \(Av_i = \sigma_iu_i\).</p>
        <p>
          <em>Solution:</em>
        </p>
        <p>To find an orthonormal basis for the row space of \(A\), we can use the process of orthogonalization. Starting with the rows of \(A\), we can apply the Gram-Schmidt process to obtain the orthonormal vectors:</p>
        <p>\[ \begin{align*} \mathbf{v}_1 &= \frac{1}{\sqrt{14}}\begin{bmatrix} 1 & 2 & 3 \end{bmatrix} \\ \mathbf{v}_2 &= \frac{1}{\sqrt{2}}\begin{bmatrix} 0 & -1 & -2 \end{bmatrix} \\ \mathbf{v}_3 &= \frac{1}{\sqrt{126}}\begin{bmatrix} 4 & 8 & 12 \end{bmatrix} \end{align*} \]</p>
        <p>These vectors form an orthonormal basis for the row space of \(A\).</p>
        <p>Next, let's find an orthonormal basis for the column space of \(A\). We can apply the Gram-Schmidt process to the columns of \(A\) to obtain the orthonormal vectors:</p>
        <p>\[ \begin{align*} \mathbf{u}_1 &= \frac{1}{\sqrt{66}}\begin{bmatrix} 1 \\ 4 \\ 7 \end{bmatrix} \\ \mathbf{u}_2 &= \frac{1}{\sqrt{66}}\begin{bmatrix} 2 \\ 5 \\ 8 \end{bmatrix} \\ \mathbf{u}_3 &= \frac{1}{\sqrt{66}}\begin{bmatrix} 3 \\ 6 \\ 9 \end{bmatrix} \end{align*} \]</p>
        <p>These vectors form an orthonormal basis for the column space of \(A\).</p>
        <p>Now, let's verify the relationship \(Av_i = \sigma_iu_i\) for each pair of vectors. Taking the singular values \(\sigma_i\) to be the norms of the vectors \(\mathbf{u}_i\), we have:</p>
        <p>\[ \begin{align*} A\mathbf{v}_1 &= \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{bmatrix} \left(\frac{1}{\sqrt{14}}\begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}\right) = \sqrt{14}\begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix} = \sqrt{14}\mathbf{u}_1 \\ A\mathbf{v}_2 &= \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{bmatrix} \left(\frac{1}{\sqrt{2}}\begin{bmatrix} 0 \\ -1 \\ -2 \end{bmatrix}\right) = \sqrt{2}\begin{bmatrix} 2 \\ 5 \\ 8 \end{bmatrix} = \sqrt{2}\mathbf{u}_2 \\ A\mathbf{v}_3 &= \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{bmatrix} \left(\frac{1}{\sqrt{126}}\begin{bmatrix} 4 \\ 8 \\ 12 \end{bmatrix}\right) = \sqrt{126}\begin{bmatrix} 3 \\ 6 \\ 9 \end{bmatrix} = \sqrt{126}\mathbf{u}_3 \end{align*} \]</p>
        <p>Hence, we can see that \(Av_i = \sigma_iu_i\) for each pair of vectors, as stated in the theorem.</p>
      </div>
      <hr><h2 id="spectral-theorem">Spectral Theorem</h2>
      <p>If \( A^T = A \), there are orthonormal \( q \)'s so that \( Aq_i = \lambda_iq_i \) and \( A = Q\Lambda Q^T \).</p>
      <p>Mathematically: If \( A \) is symmetric, there exist orthonormal eigenvectors (\( q \)'s) and eigenvalues (\( \lambda \)'s) such that \( Aq_i = \lambda_iq_i \).</p>
      <div>
        <p>
          <strong>Example:</strong>
        </p>
        <p>Consider the symmetric matrix:</p>
        <p>\[ A = \begin{bmatrix} 4 & -1 & 2 \\ -1 & 5 & 0 \\ 2 & 0 & 3 \\ \end{bmatrix} \]</p>
        <p>Let's find the orthonormal eigenvectors (\(q\)'s) and eigenvalues (\(\lambda\)'s) of \(A\) and verify the given relationship \(Aq_i = \lambda_iq_i\).</p>
        <p>
          <em>Solution:</em>
        </p>
        <p>To find the eigenvalues and eigenvectors of \(A\), we solve the characteristic equation \(\text{det}(A - \lambda I) = 0\), where \(I\) is the identity matrix:</p>
        <p>\[ \begin{vmatrix} 4 - \lambda & -1 & 2 \\ -1 & 5 - \lambda & 0 \\ 2 & 0 & 3 - \lambda \\ \end{vmatrix} = 0 \]</p>
        <p>Simplifying the determinant equation, we obtain:</p>
        <p>\[ (\lambda - 2)(\lambda - 6)(\lambda - 4) = 0 \]</p>
        <p>So, the eigenvalues of \(A\) are \(\lambda_1 = 2\), \(\lambda_2 = 6\), and \(\lambda_3 = 4\).</p>
        <p>Next, we find the corresponding eigenvectors by solving \(Aq_i = \lambda_iq_i\) for each eigenvalue:</p>
        <p>For \(\lambda_1 = 2\), we have:</p>
        <p>\[ \begin{bmatrix} 4 - 2 & -1 & 2 \\ -1 & 5 - 2 & 0 \\ 2 & 0 & 3 - 2 \\ \end{bmatrix} \begin{bmatrix} q_{11} \\ q_{21} \\ q_{31} \\ \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ 0 \\ \end{bmatrix} \]</p>
        <p>Solving the system, we find \(q_1 = \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix}\).</p>
        <p>Similarly, for \(\lambda_2 = 6\), we find \(q_2 = \begin{bmatrix} -1 \\ 1 \\ 2 \end{bmatrix}\), and for \(\lambda_3 = 4\), we find \(q_3 = \begin{bmatrix} 1 \\ 0 \\ -2 \end{bmatrix}\).</p>
        <p>Now, let's verify the relationship \(Aq_i = \lambda_iq_i\) for each pair of eigenvectors and eigenvalues:</p>
        <p>For \(\lambda_1 = 2\) and \(q_1 = \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix}\), we have:</p>
        <p>\[ \begin{bmatrix} 4 & -1 & 2 \\ -1 & 5 & 0 \\ 2 & 0 & 3 \\ \end{bmatrix} \begin{bmatrix} 1 \\ 1 \\ 0 \\ \end{bmatrix} = 2 \begin{bmatrix} 1 \\ 1 \\ 0 \\ \end{bmatrix} \]</p>
        <p>Hence, \(Aq_1 = 2q_1\) holds.</p>
        <p>Similarly, we can verify that \(Aq_2 = 6q_2\) and \(Aq_3 = 4q_3\).</p>
        <p>Therefore, in this example, we have found orthonormal eigenvectors (\(q\)'s) and eigenvalues (\(\lambda\)'s) such that \(Aq_i = \lambda_iq_i\), as stated in the theorem.</p>
      </div>
    </div>
    <footer class="bg-light pt-5 pb-2 px-5">
      <!-- <hr> -->
      <a href="/">Home</a>
    </footer>
    <script src="https://code.jquery.com/jquery-3.4.1.min.js"></script>
    <script src="https://code.jquery.com/ui/1.12.1/jquery-ui.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
    <!--<script src="js/main.js"></script>-->
  </body>
</html>
